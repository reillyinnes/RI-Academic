[{"authors":null,"categories":null,"content":"My name is Reilly Innes and I\u0026rsquo;m a Post Doc Researcher at the University of Amsterdam. I study decision making modelling in the Integrative Model-based Cognitive Neuroscience Research Unit. My work in the IMCN currently focuses on methods of joint modelling, using data from behavioural decision making tasks and fMRI to gather greater insight into the functional (and structural) relationships between brain and behaviour.\nPreviously, I completed my thesis and worked as a post-doc at the Newcastle Cognition Lab. My thesis focused on extending methods of cognitive workload measurement to new environments and for new purposes. I then spent a year as a post doc in the Newcastle Cognition Lab, both as a part of the the PMwG team and on various cognitive modelling projects. Previously, I worked as a data analyst at Hunter New England Local Health District with the FluTracking team.\nMy work with the samplers team primarily involved making advanced statistical methods accessible to a broader community. This work has resulted in the Particle Metropolis within Gibbs (PMWG) R package and bookdown. Keep an eye out for a series of upcoming blog posts at Newcastle Cognition Lab samplers team website and on my twitter.\n  Download my academic cv .\n  Download my resumé .\n","date":1635379200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1635379200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/reilly-innes/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/reilly-innes/","section":"authors","summary":"My name is Reilly Innes and I\u0026rsquo;m a Post Doc Researcher at the University of Amsterdam. I study decision making modelling in the Integrative Model-based Cognitive Neuroscience Research Unit. My work in the IMCN currently focuses on methods of joint modelling, using data from behavioural decision making tasks and fMRI to gather greater insight into the functional (and structural) relationships between brain and behaviour.","tags":null,"title":"Reilly Innes","type":"authors"},{"authors":null,"categories":["meta"],"content":"  In this blog post, I explain what I do as a cognitive modeller - from decision making modelling, to neuroscience, fitting mathematical models to human data and more!\nI thought you were a psychologist? I hear this too often, and technically, we are a type of psychologist in that we study (‘ology’) cognition (‘psych’). Cognition just means how we think and process things - by reading this now, you’re processing the information, a form of cognition! Cognition includes many processes we use daily, like memory, detecting signals and making decisions (that’s where we come in).\nOkay so you look at decision making? As a decision modeller, we look at decision making. I’ve had colleagues that have looked at all types of decision making, from the very simple decisions (“is this light red or green?”), to more complex decisions involving memory (“have you seen this image before?”), risk (“would you rather have a 90% chance at $10 or a 10% chance at $90?) and consumer preference (”would you rather chocolate by brand A or chips by brand B).\nIn all of these examples, we often collect lots of data from lots of people. So if you were to do one of our experiments, you’d probably be (bored) asked to make a lot of simple, seemingly repetitive decisions about the thing we’re investigating.\n Right, so what do you find? And why is this even important? Understanding the decisions people make can have huge consequences for a variety of things - safety, consumer choices, health and medical decisions and even voting behavior.\nSee when you make any decision, there are two key outcomes we’re interested in, and this holds for many applications. The choice you make and the speed in which you make that choice. Imagine you’re deciding on what to have for dinner, if you and your partner go back and forth over this for a long time, your partner might not be convinced by your choice. But say you made a choice instantly, this shows some pre-planning and confidence in your decision. These underlying factors that underpin both your choice and speed of making the choice (which we call response time) are what makes the topic important.\nLet’s take another, more consequential decision - deciding on the colour of traffic lights. If the light was difficult to see (late afternoon sun making it hard to make out the colours), you might take longer to make a decision, and in the worst circumstance, you may even make the wrong decision! If it’s night time and easy to see, then you’ll probably make a fast, accurate decision.\nBut that’s not all that goes into a decision. What if you’re running late, then you might be biased to see a green light and more likely to make a wrong decision. What if you’re a cautious driver? You might slow down coming to the light and take longer to commit to a decision. All these elements play a part in decision making and, the coolest part, we can disentangle them.\n How do you do this? We do this through mathematical models. See in decision making research, we have some input (i.e., the question “is this light red or green”) and we get some output (a choice and response time), but what we really want to know is what components make up that choice. We want to understand the black box that is the processing of the information to execute a decision.\nSo we call these models “process models”, because they help us understand the process of decision making. In psychological research there are lots of examples of models, from memory models that help us understand how we encode and retrieve information, to learning models that help us understand how we update our knowledge and representations of the world. Then there are decision models which help us break down response times and choices that we make.\n  Decision-Making Modelling Luckily, many smart people have worked on improving our models over time. When you think about it, there’s a few things a model should achieve. It should be;\n flexible and able to account for the phenomenon under many conditions not too complex - too much complexity means it’s difficult to understand or gain information from representative of the system and account for specific things we know about already testable (and falsifiable) able to produce data that is similar to what we observe.  These last two points are where mathematical models come in. Let’s take a model like this figure below. This model is for decisions in a task where someone is asked to say whether a string of letters is a word or a “non-word”.\nHere we can see a few features. First, there are two distributions (blue and yellow). These make up the representation of “words” and “non-words” in the persons mind. You’ll notice they overlap - this is because we probably have some instances where we’re not really sure whether an item is a word or not. Secondly, you’ll notice a red line - this is labeled the criterion. If I was to take a random string of letters (say PONK) and was to sample a point along this line, you would have some form of certainty of how much of a word this was. If you thought (correctly) that it was NOT a word, the value assigned here might be around 0 (middle of the yellow distribution). Because your sample of information falls on the left of the criterion, the model says that you would respond “non-word”. And that’s it!\nSo let’s think then, if we had more tricky words, your sample might fall closer to the criterion, so you might be more likely to make a mistake. What if you had just learned the language? Then maybe those distributions would overlap more as it’s harder to decide. In other instance, you might have already heard from your friend that 90% of the items in the test are words. Here, you’ll probably be biased to say “word”, so the model moves your criterion to the left, so there is a greater chance of responding “word” (and making errors when it’s actually a “non-word”)\nIncluding response times This model is nice and provides an explanation about mental representations of words. But what if we want to include response times? For this, we use response time models.\nThese models make a few assumptions about how we make decisions. The theory is that for each decision that we make, we gather evidence for each option. So when deciding on dinner you might be deciding whether to cook fish or cook burgers. If you have fish in the fridge, this might give you some evidence for choosing “fish”. But burgers taste good, sooo some evidence for burgers. But ultimately you think the fish is probably better for you, so you eventually choose fish.\nThis “eventually” point, where you make a decision, is just like the criterion from the last model. We call this a threshold. When you’ve gathered enough evidence and reached this point for a choice option, you execute the choice. The speed at which you gather evidence is called the drift rate. Finally, there is a bit of time that is spent encoding information and executing the decision (very very minute amounts of time around 200 miliseconds) and this is labeled “non-decision time”. All of these components can be seen below.\nIn this example, we can see another word vs non-word choice. On the left, the evidence for “word” and on the right, the evidence for deciding “non-word”. This time, our stimulus is FOLK. You’ll notice that the threshold is lower for “non-word” - this means there is some bias, so we might’ve just seen 5 non-words in a row, or there might be very few words in this test.\nSecondly, you’ll see that drift rate is faster (i.e., the slope is steeper) for “word”. This means we’ve gathered evidence faster. Drift rate is thought to reflect processing speed and urgency, so the difference between drift rates is a good indicator of how easy the task is. If the difference between the correct answer and incorrect answer is really big, then the task is easy, whereas if the difference is small, the task is probably harder (similar to our overlapping distributions in the previous example).\nNext, you’ll also notice the parts at the start and end that are added on for non-decision time. In the model, we simply subtract this time from the response time so that we can asses the time that was spent on actually processing and deciding.\nNow for the maths - and this is really quite simple! The response time is something we know (let’s say it took 600 miliseconds to make this choice). Then the response time (taking away non-decision time) is found by dividing the threshold by the drift rate (slope)! As we get more complex there’s other things to conisder too – such as how high the evidence began accumulating at (it’s about the same here).\n Speed-Accuracy Tradeoff One thing you may have noticed about decision making is that a trade off exists. You can go faster and make faster decisions, but this often means more errors. You could also try and be more accurate, but this often means slowing down. This is called the speed-accuracy tradeoff and is very well studied.\nIn our experiments, we might tell people to “be fast” sometimes, and other times to “be accurate”. The results are as you would expect - faster response times and lower accuracy for “be fast” and the opposite for “be accurate”.\nNow, with our models, we can work out what happens here, and the result is very neat. All we do is change our threshold! Looking at this example, it might not be immediately clear how this works, but here’s some more details about the model, which are shown below. The drift rate is actually a distribution, so it’s variable from one decision to the next. This means with a lower threshold, a slightly slow drift rate would lead to an arrow - something that is less likely with a high threshold (where we would wait for more evidence)!\n Modelling You might be wondering how we work out the values in these models. How can we tell whether one threshold is higher than the other? Especially with so much data for so many people!\nFor this, we use model estimation techniques and probability theory. Using all the data, we can pick out some values to include in the model - let’s say drift rate = 2, threshold = 0.5, non-decision time = 0.1. These numbers are arbitrary alone, but when combined, give us a response time distribution. The model estimation takes in these guesses, and applies them to our observed data, and then gives us a probability that these values are the true values.\nWe then repeat this process many many times, each time trying to find better guesses, until we get the most likely parameter estimates. This just means we’ve found values that are likely to result in the data we observed. And the cool part - we can then put these likely values back into the model and generate new data! If the model works, the new (predicted) data should look just like what we observed!\nFrom this process, we can test all kinds of things - the different in parameters between people, the difference over different conditions and manipulations, as well as which parameters are affected by different manipulations. For example, we know that harder tasks lead to slower drift rates! From this, we can gain an insight into the process. So when the task is harder, people are slower to process the information! And then vice-versa, if people are slower to process information, it may be too hard to perceive.\n More examples Imagine you’re selling a product on a website, then understanding the consumers behaviour could help you sell more products. If on test A, people clicked “buy” a lot quicker than in test B, then the model might conclude that the information was easier to absorb, or, maybe there was a lower price or more persuasive text that made them more confident in their decision.\nAnother interesting aspect is the difference between people. In this online shopping example, we might find that person A is just slow to respond, which is caused by being more cautious compared to other people, where the other people are just slower to take in the information. We overcome this by using hierarchical models.\n Hierarchical Models Hierarchical models have two main levels - each individual has a set of values for their model parameters, and then these help to establish the group’s parameters. In this shopping example, a big company might only care about the group level, because that is the best way to sell more products to more people. But we may also want to know about each individuals behaviour.\nThe strength of hierarchical models comes from what we call shrinkage. Let’s imagine a model that guesses peoples height. These guesses may resemble a normal distribution, just like below.\nYou’ll notice that most of the guesses are around the mean, right in the middle, which makes this distribution. You’ll also notice that two of the guesses (on the right) are a bit extreme. In hierarchical modelling, shrinkage helps to pull these extreme values back into a normal range, just like below.\nSo the important point here is that the individuals inform the group, but the group also informs the individuals. This relationship helps us get the best possible estimates for our model parameters as possible, so that we can answer really difficult research questions.\n  You mentioned neuroscience? Alright, final point. There are now some neuroscience researchers who are using these modelling techniques along with their EEG and fMRI measures! This means that they can look to see how different processes affect the brain - i.e., what parts of the brain light up when I’m being more cautious? And what happens in clinical patients who may display different behaviour? Maybe models can help find improve behavioural therapy by helping us understand the process of the behaviour!\n Reilly   ","date":1652313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652313600,"objectID":"ca5f77c715cb4349c236cb35ae36be7d","permalink":"/post/2022-05-12-decision-modellers/","publishdate":"2022-05-12T00:00:00Z","relpermalink":"/post/2022-05-12-decision-modellers/","section":"post","summary":"In this blog post, I explain what I do as a cognitive modeller - from decision making modelling, to neuroscience, fitting mathematical models to human data and more!","tags":["modelling","careers","outreach"],"title":"What do cognitive modellers do?","type":"post"},{"authors":null,"categories":["R"],"content":"  This blog post uses the pmwg package for a more generic example than has previously been shown (here for example).\nPMwG The particle metropolis within Gibbs sampling package (pmwg) was created by a team at the University of Newcastle. The package provides functions to estimate Bayesian hierarchical models. What this means is;\n Bayesian; probability based, sampling parameters to form a distribution of parameter values, not a point estimate. Gives us uncertainty\n hierarchical; multi levels - i.e., a group level and a subject level. Think of a school (the group mean) and classes within the school (each with their own mean)\n model-based sampling; propose lots of values for model parameters and find those which are probabilistically most liekly\n  So far, the package has mainly been used for psychological applications, where lots of data is collected per subject, and this data is then fit with complex models (which cannot be fit with standard methods). For an explanation of the full method see our other documentation or for the full maths, see here\n PMwG_new Recently, working together with Niek Stevenson, we’ve added functionality to the PMwG package. These are not detailed here, however, I do use a couple of these functions as they are pretty neat and usable. The additional functions can be seen here.\n Why would we use this? Well there’s a few reasons. For this example, using PMwG is probably overkill, but it makes a nice example for other situations where we might have many observations and many individuals. These bigger data problems represent more realistic scenarios, but importantly, we would want to get a sense of the group level parameter distribution, as well as the individual subjects’ data. For example, if we had a school with many classes (say 50), where each class included 50 students, then we could estimate a model for each class, which would inform the group level model.\nThe reason to use PMwG is when we have difficult estimation problems. Many models do not have analytic solutions, and so model estimation is often used to overcome this. Even with an analytic solution, doing hierarchical modelling can be challenging, which is where PMwG shows more usefulness. We could use optim to find the most likely values for a problem, but this is not reliable and only does calculations at one level (group or individual). Further, we could use another sampler (like stan or jags), however, with PMwG we can achieve greater efficiency, there is no reliance on multiple chains or convergence (due to sampler properties) and we can reliably estimate a more complex group level model – including the multivariate normal we use as default which allows us to estimate parameter correlations. The latter point is particularly useful for models with collinearity between parameters (i.e., as x goes up, so does y). I’ll do a blog in future showing just how bad post-hoc correlations between parameters are, and how PMwG overcomes this.\nSo that’s a lot of reasons to use this method, but lets summarise quickly.\n We use PMwG to estimate statistical models (often with no analytic solution). The models we would estimate are able to have their density calculated/approximated. The data we model should have many observations (differs depending on model complexity) for a few different individuals (really want at least 20 to inform the group). PMwG allows us to do hierarchical modelling. This means we have a group and individual level distribution. PMwG is Bayesian. This means we can evaluate a level of certainty of our parameter estimates for neater inference. PMwG is reliable and efficient. PMwG allows us to estimate between parameter correlations.   The Data In the following example, I use the the fastfood dataset from the openintro package. The data includes 515 item menus from 8 different fast food restaurants. Each item has 15 metrics to descirbe the nutritional value. Here we only use the first 11 metrics due to missing data (something I’ll address using PMwG in another blog post).\nlibrary(openintro) data \u0026lt;- fastfood[,1:13]  Setup First we need to source the pmwg_new code and set up the environment;\nrm(list=ls()) # library(pmwg) source(\u0026quot;pmwg/variants/standard.R\u0026quot;) library(mvtnorm) library(openintro) Next, we need to clean the data. Here i include a subject column. This is a requirement of the pmwg package - you must include a subject column. Here, the restaurant column is labelled as the subject. This is because I’m going to treat each restaurant as an individual entity, where each food item is an “observation” from their menu.\nI also remove the food item label for coding simplicity purposes. Next I’m going to omit any items with missing data, before finally rescaling four metrics for modelling purposes.\nNOTE: These four rescaled metrics don’t need to be normalised, but the values are far greater than the other measures. Here, by dividing them all by 100, I don’t have to set different priors (it’s mainly because I’m being lazy).\n names(data)[1]\u0026lt;-\u0026quot;subject\u0026quot; data \u0026lt;- data[c(1,3:13)] data \u0026lt;- na.omit(data) data[,c(2,3,7,8)]\u0026lt;- data[,c(2,3,7,8)]/100 Here, we don’t have many observations, which makes it hard to estimate. We also probably should have more restaurants, but this serves as a nice intro example\n Likelihood Function The likelihood function is the engine in PMwG. The nicest part of this engine, is that you can craft it yourself, giving lots of flexibility. The likelihood function takes the proposed parameters and calculates the likelihood of these given the subjects’ (restaurants) data and the model you’re using.\nBefore we write this though, we’re first going to check the shapes of the distributions of nutrition metrics.\n par(mfrow=c(3,4)) apply(data[,-1], 2, hist, main = NULL) Two things are immediately apparent that need to be accounted for in the likelihood. First, all values are greater than 0. Second, we can see that all distributions are positively skewed. We’re going to make sure we do something about these in the likelihood.\nFull Function Shown below is the full likelihood function, but I’ll break it down slowly in this section;\nll \u0026lt;- function(x, data, sample = F ){ x=exp(x) if(sample == FALSE){ n_pars = length(x) tmpData = as.matrix(data[,-1]) out = sum(TruncatedNormal::dtmvnorm(tmpData, x, diag(1, nrow = n_pars), lb = rep(0,n_pars), log = T, B = 1)) return(out) }else{ n_pars = length(x) Restaurant = data[,1] tmpData = as.matrix(data[,-1]) out = TruncatedNormal::rtmvnorm(nrow(tmpData), x, diag(1, nrow = n_pars), lb = rep(0,n_pars)) out \u0026lt;- as.data.frame(cbind(out,Restaurant)) return(out) } } In the likelihood function, we need to include three main arguments;\n x is the vector of proposed (named) parameter values data is the data (note that each time using this will only use data for the one subject being estimated) sample = FALSE is used to specify whether we are sampling from the model (i.e., TRUE) or calculating the density ( FALSE - set as the default)   Transformations The first thing I do in the likelihood is transform the input parameter values. Knowing that all the values in the data are positive, and therefore any proposed parameters will be positive (using a multivariate normal model), I need to ensure that the parameters are also positive.\nTaking the exponent of the proposed parameters means these will be strictly positive. This has implications for the implied prior, but I’ll discuss this in the advanced section later on.\n The Model For this example, I was considering using a multivariate normal model, but I found a truncated multivariate normal function that makes it slightly more interesting and accurate (we’ll do some model comparison below too). When thinking about the “model” we’re using, we’re talking about the model of the data describing the data for each individual.\nLooking at the group level distribution above, we can see a skewed distribution, but it’s worthwhile thinking about this distribution per individual restaurant as well. This means we make the assumption that each individual follows this model, and that informs a population level model (and can allow us to generate or predict other restaurants).\nFor the truncated multivariate model I use the dtmvnorm and rtmvnorm functions from the TruncatedNormal package for density calculation and random generation respectively.\n Calculating the Density Here I say that we calculate the likelihood of the data given some parameter values. All I’m doing here is testing the probability of parameters for each persons data. Before I show the functions, I like to do a small thought exercise to show how this works in practice.\nThought exercise for likelihoods For this example, let’s generate some data from a normal distribution. Here I make some “parameter” values which I’ll save. These parameters are just mean and standard deviation for the normal.\ntruePars = c(\u0026quot;mean\u0026quot;=6,\u0026quot;sd\u0026quot;=.3) exampleData \u0026lt;- rnorm(n=1000, mean = truePars[\u0026quot;mean\u0026quot;], sd= truePars[\u0026quot;sd\u0026quot;])  Now, using this data, I’ll try and input some other parameters to see how likely these are for the data;\n# first try sum(dnorm(exampleData,mean=0,sd=1, log = T)) ## [1] -18966.2 # a lot off, so lets shift the mean sum(dnorm(exampleData,mean=7,sd=1, log = T)) ## [1] -1460.141 # what if we move the sd sum(dnorm(exampleData,mean=6,sd=.5, log = T)) ## [1] -394.0612 # now lets try them together sum(dnorm(exampleData,mean=6.5,sd=.3, log = T)) ## [1] -1566.464 # very close sum(dnorm(exampleData,mean=8,sd=.4, log = T)) ## [1] -12754.75 # too far sum(dnorm(exampleData,mean = truePars[\u0026quot;mean\u0026quot;], sd= truePars[\u0026quot;sd\u0026quot;], log = T)) ## [1] -182.3819 # nice We can see how the probability increases as we get closer to the generating values. In PMwG, this process is taken care of under the hood. Hopefully this shows just what the algorithm is trying to do. This is also a good test for your model, where likelihood should increase with “better” test values. It’s also a good idea to do simulation and recovery to make sure your model “works”. This means generating values and then fitting the model with the goal of obtaining the generated values. If it doesn’t recover, there could be something wrong with the code, not enough data per condition/individual or the model does not recover.\n Density function The density function is shown below;\nif(sample == FALSE){ n_pars = length(x) tmpData = as.matrix(data[,-1]) out = sum(TruncatedNormal::dtmvnorm(tmpData, x, diag(1, nrow = n_pars), lb = rep(0,n_pars), log = T, B = 1)) #B=1 because otherwise it does many iterations and takes too long return(out) }  There are four steps in the getting the likelihood. First I grab the number of parameters. Second, I remove the names from the data for coding simplicity. Then I do the important part. Finally, I return a single value (the likelihood).\nFor the important part here, I take the sum of all the densities as they are on the log scale (otherwise I would multiply). Using the dtmvnorm function, I include the data, where each row is an observation and each column is a different metric. The means are given by the parameters (x). I use an sd of 1 for the covariance. I also include a lower bound of 0 for all metrics.\nImportantly, all I’m doing here is checking how well the parameters fit the data, just as in the thought exercise above. This means that the density is calculated for each data point (i.e., each observation or food item), and then multiplied together (summed here because I use logs for simplicity).\n  Random generation In a very similar sense, to generate random data, I use the inverse functionality.\nelse{ n_pars = length(x) Restaurant = data[,1] #remove the name column tmpData = as.matrix(data[,-1]) out = TruncatedNormal::rtmvnorm(nrow(tmpData), x, diag(1, nrow = n_pars), lb = rep(0,n_pars)) out \u0026lt;- as.data.frame(cbind(out,Restaurant)) #add the name column back on return(out) }  What this allows us to do is generate data, given some input parameters for a single subject.\n  PMwG Functions Now I’ve done all the setup, I just need to prepare the sampler. To do this, first we need to specify the vector of parameter names. These will be used by the sampler.\npars=names(data)[-1]  Priors Next I need to specify some priors. This can be daunting and stressful. But it really doesn’t need to be, especially with PMwG. The prior specification here is for the group level model. PMwG assumes that the group level distribution of parameters (i.e., the means for each restaurant - NOT the truncated model) are distributed according to a multivariate normal distribution. For this prior, we only need to set the mean and the variance.\nThe prior influences which parameter values are proposed. Here, I set the mean to 0 and variance to 10. Mean to 0?? Yes, because proposals come from a range of -Inf to Inf. In our likelihood we take care of these by taking the exponential of them, so really, the mean here is exp(0) = 1. I set the variance to 10 so that a wide range of values are used, but given our exponential transformation, I could probably keep this at 1.\nFor the rest of PMwG priors, we don’t need to consider the prior shapes of distributions of group level parameters as many researchers stress about with other fitting methods.\npriors \u0026lt;- list( theta_mu_mean = rep(0, length(pars)), theta_mu_var = diag(rep(10, length(pars))))   The sampler Finally, we can set up the sampler and run the algorithm. To set up the sampler, we use the pmwgs function, where we need to include data,pars,priors and ll_func. Then we can intialise the sampler.\nHere, the data must have a subject column so that the data can be split into data for each individual. The pars arguments should include any parameters used in the model to be estimated. Any prior should have a theta_mu_mean vector and a theta_mu_var matrix (mean and variance) of length pars and pars * pars respectively.\n# Create the Particle Metropolis within Gibbs sampler object ------------------ sampler \u0026lt;- pmwgs( data = data, pars = pars, prior = priors, ll_func = ll ) sampler \u0026lt;- init(sampler) # i don\u0026#39;t use any start points here  Sampling When this is done, we can finally do the sampling!! For sampling, there are three stages. The specifics of these can be seen here. the tldr version of this is;\n burn used for initial burn-in samples (the throwaway samples) to get to the posterior distribution adapt used to create the efficient distribution in sampling. This stage will stop if the efficient distribution can be created. sample efficiently sample from the posterior  Note that you don’t need to run each stage, you might just run burn-in and throw away the initial samples. However, we’ve seen significant improvements in the quality of samples from the final efficient stage.\nFor the run_stage argument, we need to include the sampler object, the stage we’re running, the number of iterations, particles and computer cores. For iterations and particles, this could take some fine tuning for the user, however, we tend to use 1000 burn-in iterations, 5000 adaptation and then however many desired samples for the final stage. We generally use either 100 or 300 particles depending on the model complexity. Here I use the pmwg_new function pstar. This adapts the “acceptance rate” - basically making the sampler more efficient. For more info on all the arguments, see here.\nIn this example, the model is simple and easy to estimate, so I dial back some of these tuning parameters. To know the best settings to use, you might start with something like below and then tinker as you see fit.\nsampled \u0026lt;- run_stage(sampler, stage = \u0026quot;burn\u0026quot;,iter = 100, particles = 100, n_cores = 4, pstar=.6) #epsion will set automatically to 0.5 sampled \u0026lt;- run_stage(sampled, stage = \u0026quot;adapt\u0026quot;,iter = 500, particles = 100, n_cores = 4, pstar=.6) sampled \u0026lt;- run_stage(sampled, stage = \u0026quot;sample\u0026quot;,iter = 100, particles = 50, n_cores = 4, pstar=.6)  NOTE: This will take a few minutes to run. Increasing the number of particles or iterations will slow the sampler down. However, we need to reach the posterior (i.e., the good values for the parameters), and to do this we need good information. This means that we may need more particles or iterations. Ideally, for efficiency, we should aim for between 30 and 80% acceptance rate. Here, pstar helps as I can specify a desired acceptance rate.\n  Output Once our sampler is finished, we can check the output.\nChecking First, we should check that we are in the posterior. To do this, we can check the chain plots of the samples. Here we show two plots. The first is the group level parameter chains and the second is the individual subject likelihood chains. For the parameter chains, we’re looking for “stationarity” in the samples. This means that the sampled values will show a flat line with a bit of wiggle (like a hairy caterpillar). For the subject likelihood, we’re looking for flat lines as well, however, we should see an increase in these from the start to the end. These will be less wiggly. See here for more details on this output.\ntmp \u0026lt;- sampled matplot(t(tmp$samples$theta_mu),type=\u0026quot;l\u0026quot;) matplot(t(tmp$samples$subj_ll),type=\u0026quot;l\u0026quot;) Here we can see that some parameters only reach the posterior in the final sampling stage (x-axis is iterations of sampling). We can see however, that stationarity is reached. Secondly, we can see that the likelihood increases and then stabilizes at the highest value for each restaurant.\n Posterior Inference Next, we can look at the sampled values. Here I show group level and inidvidual restaurant level parameter values. Remember that by model sampling, we generate distributions of possible parameter values, so here, I take the median of these values. I also transform these as I did in the likelihood.\ngroupMedian \u0026lt;- apply(tmp$samples$theta_mu[,tmp$samples$stage==\u0026quot;sample\u0026quot;],1,median) round(exp(groupMedian),3) ## calories cal_fat total_fat sat_fat trans_fat cholesterol ## 4.842 0.001 23.322 6.782 0.000 0.816 ## sodium total_carb fiber sugar protein ## 12.661 41.485 3.470 6.421 28.770 restaurantMedian \u0026lt;- apply(tmp$samples$alpha[,,tmp$samples$stage==\u0026quot;sample\u0026quot;],1:2, median) round(exp(restaurantMedian),3) ## Mcdonalds Chick Fil-A Sonic Arbys Burger King Dairy Queen Subway ## calories 5.904 3.870 5.600 4.872 5.672 5.107 4.952 ## cal_fat 0.000 0.006 0.000 0.006 0.000 0.003 0.000 ## total_fat 31.947 14.475 37.683 26.708 34.755 29.038 18.584 ## sat_fat 8.204 3.186 10.982 8.050 10.414 10.415 5.950 ## trans_fat 0.000 0.000 0.000 0.004 0.000 0.000 0.002 ## cholesterol 1.541 0.596 1.180 0.900 0.884 0.819 0.737 ## sodium 14.550 10.735 13.711 15.157 11.673 11.413 12.802 ## total_carb 48.858 25.488 47.153 44.797 37.188 38.741 54.857 ## fiber 3.394 2.687 2.972 3.022 2.358 3.418 6.606 ## sugar 11.041 3.834 6.320 7.477 7.713 6.219 10.094 ## protein 40.386 30.841 29.280 29.165 29.285 24.947 30.244 ## Taco Bell ## calories 4.597 ## cal_fat 0.000 ## total_fat 20.861 ## sat_fat 6.409 ## trans_fat 0.000 ## cholesterol 0.506 ## sodium 10.164 ## total_carb 46.599 ## fiber 5.707 ## sugar 3.721 ## protein 17.390 We can see the mean values overall, but that’s not overly meaningful. The more meaningful estimates are at the individual level.\nWe can also look at the distribution of the group parameter values and see how restaurants compare to the group distribution. I’ve only shown two parameters here, but this is a nice example.\nlibrary(tidyr) library(ggplot2) eg \u0026lt;- apply(tmp$samples$alpha[c(1,10),,tmp$samples$stage==\u0026quot;sample\u0026quot;],1:2, median) eg\u0026lt;-round(exp(eg),3) group \u0026lt;- exp(tmp$samples$theta_mu[c(1,10),tmp$samples$stage==\u0026quot;sample\u0026quot;]) group \u0026lt;- as.data.frame(t(group)) eg \u0026lt;- as.data.frame(t(eg)) eg$restaurant \u0026lt;- row.names(eg) group \u0026lt;- pivot_longer(group, cols = everything(), names_to = \u0026quot;metric\u0026quot;, values_to = \u0026quot;value\u0026quot;) eg \u0026lt;- pivot_longer(eg, cols = c(\u0026quot;calories\u0026quot;,\u0026quot;sugar\u0026quot;), names_to = \u0026quot;metric\u0026quot;, values_to = \u0026quot;value\u0026quot;) # add vertical mean line to density plot with geom_vline() ggplot(group, aes(x=value)) + geom_density( fill=\u0026quot;dodgerblue\u0026quot;, alpha=0.5)+ geom_vline(data=eg,aes(xintercept=value,colour = restaurant), size=.5)+ geom_text(data=eg,aes(x=value,label = restaurant,y=1), angle=90)+ theme_bw()+ facet_grid(~metric, scales = \u0026quot;free\u0026quot;) Anything unexpected here?\n Correlations Finally, one strength of PMwG is that it assumes a group level multivariate model, with variance given by a covariance matrix. This means we can see correlations between parameters of the model - something that can be very useful with models with multiple components. An example of this is shown below;\ncov\u0026lt;-apply(tmp$samples$theta_var[,,tmp$samples$stage==\u0026quot;sample\u0026quot;],1:2, median) colnames(cov)\u0026lt;-tmp$par_names rownames(cov)\u0026lt;-tmp$par_names cor\u0026lt;-cov2cor(cov) #correlation matrix library(corrplot) ## corrplot 0.84 loaded corrplot(cor, method=\u0026quot;circle\u0026quot;, type = \u0026quot;lower\u0026quot;, title = \u0026quot;Parameter Correlations\u0026quot;, tl.col = \u0026quot;black\u0026quot;,mar=c(0,0,2,0))   Model Comparison Let’s say we wanted to compare our model (the truncated normal) to something else (standard multivariate normal). How could we do this??\nFirst, let’s write the likelihood function;\nll \u0026lt;- function(x, data, sample = F ){ x=exp(x) if(sample == FALSE){ n_pars = length(x) tmpData = as.matrix(data[,-1]) out = sum(mvtnorm::dmvnorm(tmpData, x, diag(1, nrow = n_pars), log = T)) # only change is here to the model return(out) }else{ n_pars = length(x) Restaurant = data[,1] tmpData = as.matrix(data[,-1]) out = mvtnorm::rmvnorm(nrow(tmpData), x, diag(1, nrow = n_pars), log = T) out \u0026lt;- as.data.frame(cbind(out,Restaurant)) return(out) } } I ran this model using the exact same code (apart from the likelihood), but I’ve included it below;\nrm(list=ls()) # library(pmwg) source(\u0026quot;pmwg/variants/standard.R\u0026quot;) library(TruncatedNormal) library(openintro) data \u0026lt;- fastfood[,1:13] names(data)[1]\u0026lt;-\u0026quot;subject\u0026quot; data \u0026lt;- data[c(1,3:13)] data \u0026lt;- na.omit(data) data[,c(2,3,7,8)]\u0026lt;- data[,c(2,3,7,8)]/100 ## plots par(mfrow=c(4,3)) apply(data[,-1], 2, hist, main = NULL) dev.off() ### likelihood ll \u0026lt;- function(x, data, sample = F ){ x=exp(x) if(sample == FALSE){ n_pars = length(x) tmpData = as.matrix(data[,-1]) out = sum(mvtnorm::dmvnorm(tmpData, x, diag(1, nrow = n_pars), log = T)) # only change is here to the model return(out) }else{ n_pars = length(x) Restaurant = data[,1] tmpData = as.matrix(data[,-1]) out = mvtnorm::rmvnorm(nrow(tmpData), x, diag(1, nrow = n_pars), log = T) out \u0026lt;- as.data.frame(cbind(out,Restaurant)) return(out) } } pars=names(data)[-1] priors \u0026lt;- list( theta_mu_mean = rep(0, length(pars)), theta_mu_var = diag(rep(10, length(pars)))) # Create the Particle Metropolis within Gibbs sampler object ------------------ sampler \u0026lt;- pmwgs( data = data, pars = pars, prior = priors, ll_func = ll ) sampler \u0026lt;- init(sampler) # i don\u0026#39;t use any start points here sampled \u0026lt;- run_stage(sampler, stage = \u0026quot;burn\u0026quot;,iter = 100, particles = 100, n_cores = 4, pstar=.6) #epsion will set automatically to 0.5 sampled \u0026lt;- run_stage(sampled, stage = \u0026quot;adapt\u0026quot;,iter = 500, particles = 100, n_cores = 4, pstar=.6) sampled \u0026lt;- run_stage(sampled, stage = \u0026quot;sample\u0026quot;,iter = 200, particles = 50, n_cores = 4, pstar=.6)  Now we have our two sampled objects, we can do model comparison. For some advanced model comparison methods, see here. For now though, we’re going to compare using a criterion method. Here, the smaller value represents the more likely model. The function for the deviance information criterion (DIC) is below\npmwg.DIC=function(sampled,pD=FALSE){ nsubj=length(unique(sampled$data$subject)) # the mean likelihood of the overall (sampled-stage) model, separately for each subject mean.like \u0026lt;- apply(sampled$samples$subj_ll[,sampled$samples$stage==\u0026quot;sample\u0026quot;],1,mean) # the mean of each parameter across iterations. Keep dimensions for parameters and subjects mean.params \u0026lt;- t(apply(sampled$samples$alpha[,,sampled$samples$stage==\u0026quot;sample\u0026quot;],1:2,mean)) # i name mean.params here so it can be used by the log_like function colnames(mean.params)\u0026lt;-sampled$par_names # log-likelihood for each subject using their mean parameter vector mean.params.like \u0026lt;- numeric(ncol(mean.params)) data \u0026lt;- transform(sampled$data, subject=match(subject, unique(subject))) for (j in 1:nsubj) { mean.params.like[j] \u0026lt;- sampled$ll_func(mean.params[j,], data=data[data$subject==j,], sample=FALSE) } # Effective number of parameters pD \u0026lt;- sum(-2*mean.like + 2*mean.params.like) # Deviance Information Criterion DIC \u0026lt;- sum(-4*mean.like + 2*mean.params.like) if (pD){ return(c(\u0026quot;DIC\u0026quot;=DIC,\u0026quot;effective parameters\u0026quot;=pD)) }else{ return(DIC) } }  Now all we need to do is load our two sampled objects and compare!\n## Warning in -2 * mean.like + 2 * mean.params.like: longer object length is not a ## multiple of shorter object length ## Warning in -4 * mean.like + 2 * mean.params.like: longer object length is not a ## multiple of shorter object length ## DIC effective parameters ## 1105994.4 231328.4 ## Warning in -2 * mean.like + 2 * mean.params.like: longer object length is not a ## multiple of shorter object length ## Warning in -2 * mean.like + 2 * mean.params.like: longer object length is not a ## multiple of shorter object length ## DIC effective parameters ## 1115781.3 231675.7 # First load in the sampled object for the truncated multivariate normal model load(\u0026quot;fastFood_samples_tmvn.RData\u0026quot;) # test pmwg.DIC(sampled) # First load in the sampled object for the truncated multivariate normal model load(\u0026quot;fastFood_samples_mvn.RData\u0026quot;) # test pmwg.DIC(sampled)  Evidently, we can see that the smaller DIC value was for our truncated normal model - so we made a good choice picking this one first ;)\n Final Word In this data set, there are only 8 restaurants. Usually for a hierarchical model, we would want at least 20 individuals. Additionally, there are very few observations per restaurant. Similar to the group level, we want a well informed individual level model. The model still works, but because of the low amount of data, there is greater uncertainty.\nSo it’s not the best data set, but hey, it makes for a nice example. I’ve also been brief with a few explanations on specifics, the algorithm and some function arguments, but this is for brevity (and because they’re generally not super important). If you want to know more about specifics, please see the bookdown. Hopefully for users outside of psychology (or even outside of decision making modelling), this example can help contextualize the bookdown.\nHappy coding!\nReilly\n ","date":1651276800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651276800,"objectID":"68cb442db5e4464ce4343dd096c7be6f","permalink":"/post/2022-04-30-pmwg-fastfood/","publishdate":"2022-04-30T00:00:00Z","relpermalink":"/post/2022-04-30-pmwg-fastfood/","section":"post","summary":"This blog post uses the pmwg package for a more generic example than has previously been shown (here for example).\nPMwG The particle metropolis within Gibbs sampling package (pmwg) was created by a team at the University of Newcastle.","tags":["pmwg","modelling"],"title":"Using PMwG for NOT psychology","type":"post"},{"authors":["Max C.Keuken","Anneke Alkemade","Niek Stevenson","Reilly Innes","Birte U.Forstmann"],"categories":null,"content":"","date":1635379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635379200,"objectID":"ccb32a3558e01c9ca2e927460cc54c7a","permalink":"/publication/keuken2021/","publishdate":"2021-10-28T00:00:00Z","relpermalink":"/publication/keuken2021/","section":"publication","summary":"Deep Brain Stimulation (DBS) is an effective neurosurgical treatment to alleviate motor symptoms of advanced Parkinson’s disease. Due to its potential, DBS usage is rapidly expanding to target a large number of brain regions to treat a wide range of diseases and neuropsychiatric disorders. The identification and validation of new target regions heavily rely on the insights gained from rodent and primate models. Here we present a large-scale automatic meta-analysis in which the structure-function associations within and between species are compared for 21 DBS targets in humans. The results indicate that the structure-function association for the majority of the 21 included subcortical areas were conserved cross-species. A subset of structures showed overlapping functional association. This can potentially be attributed to shared brain networks and might explain why multiple brain areas are targeted for the same disease or neuropsychiatric disorder.","tags":null,"title":"Structure-function similarities in deep brain stimulation targets cross-species","type":"publication"},{"authors":["Zachary Howard","Reilly Innes","Ami Eidels","Shayne Loft"],"categories":null,"content":"Supplementary material can be found here.\n","date":1624320000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624320000,"objectID":"ed188668ce2d69d12707a2c9f9af2d89","permalink":"/publication/howard2021/","publishdate":"2021-06-22T00:00:00Z","relpermalink":"/publication/howard2021/","section":"publication","summary":"Cognitive workload is assumed to influence performance due to resource competition. However, there is a lack of evidence for a direct relationship between changes in workload within an individual over time and changes in that individual’s performance. We collected performance data using a multiple object-tracking task in which we measured workload objectively in real-time using a modified detection response task. Using a multi-level Bayesian model controlling for task difficulty and past performance, we found strong evidence that workload both during and preceding a tracking trial was predictive of performance, such that higher workload led to poorer performance. These negative workload-performance relationships were remarkably consistent across individuals. Importantly, we demonstrate that fluctuations in workload independent from the task demands accounted for significant performance variation. The outcomes have implications for designing real-time adaptive systems to proactively mitigate human performance decrements, but also highlight the pervasive influence of cognitive workload more generally.","tags":null,"title":"Using Past and Present Indicators of Human Workload to Explain Variance in Human Performance","type":"publication"},{"authors":["Reilly Innes","Juanita Todd","Scott Brown"],"categories":null,"content":"   ","date":1623762000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623762000,"objectID":"431f55ff269512acaf68ef1a7cbb0a48","permalink":"/talk/mathpsyc2021/","publishdate":"2021-06-15T00:00:00Z","relpermalink":"/talk/mathpsyc2021/","section":"talk","summary":"Modelling the distraction task using the LBA and neural covariates","tags":[],"title":"Modelling the distraction task using the LBA and neural covariates","type":"talk"},{"authors":null,"categories":["R"],"content":"  A blog post to help with writing log likelihood functions to be used in PMwG. The examples shown in the PMwG Sampler Doc are quite specific, and so this blog aims to outline the general process of constructing a likelihood function. This blog is probably most useful for anyone who is looking for more info on how to make these functions for different RT models, or any general probability based hierarchical models. Probability based modelling involves working out the probability of some data given a model (and certain model parameter values). This could be evidence accumulation models, signal detection theory models, categorisation models and many, many more.\nTo get started with PMwG, you’ll need some data - which can be any shape and can take any values, as long as there is a subject column which identifies the separate subjects - and a likelihood function - which calculates the likelihood of data given some parameter values. The data can be small or large, but should have multiple entries per person. Further, the data should be able to be split be subject column (i.e. subject_i \u0026lt;- data[data$subject==i,]) and will link closely to the structure of the likelihood function. For example, in the SDT example in Chapter 2, the data for the fast likelihood shows 16 possible values for each participant, which counts the number of responses in each cell of the design. In the likelihood function in Chapter 2, probability is calculated for each response type and is multiplied by the number of these responses (from the column n). This shows that the data can take any form, as long as it is accounted for in the function.\nWriting Log-Likelihood Functions Overview PMwG is a Bayesian hierarchical model sampling method proposed by Gunawan et al., (2020). In our PMwG sampler documentation, there are several examples of (and countless references to) writing your log likelihood function. For anyone new to modelling, or just new to this style of modelling, this step is the equivalent of STAN and JAGS model.text files. The way these operate though is vastly different.\nThe main purpose of the log likelihood function, which I’ll refer to as LL from here on, is to return a single logged likelihood value. This value can be thought of as a probability, or marginal likelihood, of some data given some model parameters under this model. If this stuff and modelling is all pretty familiar to you, you can skip straight to the “How to Make Your LL function” section.\nNOTE: For people new to this kind of modelling, by sampling, here I mean “training” your model on the data. This is done in a Markov Chain Monte-Carlo (MCMC) kind of way for PMwG, where values are selected when they are good on each iteration, until the model is trained to be in the best place. We call it “sampling” as we are sampling the parameter space (i.e. all possible values of parameters), of which we need many plausible values at the end so that we can process these in post (i.e. essentially if we only get one “sample” of parameters, we restrict ourselves, sampling lots of good values allows us to see the variance and do more accurate posterior calculations). Later on, after the model is trained, we may do anpother type of sampling where we sample “posterior predictive data” - i.e. we use the trained model parameters to generate data. For this second kind of sampling, I’ll try and refer to it as “generating”.\nUsually, for data, we can easily return the density of a value given some input. For example, if I’d like to know the probability of a response coming from a certain normal distribution, i could do;\nvalue \u0026lt;- 3 m \u0026lt;- 4 #mean s \u0026lt;- 0.5 #standard deviation dnorm(x=value, mean=m,sd=s, log=FALSE) #density function ## [1] 0.1079819 Which returns a probability value. Plotting this shows us a little bit more;\nggplot(data = data.frame(x = c(2, 6)), aes(x)) + stat_function(fun = dnorm, n = 101, args = list(mean = 4, sd = 0.5)) + ylab(\u0026quot;\u0026quot;) + theme_bw()+geom_vline(xintercept=3, colour=\u0026quot;red\u0026quot;) And so this looks about right. There’s a 10% chance this response is under this distribution. So now we know that we can get the density of data for values. If i change log to TRUE, i get the log likelihood for these parameters and the data point I have. Lets see what this would look like;\n dnorm(x=value, mean=m,sd=s, log=TRUE) ## [1] -2.225791 For PMwG we need to return the log-likelihood so that the values are useful to the algorithm. This also means we can add values together rather than multiplying probabilities together - and this protects against tiny values.\nIn a modelling scenario, we would repeat this density calculation many times (i.e. for each data point). But what if there were two conditions, which differed for the mean of the distribution? The distributions might be hypothesized to look like this for example;\nNow we can check different data points for different conditions. Lets imagine a person’s responses in condition 1 were 4, 3 and 3.5 and in condition 2 were 2, 2.1 and 4. Using the same method as above, we can calculate the probability of these responses (data) given some parameters (here, the parameters are the mean and standard deviation, but we only vary the mean).\nvalues_1 \u0026lt;- c(4,3,3.5) #data for condition 1 values_2 \u0026lt;- c(2,2.1,4) # data for condition 2 m1 \u0026lt;- 4 m2 \u0026lt;- 2 s \u0026lt;- 0.5 cond1 \u0026lt;- dnorm(x=values_1, mean=m1,sd=s, log=FALSE) cond2 \u0026lt;- dnorm(x=values_2, mean=m2,sd=s, log=FALSE) cond1 ## [1] 0.7978846 0.1079819 0.4839414 cond2 ## [1] 0.7978845608 0.7820853880 0.0002676605 So we can see here that the two means we’ve proposed are pretty good guesses for the probability of these responses. In the PMwG algorithm, each particle proposes new guesses for the parameters for each subjects’ data. These values are then input into the model (similar to above), and then the sum of these logged values is returned. The particle with the highest log likelihood is chosen on each iteration so that eventually, the best parameter values are chosen for each subjects’ data.\nThe model above is a model that assumes that responses come from normal distributions which may differ across conditions. If there was no difference in conditions, then PMwG would likely return equivalent mean parameter values (m1 and m2). This is a simple model, and so there is much more complexity to add before reaching a full drift diffusion or LBA model. In the next section I outline some considerations for modelling and then steps to creating your likelihood function.\nLinking Models and LLs It’s important to note here about how LLs and models link. You may think the LL is the model, and it kind of is. We use the LL function to estimate the likelihood under the model - that’s pretty clear from above. So how do I fit my model in? Well this comes in the call to the density function (like dnorm or dLBA). Many density functions for probability models already exist in a variety of packages. These functions are often detailed in papers, and so if a density function does not exist, you can still easily calculate the density for models given these equations in papers – although it is likely someone has already done this for you. If a density function does exist, this is usually as easy as inputting the observations (data) and the parameter values. For some examples of this see the rtdists package\n  What’s the purpose of modelling your data? The first question to ask is “what’s the purpose of modelling my data?” or “what should the modelling tell me about the data?”. There are many different flavours of answers to this question which should be considered from the outset of any experiment/modelling exercise. These ‘flavours’ fall into several main categories.\n selective influence model comparison comparing groups finding the ‘best’ model  Selective influence Selective influence is a manipulation used to check whether a parameter maps to the appropriate parameter. For example, in a model where a parameter was thought to map to memory strength, then values for this parameter should be higher in conditions where memories are stronger or more deeply encoded (and not other parameters of the model). This is a type of experiment we do to test the validity and reliability of our models to make sure that they capture the effects we are interested in.\n Model comparison Model comparison is a highly common practice in statistical methods which involves comparing different model accounts of the data. In the PMwG documentation, we briefly touch on how to compare models and different comparison methods. For the purposes of writing an LL however, this often involves writing several different LLs which vary (in parameters or models used) and running them separately. The results from these are then compared in model comparison methods. An example of a model comparison could be from the above example, where we might compare the descriptive adequacy of normal distributions to truncated normal, uniform distributions, log normal and exponential distributions to decide which best describes the data.\n Comparing groups Comparing groups is highly important for psychology and other sciences, especially in modelling exercises, as we may wish to discover how groups differ in underlying parameters or even if they differ. There are many varying ways of doing this. In PMwG we can do between-subjects model fits or we may run the models separately for the different data sets and compare parameter outputs.\n Finding the ‘best’ model Similar to model comparison, finding the best model is often done when researchers aim to describe certain behavior or data. In finding the best model, one may avoid fitting the full model space so as to reduce the complexity (and not limit flexibility) of models. After this, complexity may be added in to capture more of the data. This is often done by fitting a model, checking the model to see how well it did, and then adding in parameters or complexity to see if this can help improve the model fit. This can also grow into a model compariosn exercise, where the models to compare do not cover the full model space.\n So what’s the point? Evidently, there are many different methods for modelling that achieve different outcomes, and this can influence how we construct our LLs. For now though, we’re going to stick with the “finding the best model” approach.\n  Which model will capture the effects? So now you have the purpose of your model, you need to ask what kind of model will capture the effects. In most of the research I do, I’m working with response times and decision making, so I work with evidence accumulation models. For these models, I’m lucky because the density functions are already written. For many applications this is the case, although in some, you may have to put in some extra effort to write your own density functions.\nThe effects you’re looking at are important for considering what models you’ll use. Being in your field, you’ll probably already know an array of different models that exist and are evidence based that you will want to fit. But for now, lets go through some key considerations.\nWhat type of model is needed to answer the purpose? In the example I gave above about model comparison, I talked about comparing different distributions. For the purpose of this question (i.e. what distribution do these responses come from), then these models (i.e. different distributions) answer the purpose. For a question where I wanted to know where people set their mean and sd given I think the distribution is normal - this would require only a normal distribution and looking at the PMwG parameter estimates.\nIn an evidence accumulation model, I may want to know about start point variability or within trial drift variability, and so it is better for me to use the DDM rather than LBA, as these parameters are not evaluated by the LBA. Put simply, make sure the model you use answers the purpose you set out to achieve.\n Is this model flexible for my effects? Secondly, it’s important to consider the flexibility of the model and whether the model will allow you to capture effects. For example, fitting an exponential curve to data could really restrict the space I’m sampling (i.e. positive only and exponential values), and may bias models towards inflated values which don’t capture the true tails of the distributions of my responses. Obviously this is an extreme example, but this needs to be considered when planning which model to use.\n Is your data suitable? The next consideration is with your data. This should probably come earlier, or even before the experiment has been done. Remembering that PMwG is for hierarchical modelling, which means we get both group and individual level parameter estimates. If we wanted to estimate means and standard deviations for the top example, this would work fine - we could collect lots of data from a large sample and then see how individuals set their means (and sd’s) for the two conditions. This would give us both individual estimates, but also will allow us to see group level estimates.\nHowever, if we take only one data point per person and randomly allocate people to conditions, this causes trouble. First of all, at the individual level, we will not have enough data to fit models. Secondly, for the m1 and m2 parameters, people would be missing values, and so this estimated value would revert to the prior, thus being uninformative at individual subject level and less useful at the group level. Evidently, we need to collect enough data to fit our models, but also consider our experimental manipulations - ensuring we have enough data in each cell of the design. This extends to the type of design, where between-subjects effects become difficult to account for. This can be done, but experimental rigor is sacrificed as it either requires separate fitting or uninformed random effects.\n What will the models priors be? Finally, it’s important to consider priors. PMwG uses a multivariate normal prior (and Huang and Wand’s (2013) prior on the variance). This isn’t super important for using the sampler, but is important to the methodology. What’s important for using the sampler, is that you specify the prior mean and mean variance (remembering this is on the real number line). For most models we do, we generally set the mean at 0 and the variance at 1, but this may be too restrictive for some models. Further, you should consider parameter transformations you may undertake and how these parameters will appear before they are transformed. For example we usually take the exponenital of input parameters in LBA modelling to ensure they’re strictly positive, this means the exp of 0 is 1, and such our prior within the model is 1. This is also the case for variance, where the exponential of -1 is 0.37 and the exponential of 1 is 2.72 which would be the case for one standard deviation either side of the mean in this example.\nAn example of these priors are shown below;\n priors \u0026lt;- list( theta_mu_mean = rep(0, length(pars)), theta_mu_var = diag(rep(3, length(pars))))    What are the important effects? So we now know our purpose and we’ve chosen a model to fit. Next things to consider are the effects. Most of the time, you’ll know the effects you’re interested in because these will form the research questions and will be evident from descriptive statistics. These are effects like condition 1 vs condition 2 from our top example, but could also be related to stimuli, responses and more. Lets go through some key considerations when designing the LL and the experiment.\nExperimental manipulations?  What were the manipulations in the experiment? Were these between or within subjects? What would we actually be looking for in the data (i.e. differences in response times, accuracy, choice proportions etc)?   How do manipulations manifest? From these experimental manipulations, we need to consider how these might manifest in the cognitive model. In a cognitive model, the parameters relate to underlying cognitive processes or representations. Consequently, we should propose models, and parameterisations, that relate to our experimental conditions. This might mean that in a 3 parameter model for a 3 condition experiment, we could have up to 9 parameters (or more if there was added model complexity), where for each condition, the model has 3 unique parameters. We also might hypothesize that in the 3 parameter model for the 3 condition experiment, that 1 parameter is fixed across conditions, while the other two vary. Comparing the 9 parameter model to the 7 parameter model (and other combinations) becomes a model comparison question.\nWorking out how these manipulations may manifest is highly important in modelling – both going forward and looking back. For example, when planning an experiment that involves modelling, one should consider the type of modelling question and the best experimental manipulation to implement to answer this question. When planning LLs, one should consider how the experimental manipulation would logically, and rationally, map to parameters. This is not only important for the model, but is vital for drawing clear conclusions.\n Group differences It’s also important to consider potential group differences when writing LLs. Where group differences exist, we may see the model under-fitting the data, however, including these effects in the LL could lead to uninformed cells in the model. If this is the case, it may be best to run the model separately for the different groups (but this method also has a weakness in that the fits are not informed by one another).\n Important effects? Are these present in descriptives? What does the literature say? If we do expect to see effects of manipulations in the model, we should first check if the effects of manipulations can be seen in the descriptives. It’s important that the manipulation actually worked before we start explaining how/why it worked. This can also be useful to see main and interaction effects which could be important to the model.\n  Considerations of writing the LL Finally, we’ve made it to the LL function! You’ve now thought about all components of your data and your experiment (probably). But, before writing this, lets take some time for final considerations.\nConsider the stimuli and responses When considering your stimuli and responses, consider how these may vary across conditions, how these might vary with response types/proportions/biases and what effects these may have on the underlying cognitive process. It is often the case that certain stimuli or responses can lead to inconsistencies in the model (and in the descriptives). Consequently, consider doing a deep dive into the descriptives first up to check for any weirdness - then decide what to do with these. It might also be an idea to run a full model (in addition to other models) to ensure nothing strange is going on in certain conditions or with certain parameters.\n Consider your data Following on from the above, when doing a deep dive into the data, consider cut points for outliers and standardization. As PMwG fits hierarchical models, it is often unlikely that corrections are required (such as logging RTs or minimizing the impacts of extreme data), however, occasionally data may need to be removed, such as for bad participants or lapses of attention (where these aren’t fit by the model).\n  How to make your LL In this section, I’ll detail the methods I generally use to make my LL function, and in the next section, I’ll test it. There are six main steps to making a LL.\nPrepare the parameters to be used Ensure no bad parameter values Map parameters to conditions Make the ‘sample’ component Make the ‘likelihood’ component Return  PMwG samples random values on the real line for the parameters of the model. These values are taken from a distribution of values centered on the group mean or the individual mean, with some variance, in a multivariate normal distribution. So for example, for 3 true parameter values in a model (say -1, 2 and 9), PMwG will propose 3 values as a particle (where we have n particles) for n iterations, constantly returning the most likely combination of parameters. On iteration 1, three values are selected at random (say 0,0 and 0) and a likelihood is returned. On the second step, n particles more values will be proposed using the previous particle as a reference point (from both the group and individuals previous particle) and the winner will be selected (say 0,1 and 5). This process happens over and over until the posterior space is reached.\nPrepare the parameters to be used As PMwG returns values on the real number line, we first have to prepare our parameters to be used properly in the function. This may require corrections, such as taking the exponent of these values (as is done by Gunawan et al., 2020, JMP), converting a number to between 0 and 1 (for probability parameters), or may require a mathematical function, for example, certain parameters may become 1/parameter. This should be done early in the LL.\nNOTE: if there are any transformations or corrections carried out, it is important to remember and account for these when evaluating the output and generating posterior predictive data.\n ll \u0026lt;- function(x, data, sample=FALSE){ x = exp(x) In this example, and those shown in the sampler documentation, we use an exponential transformation. This is to ensure parameter values are always greater than 0 (as the model doesn’t work if values are below 0). Evidence for this transformation comes from the main PMwG paper by David Gunawan and colleagues (Gunawan, et al., 2020, JMP). It is not always necessary to carry out transformations, however, it is important to consult the literature for advice on general practice (especially keeping in mind that PMwG samples from the real line).\n Ensure no bad parameter values Secondly, to save computation time and protect against bad values, I often include code to exclude ‘bad’ parameters. For example, if specific parameters should be non-negative and a negative value is proposed from the PMwG algorithm, the LL automatically returns -1e10. Example of this is included below.\n if (!sample){ if (any(data$RT \u0026lt; x[\u0026quot;t0\u0026quot;])) { ### for this example, i need to make sure t0 is not below any RT, if there are RTs below t0, this is bad for the model return(-1e10) } if (x[\u0026quot;b\u0026quot;] \u0026lt;= any(abs(c(x[\u0026quot;b.1\u0026quot;],x[\u0026quot;b.2\u0026quot;],x[\u0026quot;b.3\u0026quot;])))) { # for this example, i need to add b.n to b, and so if there are negative b.n\u0026#39;s this would make b below 0, which again is bad for the model return(-1e10) } } Here, x is the parameter vector, and the parameters have names such as “b”, “t0”, “b.1” etc. To refer to parameters, I use x[“b”] naming convention. You can also refer to these numerically, such as x[1], however, this is risky and can lead to potential errors. Naming is safer.\nI only use this when sample = FALSE, as this can mess up posterior predictive sampling.\nIt’s important that if the density function cannot handle specific values (i.e. dLBA can’t work with negative values), then this safety check should be put in, so that NAs aren’t returned (which would break the sampling).\n Map parameters to conditions Next up, I need to map parameters to conditions. I referred to this earlier when discussing experimental manipulation effects on the data/model. When I refer to mapping parameters to conditions, this means that parameters which refer to conditions are associated with the correct data. i.e. parameter “b.1” is associated with condition 1 and so fourth. All this step does is ensure that the correct parameter values are input into the model for each data point evaluated.\nNOTE: In process models, parameters, like A and B used in the examples below, often refer to specific components. For example, in cognitive modelling, parameter A may refer to the individuals bias in responding and parameter B could refer to the sensitivity of responding. Many models used in cognitive modelling link to specific cognitive processes or structures, and links to these processes are made through selective influence studies. There is often a significant amount of literature justifying parameters in a model and underlying processes these relate to.\nWhen writing your LL you could do parameter:condition associations line by line, and I recommend this at first, or through vectorising or other methods. The line by line method is slow, but also more accurate. Later on you can move to more advanced methods which quickly loops over conditions. Note that the more calls you make to the density function, and the more loops in the function, lead to more time.\nI’ve included two examples below of mapping parameters to conditions, in both slow and fast ways. The slow way is computationally inefficient, but is quite safe. The fast way is fast, but risks parameters not mapping as effectively. You should write your function in both ways, then compare them to ensure the output is the same and correctly mapping. For more info on this, see here\nSlow  parameter.b = parameter.A = numeric(nrow(data)) # first i create a vector to store the values in parameter.A = x[\u0026quot;A\u0026quot;] #here A is the same across conditions, so i store A in here for (i in 1: nrow(data)){ #here i loop across rows to see which condition the data relates to and store the appropriate parameter for that row - either b.1 or b.2 if(data$condition[i]==1) parameter.b[i] \u0026lt;- x[\u0026quot;b.1\u0026quot;] } else if(data$condition[i]==2) { parameter.b[i]\u0026lt;- x[\u0026quot;b.2\u0026quot;] }   Fast  parameter.b = parameter.A = numeric(nrow(data)) # first i create a vector to store the values in parameter.A \u0026lt;- x[\u0026quot;A\u0026quot;] #here A is the sham across conditions, so i store A in here parameter.b \u0026lt;- x[c(\u0026quot;b1\u0026quot;, \u0026quot;b2\u0026quot;)][data$condition] #here i do the same as above, in a faster way.  Evidently, there are many ways of doing this to make this operation more computationally efficient. You may also find you need to call the density function several times (i.e. density for one subset of data, and then for another or even in a line by line way), however, the less calls the better (faster). Also, you may find that you have main and interaction effects in the model, these are fine too, and there are many ways of implementing this, but need to be closely checked. Examples of this can be seen below.\nIn sum, for each trial, we need to ensure that the call to the density function is referring to the correct parameter values. \n  main and interaction effects In this example code snippet below, I show slow and fast ways for mapping out parameters across conditions. Here, this model includes all types of effects (similar to that in an ANOVA), where each cell of a design contains a parameter. This could also only look at main effects (i.e. only b.1 and b.2 and only slow and fast without having the interaction) or could include some interactions in addition to the main effect. This becomes particularly important in designs with more experimental factors.\nSlow  parameter.b = parameter.A = numeric(nrow(data)) # first i create a vector to store the values in parameter.A = x[\u0026quot;A\u0026quot;] #here A is the same across conditions, so i store A in here for (i in 1: nrow(data)){ #here i loop across rows to see which condition the data relates to and store the appropriate parameter for that row - either b.1 or b.2 if(data$condition[i]==1) if(data$speed[i]==\u0026quot;slow\u0026quot;){ parameter.b[i] \u0026lt;- x[\u0026quot;b.1.slow\u0026quot;] } else if(data$speed[i]==\u0026quot;fast\u0026quot;) parameter.b[i] \u0026lt;- x[\u0026quot;b.1.fast\u0026quot;] } else if(data$condition[i]==2) { if(data$speed[i]==\u0026quot;slow\u0026quot;){ parameter.b[i]\u0026lt;- x[\u0026quot;b.2.slow\u0026quot;] } else if(data$speed[i]==\u0026quot;fast\u0026quot;){ parameter.b[i] \u0026lt;- x[\u0026quot;b.2.fast\u0026quot;] } }   Fast  parameter.b = parameter.A = numeric(nrow(data)) # first i create a vector to store the values in parameter.A \u0026lt;- x[\u0026quot;A\u0026quot;] #here A is the same across conditions, so i store A in here for (cond in unique(data$condition)){ for (speed in unique(data$speed)){ parameter.b \u0026lt;- x[paste0(\u0026quot;b\u0026quot;,cond,speed)] } }  Again, this can be done in a variety of ways, but does require thorough checking to ensure parameters map correctly and effects carry.\n Point/intercept method Another method for dealing with main and interaction effects is with a “point-intercept” method of parameter mapping. We often use this method to limit the number of parameters when the parameter space is growing. This method is similar to a treatment effect method used in many ANOVA analyses. This is referred to above when I allude to main and interaction effect parameters.\nFor the point intercept method, there is a grand mean. For each specific condition, the parameter becomes the difference from the grand mean parameter. This means in our parameters, we have a grand mean parameter and a difference parameter. In the example above, the parameterisation is too simple, so lets look at two examples from the PMwG Doc in Chapters 2 and 3.\nIn Chapter 2, an SDT model is used. For the parameterisation, the d’ is the difference between parameters. We could estimate a mean for non-targets, a mean for targets and a mean for lures. However, these values are arbitrary and so could move along the scale unconstrained. Instead, we set the mean of non-targets to 0. We then only need to calculate the distance (d’) from non-targets to lures and from non-targets to targets. So lets start with targets, we call this parameter d’. Then, all we need to do is add a d’ increment to the lures parameter (i.e. d’ + d’ increment) to get the mean. These parameters are all on the real line, and so the d’ increment could be negative, allowing the mean for lures to fall anywhere on the line.\nSecondly, in chapter 3, we use an LBA model. In this example, we could parameterise in a way where we have a grand mean for threshold with differences for the conditions. This means that threshold is constrained by the mean parameter, and then for the other parameters, we add an increment to see differences between conditions. This is especially useful if we had bias in responses, where threshold for left was different than threshold for right responses. In this example, we could do this;\n bL=bR=numeric(nrow(data)) for (cond in unique(data$condition)){ bL\u0026lt;-x[\u0026quot;b.mean\u0026quot;]+x[paste0(\u0026quot;b.\u0026quot;), cond] bR\u0026lt;-x[\u0026quot;b.mean\u0026quot;]-x[paste0(\u0026quot;b.\u0026quot;), cond] }  In this example, the x[“b.i”] parameters need to be on the real line so that they can move the bias up or down for either responses (in each condition). This means that we should not take the exponential of these specific parameters at the start of the LL.\nNOTE: This kind of function would also require a check so that the overall threshold parameter (after adding/subtracting the difference) couldn’t be negative, and so something like the below is needed;\nif(!sample){ if (x[\u0026quot;b\u0026quot;] \u0026lt;= any(abs(c(x[\u0026quot;b.1\u0026quot;],x[\u0026quot;b.2\u0026quot;],x[\u0026quot;b.3\u0026quot;])))) { return(-1e10) } }   Make the ‘sample’ component Next up, we make the sample function for when sample = TRUE. This is not essential for PMwG to work, but does make post-processing much easier. In this step, we use the parameters above to create data with random generation functions (such as rnorm, rbinom or rLBA). This generally requires one call to a random generation function with the parameters created above. We usually do this as a loop over rows to protect against any weirdness from the generating function. This is slower, but safer (and is only called a handful of times).\n data$rt\u0026lt;-NA for (i in 1:nrow(data)){ tmp \u0026lt;- rpackage::rfunction(n=1, A = parameter.A[i], b = parameter.b[i]) ## rpackage::rfunction could be for example rtdists::rLBA data$rt[i]\u0026lt;-tmp }   Make the ‘likelihood’ component Next up, we make the density function for when sample = FALSE. This IS essential for PMwG to work. In this step, we use the parameters above to obtain the probability of each data point for the given parameters. This means that following the parameter specifications above, the density for data from trial i is calculated under the correct parameter inputs (i.e. parameter.A[i] and parameter.b[i]). So on each trial, we should see density[i] (of data[i]) = model_function(b=parameter.b[i], A=parameter.A[i]).\nEvidently, you could do this as a loop over rows, however, most density functions allow you to specify vectors of data and parameters (which is what we created above). The likelihood is generally obtained through density functions (such as dnorm, dbinom or dLBA), however, you may need to specify your own depending on the model.\n out \u0026lt;- numeric(nrow(data)) out \u0026lt;- rpackage::dfunction(data=data$rt, A = parameter.A[i], b = parameter.b[i]) ## rpackage::dfunction could be for example rtdists::dLBA }  Usually, density functions already exist or are pre-specified - many R packages are available for these. If a density function does not already exist, you probably have a mathematical model in mind. It’s important that this is tractable and able to recover effectively.\n Return Finally, you need to return the likelihood (or randomly generated data). If you have multiple probabilities, this will require combining these. We generally do this through;\nif(!sample){ return(sum(log(out))) }  Although occasionally, you may want to protect against really small values, and so we do;\n bad \u0026lt;- (out \u0026lt; 1e-10) | (!is.finite(out)) out[bad] \u0026lt;- 1e-10 out \u0026lt;- sum(log(out))  or;\n return(sum(log(pmax(out,1e-10)))  For returning data when sample = TRUE, you should write your sample function similar to above to ensure that the generated data is returned (not the original data). Then, we just need to return data;\nif(sample){ return(data) }  WARNING: if you return data, you need to ensure that this is only when sample = TRUE and - for a posterior generating function to work - must replace the values in the actual data. For more on simulating from the posterior, see here\n Full function Shown below is how your full likelihood function will look;\n(NOTE: this won’t work because it’s all example text)\n LL \u0026lt;- function(x,data,sample=FALSE){ #note, x is a vector of named parameter values # data is data - but in pmwg will be data for each subject #sample = FALSE should be default. if true, uses x to randomly generate. If false, uses x for density. #transform vector of parameters x\u0026lt;-exp(x) #map parameter.b = parameter.A = numeric(nrow(data)) # first i create a vector to store the values in parameter.A \u0026lt;- x[\u0026quot;A\u0026quot;] #here A is the same across conditions, so i store A in here for (cond in unique(data$condition)){ for (speed in unique(data$speed)){ parameter.b \u0026lt;- x[paste0(\u0026quot;b\u0026quot;,cond,speed)] } } #generate if(sample){ data$rt\u0026lt;-NA for (i in 1:nrow(data)){ tmp \u0026lt;- rpackage::rfunction(n=1, A = parameter.A[i], b = parameter.b[i]) ## i.e. rtdists::rLBA data$rt[i]\u0026lt;-tmp } #return return(data) } else{ #checks if(!sample){ if (any(data$RT \u0026lt; x[\u0026quot;t0\u0026quot;])) { return(-1e10) } } out \u0026lt;- numeric(nrow(data)) out \u0026lt;- rpackage::dfunction(data=data$rt, A = parameter.A[i], b = parameter.b[i]) ## i.e. rtdists::dLBA return(sum(log(pmax(out,1e-10))) } }    Testing your LL function. Finally, it’s REALLY important to test your likelihood function. In this section, I’ll go through a couple of methods I’ve given titles to.\nLine by line testing For this method, first make some x values, for example;\npars\u0026lt;- c(\u0026quot;A\u0026quot;,\u0026quot;b.1\u0026quot;,\u0026quot;b.2\u0026quot;) #names of params x \u0026lt;- c(1,0.5,0.7) #make sure you give different conditions different values so they don\u0026#39;t all look the same and its confusing. names(x)\u0026lt;-pars  From here, use this x and your data (or a subset) to work through each line of the function. What you should see is values for the correct parameters mapping to the correct conditions in your vector (for example x[“b.1”] matches with data that is from condition 1 etc). In our example above, parameter.A will be a vector of 1’s (x[“A’]=1) and parameter.b will be a vector of 0.5’s and 0.7’s which match to data$condition.\nFrom here, work through your LL running each line (or loop) of the function to ensure there are no errors, values map correctly, and the density function works correctly. This means that parameter vectors should use the correct conditions and values and output as expected.\n Alternate input values On the topic of density functions working correctly, this is the next step. For this step, again, create some parameter values. Then just run your LL with these values and data. Then, make some new parameter vectors, and repeat. What we should see is that;\nthe likelihood changes with different input values the likelihood changes in the right direction (i.e. for silly parameter values, a low likelihood is returned) the likelihood should quickly return bad likelihood for bad parameter values (as discussed above)  This is a quick and easy test that can be done many times and should give you some idea of whether your likelihood is ‘working’. There are more thorough and robust checks shown in Chapter 3 of the sampler doc, but this is a good first pass.\n Profile Plots Another strong way to test the likelihood is with profile plots. This is a basic kind of simulation and recovery exercise where we first generate some data using the sample = TRUE argument and then test different values using the sample = FALSE argument. Here, I use a function from the PMwG Toolkit to make my profile plots.\n#### profile plot function #### Contributed by Reilly Innes #### Use pwmg_profilePlot on an initiated PMwG object to ensure your likelihood functions correctly #### This function takes in the initial theta_mu estimates and simulates a small amount of data based on these #### plots are returned which show how the likelihood changes as the generating value changes #### For the different generating values, we use small increments (which can be negative or positive) #### We expect to see inverse U shaped curves, where the likelihood is most likely at the generating value and falls away as we get further from the generating value #### NOTE: The likelihood function needs both sample = TRUE and sample = FALSE arguments to function correctly #### Also, avoid putting protective statements (like if(any(data$rt)\u0026lt;t0) etc) at the beginning of the function #### These statements should go in the if(sample=FALSE) part require(ggplot2) require(tidyr) ## Loading required package: tidyr require(pmwg) ## Loading required package: pmwg pmwg_profilePlot = function(sampler, generating_values=NULL){ if(is.null(generating_values)){ #create generating values based on theta_mu generating_values \u0026lt;- sampler$samples$theta_mu names(generating_values)\u0026lt;- sampler$par_names } else{ names(generating_values)\u0026lt;-sampler$par_names } #make the test data set. here I use a tenth of the total data for speed test \u0026lt;- sampler$ll_func(x = generating_values, data = sampler$data[c(1:(nrow(sampler$data)/10)),], sample = TRUE) # this is the number of values to test and plot. n_values \u0026lt;- 9 tmp \u0026lt;- array(dim=c(n_values,sampler$n_pars)) #here i make the increment, however, you may wish to make this smaller or larger. #the increment here goes from theta_mu - .2 to theta_mu + .2, with n_values length increment\u0026lt;- seq(from=-.2, to=.2, length.out = n_values) for (i in 1:sampler$n_pars){ for (j in 1:n_values){ #need to use all generating values except the current parameter being tested test_values \u0026lt;- generating_values #here we change the current parameter by adding the increment test_values[i] \u0026lt;- generating_values[i] + increment[j] #test the likelihood given these new values and the test data tmp[j, i] \u0026lt;- sampler$ll_func(x = test_values, data=test, sample=F) } } #prepare output for plotting colnames(tmp)\u0026lt;-sampler$par_names tmp\u0026lt;-as.data.frame(tmp) tmp \u0026lt;- tidyr::pivot_longer(tmp, everything(), names_to = \u0026quot;pars\u0026quot;, values_to = \u0026quot;likelihood\u0026quot;) tmp$increment \u0026lt;- rep(increment, each=sampler$n_pars) ### next, plot these values for each parameter ggplot2::ggplot(tmp, aes(x=increment, y= likelihood))+ geom_point()+ geom_line()+ facet_wrap(~pars, scales = \u0026quot;free\u0026quot;)+ theme_bw()+ geom_vline(xintercept = 0, color=\u0026quot;red\u0026quot;, alpha = 0.3) }  For this function, it is essential that both components of the likelihood (sample=TRUE and FALSE) work correctly. This means that parameters are first transformed and mapped before being used by each component. It also means any protective statements are limited to the sample = FALSE section. The function then takes in an initialised PMwG object (from the init function in PMwG - although you can also use a sampled object). Now let’s test this out with a simple LBA example from Forstmann (2008).\n data\u0026lt;- forstmann data$condition \u0026lt;- as.factor(data$condition) lba_loglike \u0026lt;- function(x, data, sample = FALSE) { x \u0026lt;- exp(x) bs \u0026lt;- x[\u0026quot;A\u0026quot;] + x[c(\u0026quot;b1\u0026quot;, \u0026quot;b2\u0026quot;, \u0026quot;b3\u0026quot;)][data$condition] t0 \u0026lt;- x[c(\u0026quot;t0.1\u0026quot;,\u0026quot;t0.2\u0026quot;,\u0026quot;t0.3\u0026quot;)][data$condition] out \u0026lt;- nrow(data) if (sample) { data$rt=NA data$resp = NA for (i in 1:nrow(data)){ out \u0026lt;- rtdists::rLBA(n = 1, A = x[\u0026quot;A\u0026quot;], b = bs[i], t0 = t0[i], mean_v = x[c(\u0026quot;v1\u0026quot;, \u0026quot;v2\u0026quot;)], sd_v = c(1, 1), distribution = \u0026quot;norm\u0026quot;, silent = TRUE) data$rt[i] \u0026lt;- out$rt data$resp[i] \u0026lt;- out$resp } return(data) } else { if (any(min(data$rt) \u0026lt; c(x[\u0026quot;t0.1\u0026quot;], x[\u0026quot;t0.2\u0026quot;], x[\u0026quot;t0.3\u0026quot;])) ) { return(-1e10) } out \u0026lt;- rtdists::dLBA(rt = data$rt, response = data$resp, A = x[\u0026quot;A\u0026quot;], b = bs, t0 = t0, mean_v = x[c(\u0026quot;v1\u0026quot;, \u0026quot;v2\u0026quot;)], sd_v = c(1, 1), distribution = \u0026quot;norm\u0026quot;, silent = TRUE) bad \u0026lt;- (out \u0026lt; 1e-10) | (!is.finite(out)) out[bad] \u0026lt;- 1e-10 out \u0026lt;- sum(log(out)) return(out) } } # Specify the parameters and priors ------------------------------------------- # Vars used for controlling the run pars \u0026lt;- c(\u0026quot;b1\u0026quot;, \u0026quot;b2\u0026quot;, \u0026quot;b3\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;v1\u0026quot;, \u0026quot;v2\u0026quot;, \u0026quot;t0.1\u0026quot;,\u0026quot;t0.2\u0026quot;,\u0026quot;t0.3\u0026quot;) priors \u0026lt;- list( theta_mu_mean = c(0,0,0,0,1,1,-3,-3,-3), theta_mu_var = diag(c(1,1,1,1,2,2,0.1,0.1,0.1)) ) # Create the Particle Metropolis within Gibbs sampler object ------------------ sampler \u0026lt;- pmwgs( data = data, pars = pars, prior = priors, ll_func = lba_loglike ) start_points \u0026lt;- list( mu = log(c(1,1,1,1,2,3,0.07,0.08,0.09)), sig2 = diag(rep(.1, length(pars))) ) sampler \u0026lt;- init(sampler, start_mu = start_points$mu, start_sig = start_points$sig2, display_progress = FALSE, particles = 250) pmwg_profilePlot(sampler) From these plots we can see that for most of the parameters, there is a nice inverse U curve, which centers at 0 increment. This means that the generating value is also the most likely value for that parameter. This indicates that the parameter values simulate and recover well and our likelihood seems to function correctly. If you see different patterns, try using sensible start points or check line by line what could cause the function to return weird values (this could be due to one of the protective statements or could be due to parameters not mapping correctly).\n  References Gunawan, D., Hawkins, G. E., Tran, M. N., Kohn, R., \u0026amp; Brown, S. D. (2020). New estimation approaches for the hierarchical Linear Ballistic Accumulator model. Journal of Mathematical Psychology, 96, 102368.\nHuang, A., \u0026amp; Wand, M. P. (2013). Simple marginally noninformative prior distributions for covariance matrices. Bayesian Analysis, 8(2), 439-452.\n  ","date":1620777600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620777600,"objectID":"256558460a1298b6d8fb0511892e0500","permalink":"/post/2021-05-12-ll/writing-ll/","publishdate":"2021-05-12T00:00:00Z","relpermalink":"/post/2021-05-12-ll/writing-ll/","section":"post","summary":"A blog post to help with writing log likelihood functions to be used in PMwG. The examples shown in the PMwG Sampler Doc are quite specific, and so this blog aims to outline the general process of constructing a likelihood function.","tags":["pmwg","modelling"],"title":"PMwG Log-Likelihood Functions","type":"post"},{"authors":null,"categories":["R"],"content":"  So you’ve made the PMwG sampler work and now you have a sampled object, what’s next? Well now is where the fun stuff happens - checking, inference, plots and posteriors. In this short blog post, I’ll show several user contributed functions to make plots and output that is a great starting step for analysing your posterior model estimates.\nTo start with, we’ll go through some checks to make sure the sampler has run as we expected. Next, we’ll go through analysing posterior parameter and random effects (and convariance) estimates. Finally, I’ll provide code to create posterior predictive data so that you can compare the model estimates to the real data.\nChecking The first thing we should do with a PMwG object is check that it sampled correctly. For this, there are two main plots I look at - the parameter ‘chains’ and the subject likelihood ‘chains’. These are shown in the sampler documentation and are great ways to quickly visualise the sampling process.\nChain plots From the parameter chains, we would expect to see stationarity of parameters. By this I mean that the parameter chains are flat and not trending up or down. If they are moving up or down, we may not have yet reached the posterior. Further, the chains should be thin and not filling all of the prior space. If they are quite wide, this could indicate that the parameter is not informed. In the acceptance rates, if this is happening, it is likely that you would see very high acceptance as well. Often this happens when the likelihood isn’t specified correctly (for example, those pesky factors being read in the wrong way) and as a consequence, the likelihood returned on each step doesn’t change. This will be evident in the chain plot.\nSecondly, another good plot to look at is the subject likelihood plot. In this plot, similar to the parameter chains, we are looking for stationarity in the likelihood estimates for each subject (each line). It is likely that the subject likelihood plot will rapidly jump from a low value to a high one and remain in this region. If all estimates are the same for all subjects, this can also indicate a problem in the likelihood function, similar to the one above.\nShown below is code and example outputs for the chain plots.\n pmwg_chainPlots \u0026lt;- function(samples, subjectParPlot = F, parameterPlot = T, subjectLLPlot = T){ if (subjectParPlot){ par(mfrow = c(2, ceiling(samples$n_pars/2))) for (par in samples$par_names){ matplot(t(samples$samples$alpha[par,,]),type=\u0026quot;l\u0026quot;, main = par, xlab = \u0026quot;samples\u0026quot;, ylab = \u0026quot;ParameterValue\u0026quot;) } } par(mfrow=c(1,1)) if(parameterPlot) matplot(t(samples$samples$theta_mu), type=\u0026quot;l\u0026quot;, main = \u0026quot;Paramater chains\u0026quot;, ylab = \u0026quot;Parameter Value\u0026quot;, xlab = \u0026quot;samples\u0026quot;) if(subjectLLPlot) matplot(t(samples$samples$subj_ll), type=\u0026quot;l\u0026quot;, main = \u0026quot;LogLikelihood chains per subject\u0026quot;, ylab = \u0026quot;LogLikelihood\u0026quot;, xlab = \u0026quot;samples\u0026quot;) if(sum(subjectParPlot, parameterPlot, subjectLLPlot) \u0026gt; 1) print(\u0026#39;plots presented behind eachother\u0026#39;) } pmwg_chainPlots(sampled) ## [1] \u0026quot;plots presented behind eachother\u0026quot; From these outputs we can see that the parameter plots show thin stationary lines. Secondly, we can see the likelihood plots show a rapid rise and then stationary, separated lines. These are indicative that sampling worked as intended.\n Parameter histograms Secondly, it’s important to look at the posterior density of parameter estimates. To do this, I use a function that plots a historgram of each parameter. Additionally, I overlay the prior density to see if our priors were too restrictive or if we haven’t learnt anything new (i.e. prior = posterior).\n pmwg_parHist \u0026lt;- function(samples, bins =30, prior = FALSE ){ if (!prior){ chains \u0026lt;- as.array(as_mcmc(samples)) mcmc_hist(chains) } else{ theta \u0026lt;- t(sampled$samples$theta_mu) theta\u0026lt;-as.data.frame(theta) long \u0026lt;- sum(sampled$samples$stage==\u0026quot;sample\u0026quot;) theta \u0026lt;- theta[c((length(theta[,1])-long+1):length(theta[,1])),] theta \u0026lt;- pivot_longer(theta, cols = everything(), names_to = \u0026quot;pars\u0026quot;, values_to = \u0026quot;estimate\u0026quot; ) prior_mean \u0026lt;- sampled$prior$theta_mu_mean prior_var \u0026lt;- diag(sampled$prior$theta_mu_var) priors = NULL for (i in 1:sampled$n_pars){ tmp \u0026lt;- rnorm(n=long, mean=prior_mean[i], sd=prior_var[i]) tmp \u0026lt;- as.data.frame(tmp) priors\u0026lt;- c(priors, tmp[1:long,]) } priors\u0026lt;-as.data.frame(priors) y \u0026lt;- as.factor(sampled$par_names) theta\u0026lt;-theta[order(factor(theta$pars, levels = y)),] theta$prior \u0026lt;- priors$priors theta$pars\u0026lt;- as.factor(theta$pars) ggplot(theta, aes(estimate))+ geom_histogram(aes(y =..density..), bins = bins)+ geom_density(aes(prior))+ facet_wrap(~pars, scales = \u0026quot;free_y\u0026quot;)+ theme_bw() } } pmwg_parHist(sampled, bins=50, prior =T) In this example, we can see that for t0, the prior may be too restrictive, however, for other estimates, all seems as expected, with posterior samples falling in a thin range.\n  Posterior Estimates Now we know the sampler worked as intended, we can look at the posterior parameter estimates. For this part, it is crucial to remember the parameter transformations. For example, in the LBA, all input parameters must be positive, so we take the exponent of the proposed values on each iteration in the likelihood function. This means that when we interpret posterior parameters, we should also take the exponent of the value. For example, t0 in the example above is centered around -2 – an impossible value– and so we should take the exponent of it (0.135) as this is what the model is actually using.\nParameter histograms From the previous section, we can also use this plot to see where the posterior parameters lie. As mentioned above, we need to take the exponent of these LBA values, so this time I’ll include the transformations in the function.\n pmwg_parHist \u0026lt;- function(samples, bins =30, prior = FALSE ){ if (!prior){ chains \u0026lt;- as.array(as_mcmc(samples)) mcmc_hist(chains) } else{ theta \u0026lt;- exp(t(sampled$samples$theta_mu)) ### exp here theta\u0026lt;-as.data.frame(theta) long \u0026lt;- sum(sampled$samples$stage==\u0026quot;sample\u0026quot;) theta \u0026lt;- theta[c((length(theta[,1])-long+1):length(theta[,1])),] theta \u0026lt;- pivot_longer(theta, cols = everything(), names_to = \u0026quot;pars\u0026quot;, values_to = \u0026quot;estimate\u0026quot; ) prior_mean \u0026lt;- exp(sampled$prior$theta_mu_mean) ## exp here prior_var \u0026lt;- exp(diag(sampled$prior$theta_mu_var)) ## exp here priors = NULL for (i in 1:sampled$n_pars){ tmp \u0026lt;- rnorm(n=long, mean=prior_mean[i], sd=prior_var[i]) tmp \u0026lt;- as.data.frame(tmp) priors\u0026lt;- c(priors, tmp[1:long,]) } priors\u0026lt;-as.data.frame(priors) y \u0026lt;- as.factor(sampled$par_names) theta\u0026lt;-theta[order(factor(theta$pars, levels = y)),] theta$prior \u0026lt;- priors$priors theta$pars\u0026lt;- as.factor(theta$pars) ggplot(theta, aes(estimate))+ geom_histogram(aes(y =..density..), bins = bins)+ geom_density(aes(prior))+ facet_wrap(~pars, scales = \u0026quot;free_y\u0026quot;)+ theme_bw() } } pmwg_parHist(sampled, bins=50, prior =T) Here, we can see that t0 is just above 0, v1 is greater than v2 (correct \u0026gt; error drift rates) and there are three separable threshold values.\n Output tables Whilst the above plot is useful, we may wish to get some more fine grained analysis. For this, again looking at the group level (theta) values, we can create some output tables.\nFirst we look at mean parameter estimates (and variance using 95% credible intervals)\n qL=function(x) quantile(x,prob=.05,na.rm = TRUE) #for high and low quantiles qH=function(x) quantile(x,prob=.95,na.rm = TRUE) tmp \u0026lt;- exp(apply(sampled$samples$theta_mu[,sampled$samples$stage==\u0026quot;sample\u0026quot;],1,mean)) lower \u0026lt;- exp(apply(sampled$samples$theta_mu[,sampled$samples$stage==\u0026quot;sample\u0026quot;],1,qL)) upper \u0026lt;- exp(apply(sampled$samples$theta_mu[,sampled$samples$stage==\u0026quot;sample\u0026quot;],1,qH)) tmp \u0026lt;- t(rbind(tmp,lower,upper)) kable(tmp)    tmp lower upper    b1 0.8832800 0.7672251 1.0082430  b2 1.0173988 0.8724713 1.1971625  b3 1.4128471 1.2075098 1.6497759  A 1.5383791 1.3499831 1.7429653  v1 3.3117580 2.7409280 3.9455836  v2 1.1322673 0.9322809 1.3405226  t0 0.1540344 0.1243581 0.1915416    This is a nice way to view the posterior parameter means and ranges. Next, we can look at the mean covariance structure, which we will transform into a correlation matrix.\ncov\u0026lt;-apply(sampled$samples$theta_sig[,,sampled$samples$idx-1000:sampled$samples$idx] ,1:2, mean) colnames(cov)\u0026lt;-pars rownames(cov)\u0026lt;-pars cor\u0026lt;-cov2cor(cov) #transforms covariance to correlation matrix kable(cor)     b1 b2 b3 A v1 v2 t0    b1 1.0000000 0.2261599 0.0722276 0.0124526 0.2234376 0.1664706 -0.2142574  b2 0.2261599 1.0000000 0.1250495 -0.0768053 0.1306218 -0.0276819 -0.0462468  b3 0.0722276 0.1250495 1.0000000 0.0710326 0.1006157 0.0548435 0.1736373  A 0.0124526 -0.0768053 0.0710326 1.0000000 0.1164379 0.1028593 0.0674493  v1 0.2234376 0.1306218 0.1006157 0.1164379 1.0000000 0.0823956 0.0483617  v2 0.1664706 -0.0276819 0.0548435 0.1028593 0.0823956 1.0000000 -0.1596716  t0 -0.2142574 -0.0462468 0.1736373 0.0674493 0.0483617 -0.1596716 1.0000000    This is a good first summary, but now lets plot this with corrplot.\n corrplot(cor, method=\u0026quot;color\u0026quot;, type = \u0026quot;lower\u0026quot;) This covariance analysis looks at the correlations between parameter values. Here we can see that b1 and v1 are positively correlated, whereas t0 and b1 are negatively correlated. PMwG is great at dealing with models with high autocorrelation of parameters, and so it is important to check these kinds of outputs to see where correlations exist in the model and what could underpin these (or whether these should NOT be correlated).\n Inference Importantly for psych research, we might wish to do inference on the parameter estimates. Here, we can use any kind of classic inference test to look for differences between parameter values. We can also test this by looking at whether the difference between posterior parameter estimates crosses zero.\nFirst though, lets look to see if there’s a difference between the b parameters using a bayesian anova.\nlibrary(BayesFactor) ## Loading required package: coda ## ************ ## Welcome to BayesFactor 0.9.12-4.2. If you have questions, please contact Richard Morey (richarddmorey@gmail.com). ## ## Type BFManual() to open the manual. ## ************ tmp \u0026lt;- as.data.frame(apply(sampled$samples$theta_mu[,sampled$samples$stage==\u0026quot;sample\u0026quot;],1,exp)) tmp \u0026lt;- tmp[,c(1:3)] tmp \u0026lt;- tmp %\u0026gt;% pivot_longer(everything() , names_to = \u0026quot;pars\u0026quot;, values_to = \u0026quot;estimate\u0026quot;) tmp \u0026lt;- as.data.frame(tmp) tmp$pars\u0026lt;-as.factor(tmp$pars) anovaBF(estimate ~ pars, data=tmp) ## Bayes factor analysis ## -------------- ## [1] pars : 7.809663e+1090 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS From this output, there is strong evidence for a difference between these conditions. Next we can plot these estimates to see the difference between b values visually.\n plot.data \u0026lt;- tmp %\u0026gt;% group_by(pars) %\u0026gt;% summarise(avg = mean(estimate), lower = qL(estimate), upper = qH(estimate)) %\u0026gt;% ungroup() ggplot(plot.data, aes(x=pars, y=avg)) + geom_point(size=3) + geom_errorbar(aes(ymin=lower,ymax=upper),width=0.2) + theme_bw() Another method we can use to look for a difference between parameters is to check if the difference between posterior parameters crosses 0. If it does not, then one posterior estimate is reliably greater than the other. Lets check this for b1 and b2. Here, we can set a frequentist style criterion (lets say 95%), and if 95% of the difference between samples are on one side of zero, we can conclude that there is a difference between these parameters. Alternatively, we could run a separate model where these parameters were not separated and then do model comparison methods for these models.\ntmp \u0026lt;- as.data.frame(apply(sampled$samples$theta_mu[,sampled$samples$stage==\u0026quot;sample\u0026quot;],1,exp)) tmp \u0026lt;- tmp[,c(1:2)] tmp$diff \u0026lt;- tmp$b1-tmp$b2 mean(tmp$diff\u0026gt;0) ## [1] 0.062 Here we can see that 93.8% of the samples are greater than zero, so there is not conclusive evidence that these parameters vary at the group level.\n  Posterior Predictive Data Finally, an important part of post-model processing is generating posterior predictive data. Using this generated data, we can check to see if the posterior data ‘fits’ the data. To generate posterior predictive data, we first need to randomly sample some individual random effects. We then use these in the log-likelihood function (using sample=TRUE) to generate data. For this part, it is important that your likelihood function is set up correctly to generate data. However, if this is not the case, you can restructure your likelihood function and change the function below so that it still operates correctly.\n pmwg_generatePosterior \u0026lt;- function(sampled, n){ n.posterior=n # Number of parameter samples from posterior distribution. pp.data=list() S = sampled$n_subjects data=sampled$data sampled_stage = length(sampled$samples$stage[sampled$samples$stage==\u0026quot;sample\u0026quot;]) for (s in 1:S) { cat(s,\u0026quot; \u0026quot;) iterations=round(seq(from=(sampled$samples$idx-sampled_stage) , to=sampled$samples$idx, length.out=n.posterior)) for (i in 1:length(iterations)) { x \u0026lt;- sampled$samples$alpha[,s,iterations[i]] names(x) \u0026lt;- sampled$par_names tmp=sampled$ll_func(x=x,data=data[as.integer(as.numeric(data$subject))==s,],sample=TRUE) ##change here to your own sampled function if your likelihood was not set up correctly if (i==1) { pp.data[[s]]=cbind(i,tmp) } else { pp.data[[s]]=rbind(pp.data[[s]],cbind(i,tmp)) } } } return(pp.data) } pp.data\u0026lt;-generate.posterior(sampled,20) #i do 20 because that seems fine tmp=do.call(rbind,pp.data) #binds together  ## i subject rt resp condition ## 1 1 1 0.8109699 1 1 ## 2 1 1 1.3727252 1 2 ## 3 1 1 1.0959211 2 3 ## 4 1 1 0.4747878 1 1 ## 5 1 1 0.9924096 1 2 ## 6 1 1 2.8030712 1 3 Here you can see that I’ve generated 20 samples for each person. The column i shows the iteration. This does take a little bit of time, but not too long (it depends on your likelihood). When writing your likelihood, we often try and make this step as safe as possible (rather than fast) as it is only called several times.\nNow we have posterior predictive data, we can plot this against the real data.\n summ \u0026lt;- tmp %\u0026gt;% group_by(i,condition) %\u0026gt;% summarise(MeanRT = mean(rt,na.rm=TRUE), ACC = mean(resp), qL = qL(rt), qH = qH(rt), qLA = qL(resp), qHA = qH(resp))%\u0026gt;% ungroup() ## `summarise()` has grouped output by \u0026#39;i\u0026#39;. You can override using the `.groups` argument. summ$source \u0026lt;- \u0026quot;model\u0026quot; summD \u0026lt;- data %\u0026gt;% group_by(condition) %\u0026gt;% summarise(MeanRT = mean(rt,na.rm=TRUE), ACC = mean(resp), qL = qL(rt), qH = qH(rt), qLA = qL(resp), qHA = qH(resp))%\u0026gt;% ungroup() summD$source \u0026lt;- \u0026quot;data\u0026quot; ggplot(summ, aes(x=condition, y=ACC, color=source, group=source))+geom_point()+geom_col(data=summD, aes(x=condition, y=ACC), alpha=0.2)+theme_bw()  ggplot(summ, aes(x=condition, y=MeanRT, color=source, group=source))+geom_point()+geom_col(data=summD, aes(x=condition, y=MeanRT), alpha=0.2)+theme_bw()  In these plots, I simply show the mean rt and accuracy for both the data (bars) and each posterior predictive iteration (20 dots). There are a variety of alternative methods to showing the descriptive adequacy of the model, such as Q-Q plots and plots across subjects, however, this is a quick way to check the fit of the model and a stepping stone to better analysis and graphs.\n Model Comparison Finally, an important part of post modelling is often model comparison. There are many methods of model comparison, and they all have advantages and disadvantages - which I’m not going to go into. Here, I show two methods of model comparison - DIC and IS2. If you have the time and computer resources, we recommend using IS2 with PMwG objects. This comes from the paper by Minh-Ngoc Tran and colleagues for robustly estimating the marginal likelihood by importance sampling. An important caveat of this method however, is that it relies heavily on the settings of the prior (which are input before sampling). If the prior is too restrictive, too broad or just wrong, then estimates will be biased. This means that more complex models will be more penalized when the prior is set incorrectly (or uninformed), and why it is crucial to check the posterior parameter estimates (with functions like that above - e.g. pmwg_ParHist) before doing this kind of model comparison.\nDIC DIC is a commonly used information criteria for comparing models. The function below takes in a sampled pmwg object and returns the DIC value.\n pmwg_DIC=function(sampled,pD=FALSE){ nsubj=length(unique(sampled$data$subject)) # the mean likelihood of the overall (sampled-stage) model, separately for each subject mean.like \u0026lt;- apply(sampled$samples$subj_ll[,sampled$samples$stage==\u0026quot;sample\u0026quot;],1,mean) # the mean of each parameter across iterations. Keep dimensions for parameters and subjects mean.params \u0026lt;- t(apply(sampled$samples$alpha[,,sampled$samples$stage==\u0026quot;sample\u0026quot;],1:2,mean)) # i name mean.params here so it can be used by the log_like function colnames(mean.params)\u0026lt;-sampled$par_names # log-likelihood for each subject using their mean parameter vector mean.params.like \u0026lt;- numeric(ncol(mean.params)) data \u0026lt;- transform(sampled$data, subject=match(subject, unique(subject))) for (j in 1:nsubj) { mean.params.like[j] \u0026lt;- sampled$ll_func(mean.params[j,], data=data[data$subject==j,], sample=FALSE) } # Effective number of parameters pD \u0026lt;- sum(-2*mean.like + 2*mean.params.like) # Deviance Information Criterion DIC \u0026lt;- sum(-4*mean.like + 2*mean.params.like) if (pD){ return(c(\u0026quot;DIC\u0026quot;=DIC,\u0026quot;effective parameters\u0026quot;=pD)) }else{ return(DIC) } } pmwg_DIC(sampled) ## DIC effective parameters ## 2135.809 124.878 A DIC value on it’s own is relatively meaningless unless compared against competing models, so we could fit another model where threshold did not vary between condition 1 and condition 2 and then check which of the models had the lower DIC.\n IS2 Importance sampling squared is another method of model comparison. IS2 allows for robust and unbiased estimation of the marginal likelihood. The script below will run IS2 on a sampled object once loaded in, however, it can take some time. Typically, we generate around 10,000 IS samples, with 250 particles. The IS2 method works by first generating an importance distribution for the fixed parameters. This importance distribution is constructed by fitting a mixture of normal or Student t distributions to these MCMC samples. We then construct conditional proposal parameters - called particles - for each subject. The marginal likelihood is then estimated unbiasedly which is combined with the importance distribution. From this method, the importance sampling procedure is in itself an importance sampling procedure which can be used to estimate the likelihood.\nIS2 Script;\n ####### IS2 t-distribution Code ######### ## set up environment and packages rm(list=ls()) library(mvtnorm) library(MCMCpack) library(rtdists) library(invgamma) library(mixtools) library(condMVNorm) library(parallel) load(\u0026quot;sampled.Rdata\u0026quot;) cpus = 20 ###### set up variables ##### # number of particles, samples, subjects, random effects etc n_randeffect=sampled$n_pars n_subjects = sampled$n_subjects n_iter = length(sampled$samples$stage[sampled$samples$stage==\u0026quot;sample\u0026quot;]) length_draws = sampled$samples$idx #length of the full transformed random effect vector and/or parameter vector IS_samples = 10000 #number of importance samples n_particles = 250 #number of particles v_alpha = 2 #? pars = sampled$pars # grab the sampled stage of PMwG # store the random effects alpha \u0026lt;- sampled$samples$alpha[,,sampled$samples$stage==\u0026quot;sample\u0026quot;] # store the mu theta \u0026lt;- sampled$samples$theta_mu[,sampled$samples$stage==\u0026quot;sample\u0026quot;] # store the cholesky transformed sigma sig \u0026lt;- sampled$samples$theta_sig[,,sampled$samples$stage==\u0026quot;sample\u0026quot;] # the a-hlaf is used in calculating the Huang and Wand (2013) prior. # The a is a random sample from inv gamma which weights the inv wishart. The mix of inverse wisharts is the prior on the correlation matrix a_half \u0026lt;- log(sampled$samples$a_half[,sampled$samples$stage==\u0026quot;sample\u0026quot;]) unwind=function(x,reverse=FALSE) { if (reverse) { ## if ((n*n+n)!=2*length(x)) stop(\u0026quot;Wrong sizes in unwind.\u0026quot;) n=sqrt(2*length(x)+0.25)-0.5 ## Dim of matrix. out=array(0,dim=c(n,n)) out[lower.tri(out,diag=TRUE)]=x diag(out)=exp(diag(out)) out=out%*%t(out) } else { y=t(chol(x)) diag(y)=log(diag(y)) out=y[lower.tri(y,diag=TRUE)] } return(out) } robust_diwish = function (W, v, S) { if (!is.matrix(S)) S \u0026lt;- matrix(S) if (nrow(S) != ncol(S)) { stop(\u0026quot;S not square in diwish().\\n\u0026quot;) } if (!is.matrix(W)) W \u0026lt;- matrix(W) if (nrow(W) != ncol(W)) { stop(\u0026quot;W not square in diwish().\\n\u0026quot;) } if (nrow(S) != ncol(W)) { stop(\u0026quot;W and X of different dimensionality in diwish().\\n\u0026quot;) } if (v \u0026lt; nrow(S)) { stop(\u0026quot;v is less than the dimension of S in diwish().\\n\u0026quot;) } p \u0026lt;- nrow(S) gammapart \u0026lt;- sum(lgamma((v + 1 - 1:p)/2)) ldenom \u0026lt;- gammapart + 0.5 * v * p * log(2) + 0.25 * p * (p - 1) * log(pi) cholS \u0026lt;- chol(S) #cholW \u0026lt;- chol(W) cholW \u0026lt;- tryCatch(chol(W),error= return(1e-10)) halflogdetS \u0026lt;- sum(log(diag(cholS))) halflogdetW \u0026lt;- sum(log(diag(cholW))) invW \u0026lt;- chol2inv(cholW) exptrace \u0026lt;- sum(S * invW) lnum \u0026lt;- v * halflogdetS - (v + p + 1) * halflogdetW - 0.5 * exptrace lpdf \u0026lt;- lnum - ldenom return(exp(lpdf)) } #unwound sigma pts2.unwound = apply(sig,3,unwind) n.params\u0026lt;- nrow(pts2.unwound)+n_randeffect+n_randeffect all_samples=array(dim=c(n_subjects,n.params,n_iter)) mu_tilde=array(dim = c(n_subjects,n.params)) sigma_tilde=array(dim = c(n_subjects,n.params,n.params)) for (j in 1:n_subjects){ all_samples[j,,] = rbind(alpha[,j,],theta[,],pts2.unwound[,]) # calculate the mean for re, mu and sigma mu_tilde[j,] =apply(all_samples[j,,],1,mean) # calculate the covariance matrix for random effects, mu and sigma sigma_tilde[j,,] = cov(t(all_samples[j,,])) } X \u0026lt;- cbind(t(theta),t(pts2.unwound),t(a_half)) muX\u0026lt;-apply(X,2,mean) sigmaX\u0026lt;-var(X) # generates the IS proposals prop_theta=mvtnorm::rmvt(IS_samples,sigma = sigmaX, df=1, delta=muX) #prop_theta_compare=rmvnorm(IS_samples,muX,sigmaX) ### main functions group_dist = function(random_effect = NULL, parameters, sample = FALSE, n_samples = NULL, n_randeffect){ param.theta.mu \u0026lt;- parameters[1:n_randeffect] param.theta.sig.unwound \u0026lt;- parameters[(n_randeffect+1):(length(parameters)-n_randeffect)] param.theta.sig2 \u0026lt;- unwind(param.theta.sig.unwound, reverse = TRUE) if (sample){ return(mvtnorm::rmvnorm(n=n_samples, mean=param.theta.mu,sigma=param.theta.sig2)) }else{ logw_second\u0026lt;-mvtnorm::dmvnorm(random_effect, param.theta.mu,param.theta.sig2,log=TRUE) return(logw_second) } } prior_dist = function(parameters, prior_parameters = sampled$prior, n_randeffect){ ###mod notes: the sampled$prior needs to be fixed/passed in some other time param.theta.mu \u0026lt;- parameters[1:n_randeffect] param.theta.sig.unwound \u0026lt;- parameters[(n_randeffect+1):(length(parameters)-n_randeffect)] ##scott would like it to ask for n(unwind) param.theta.sig2 \u0026lt;- unwind(param.theta.sig.unwound, reverse = TRUE) param.a \u0026lt;- exp(parameters[((length(parameters)-n_randeffect)+1):(length(parameters))]) v_alpha=2 log_prior_mu=mvtnorm::dmvnorm(param.theta.mu, mean = prior_parameters$theta_mu_mean, sigma = prior_parameters$theta_mu_var, log =TRUE) log_prior_sigma = log(robust_diwish(param.theta.sig2, v=v_alpha+ n_randeffect-1, S = 2*v_alpha*diag(1/param.a))) #exp of a-half -\u0026gt; positive only log_prior_a = sum(invgamma::dinvgamma(param.a,scale = 0.5,shape=1,log=TRUE)) logw_den2 \u0026lt;- sum(log(1/param.a)) # jacobian determinant of transformation of log of the a-half logw_den3 \u0026lt;- log(2^n_randeffect)+sum((n_randeffect:1+1)*log(diag(param.theta.sig2))) #jacobian determinant of cholesky factors of cov matrix return(log_prior_mu + log_prior_sigma + log_prior_a + logw_den3 - logw_den2) } get_logp=function(prop_theta,data,n_subjects,n_particles,n_randeffect,mu_tilde,sigma_tilde,i, group_dist=group_dist){ #make an array for the density logp=array(dim=c(n_particles,n_subjects)) # for each subject, get 1000 IS samples (particles) and find log weight of each for (j in 1:n_subjects){ #generate the particles from the conditional MVnorm AND mix of group level proposals wmix \u0026lt;- 0.95 n1=rbinom(n=1,size=n_particles,prob=wmix) if (n1\u0026lt;2) n1=2 if (n1\u0026gt;(n_particles-2)) n1=n_particles-2 ## These just avoid degenerate arrays. n2=n_particles-n1 # do conditional MVnorm based on the proposal distribution conditional = condMVNorm::condMVN(mean=mu_tilde[j,],sigma=sigma_tilde[j,,],dependent.ind=1:n_randeffect, given.ind=(n_randeffect+1):n.params,X.given=prop_theta[i,1:(n.params-n_randeffect)]) particles1 \u0026lt;- mvtnorm::rmvnorm(n1, conditional$condMean,conditional$condVar) # mix of proposal params and conditional particles2 \u0026lt;- group_dist(n_samples=n2, parameters = prop_theta[i,],sample=TRUE, n_randeffect=n_randeffect) particles \u0026lt;- rbind(particles1,particles2) for (k in 1:n_particles){ x \u0026lt;-particles[k,] #names for ll function to work #mod notes: this is the bit the prior effects names(x)\u0026lt;-sampled$par_names # do lba log likelihood with given parameters for each subject, gets density of particle from ll func logw_first=sampled$ll_func(x,data = data[as.numeric(factor(data$subject))==j,]) #mod notes: do we pass this in or the whole sampled object???? # below gets second part of equation 5 numerator ie density under prop_theta # particle k and big vector of things logw_second\u0026lt;-group_dist(random_effect = particles[k,], parameters = prop_theta[i,], sample= FALSE, n_randeffect = n_randeffect) #mod notes: group dist # below is the denominator - ie mix of density under conditional and density under pro_theta logw_third \u0026lt;- log(wmix*dmvnorm(particles[k,], conditional$condMean,conditional$condVar)+(1-wmix)*exp(logw_second)) #mod notes: fine? #does equation 5 logw=(logw_first+logw_second)-logw_third #assign to correct row/column logp[k,j]=logw } } #we use this part to centre the logw before addign back on at the end. This avoids inf and -inf values sub_max = apply(logp,2,max) logw = t(t(logp) - sub_max) w = exp(logw) subj_logp = log(apply(w,2,mean))+sub_max #means # sum the logp and return if(is.nan(sum(subj_logp))){ return(1e-10) }else{ return(sum(subj_logp)) } } compute_lw=function(prop_theta,data,n_subjects,n_particles,n_randeffect,mu_tilde,sigma_tilde,i, prior_dist=prior_dist, sampled=sampled){ logp.out \u0026lt;- get_logp(prop_theta,data,n_subjects,n_particles,n_randeffect,mu_tilde,sigma_tilde,i, group_dist=group_dist) ##do equation 10 logw_num \u0026lt;- logp.out[1]+prior_dist(parameters = prop_theta[i,], prior_parameters = sampled$prior, n_randeffect) logw_den \u0026lt;- mvtnorm::dmvt(prop_theta[i,], delta=muX, sigma=sigmaX,df=1, log = TRUE) #density of proposed params under the means logw \u0026lt;- logw_num-logw_den #this is the equation 10 return(c(logw)) #NOTE: we should leave a note if variance is shit - variance is given by the logp function (currently commented out) } ##### make it work #makes an array to store the IS samples tmp\u0026lt;-array(dim=c(IS_samples)) #do the sampling if (cpus\u0026gt;1){ tmp \u0026lt;- mclapply(X=1:IS_samples,mc.cores = cpus, FUN = compute_lw, prop_theta = prop_theta,data = data,n_subjects= n_subjects,n_particles = n_particles, n_randeffect = n_randeffect,mu_tilde=mu_tilde,sigma_tilde = sigma_tilde, prior_dist=prior_dist, sampled=sampled) } else{ for (i in 1:IS_samples){ cat(i) tmp[i]\u0026lt;-compute_lw(prop_theta,data,n_subjects,n_particles, n_randeffect,mu_tilde,sigma_tilde,i,prior_dist=prior_dist, sampled=sampled) } } # get the ML value finished \u0026lt;- tmp tmp\u0026lt;-unlist(tmp) max.lw \u0026lt;- max(tmp) mean.centred.lw \u0026lt;- mean(exp(tmp-max.lw)) #takes off the max and gets mean (avoids infs) MLE \u0026lt;-log(mean.centred.lw)+max.lw #puts max back on to get the lw  This script returns the maximum likelihood estimate for a model. Similar to DIC, this is relatively meaningless without something to compare to, where the model with the higher MLE is chosen. Further, we can also compute Bayes factors with MLE’s to show the weight of evidence for one model over another.\n  Conclusions Ultimately, post sampling analysis is up to the user, but hopefully this blog has provided some insight into what sort of analysis is typically done, some ways of checking the sampling process, methods of posterior analysis and generating posterior data, as well as some model comparison insights. For more info on the PMwG sampler, see the documentation and for some more useful functions, see the PMwG toolkit github.\nReferences Tran, M. N., Scharth, M., Gunawan, D., Kohn, R., Brown, S. D., \u0026amp; Hawkins, G. E. (2020). Robustly estimating the marginal likelihood for cognitive models via importance sampling. Behavior Research Methods, 1-18.\n  ","date":1620086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620086400,"objectID":"9cbbcc6ecb9cf50b0a26139a230be33a","permalink":"/post/2021-05-04-posterior/posterior_inference/","publishdate":"2021-05-04T00:00:00Z","relpermalink":"/post/2021-05-04-posterior/posterior_inference/","section":"post","summary":"So you’ve made the PMwG sampler work and now you have a sampled object, what’s next? Well now is where the fun stuff happens - checking, inference, plots and posteriors.","tags":["pmwg","modelling"],"title":"How to Posterior","type":"post"},{"authors":null,"categories":["R"],"content":"  This blog post presents a brief guide on how to create synthetic data, and perform parameter recovery, using the PMwG sampler. Parameter recovery is highly important in cognitive modelling procedures as it provides an indication of parameter identifiability (and in a way, model “power”). Parameter recovery involves creating synthetic data using a likelihood function, checking the synthetic data, performing a sampling regime and then checking the posterior estimates (as well as posterior predictive data). It is an essential step in the modelling process.\nParameter recovery through simulation allows us to be confident in our models for three reasons. First, it gives us confidence that our model actually works, i.e. it runs, it relates to data, it does what it’s supposed to do. Secondly, parameter recovery allows us to see whether the parameter values we input are recovered at the end, and this can give us confidence in the reliability of our model. Finally, parameter recovery allows us to test a variety of parameter combinations and models, which can test theory and help us build hypotheses.\nSynthetic data Synthetic data is data generated from your model. When simulating this synthetic data, we make a data set that looks just like real data (i.e. same structure), with the aim of later replacing it with real data. We also want this synthetic data to reflect real data - i.e. somewhere in the ballpark of means and variance. When generating synthetic data, we input parameter values that we decide (or could be informed by prior research) and then use a random generator function (like rnorm, rlba or rddm), that takes in these values to create data.\nBy creating synthetic data, we are able to control the input parameters, and so we are able to better understand how parameters affect synthetic data - i.e. whether changing given parameters changes output. Secondly, we are able to see if the original values are recovered following sampling, therefore showing how well the model identifies the actual effects. Finally, we can see how well the posterior predictive data fits the synthetic data. Often different parameters may lead to similar patterns in the data (i.e. higher thresholds and higher t0 can show similar increases in rt for accumulator models), so by comparing to the posterior predictive, this can give us an indication of whether our model can tease apart parameter differences.\nWithin PMwG, there is an assumed hierarchical structure, with group level (theta) and individual participant level (alpha) values. On each iteration of the sampling process, n particles density are compared (given the data). These particles are drawn from a multivariate normal distribution centered around each participants current particle (with added noise). To create this multivariate normal distribution, the sampler also uses a covariance matrix, which gives insight into the level of covariance between individual parameters. This makes simulating synthetic data for the PMwG sampler slightly more complex, as simulated parameters should also have a covariance structure constraining them. This is shown below.\nIn this blog post I will;\n construct the data structure construct a likelihood function with sampling and density calculation arguments. create some parameters for the model fill in the values for these parameters constrain by the covariance matrix create data perform parameter recovery.   Simulation Starting out First I load in the packages used for this exercise;\nlibrary(rtdists) library(mvtnorm) ## For the multivariate normal. library(MASS) ## For matrix inverse. library(MCMCpack) library(lme4) library(dplyr) library(ggplot2) library(tidyr)  Data structure Next, we set up the structure of the data frame to be filled with data. For this exercise, i use 3 conditions, each with 100 trials for 20 participants. You can create whatever data structure you wish, however, it should map to your likelihood function, should be the same as your real data (or at least the parts of your real data used in the likelihood function) and can be any length (but more is better). I do this setup step first as this often assists with writing the likelihood function (i.e. for correct column names and data structure).\nn.trials = 100 #number trials per subject per conditions n.subj = 20 #number of subjects n.cond = 3 #number of conditions names=c(\u0026quot;subject\u0026quot;,\u0026quot;rt\u0026quot;,\u0026quot;resp\u0026quot;,\u0026quot;condition\u0026quot;) #names of columns data = data.frame(matrix(NA, ncol = length(names), nrow = (n.trials*n.subj*n.cond))) #empty data frame names(data)=names data$condition = rep(1:n.cond,times = n.trials) #filling in condition data$subject = rep(1:n.subj, each = n.trials*n.cond) #filling in subjects  Construct the model Now that the data structure has been created, I need to establish my model. The model I use here is similar to that used in Chapter 3 of the PMwG Documention. This model is a LBA model which includes 3 different thresholds (differing across conditions).\n log_likelihood=function(x,data,sample=TRUE) { x=exp(x) bs=x[\u0026quot;A\u0026quot;]+x[c(\u0026quot;b1\u0026quot;,\u0026quot;b2\u0026quot;,\u0026quot;b3\u0026quot;)][data$condition] if (sample) { #for sampling out=rLBA(n=nrow(data),A=x[\u0026quot;A\u0026quot;],b=bs,t0=x[\u0026quot;t0\u0026quot;],mean_v=x[c(\u0026quot;v1\u0026quot;,\u0026quot;v2\u0026quot;)],sd_v=c(1,1),distribution=\u0026quot;norm\u0026quot;,silent=TRUE) } else { #for calculating density out=dLBA(rt=data$rt,response=data$correct,A=x[\u0026quot;A\u0026quot;],b=bs,t0=x[\u0026quot;t0\u0026quot;],mean_v=x[c(\u0026quot;v1\u0026quot;,\u0026quot;v2\u0026quot;)],sd_v=c(1,1),distribution=\u0026quot;norm\u0026quot;,silent=TRUE) bad=(out\u0026lt;1e-10)|(!is.finite(out)) out[bad]=1e-10 out=sum(log(out)) } out } In this example, I use responses “1” and “2”, but you can use whatever responses you would prefer (i.e. correct or error etc). Further, you could also use a different model type (i.e. diffusion).\n Create model parameters Next, I create my parameter vector;\n parameter.names=c(\u0026quot;b1\u0026quot;,\u0026quot;b2\u0026quot;,\u0026quot;b3\u0026quot;, \u0026quot;A\u0026quot;,\u0026quot;v1\u0026quot;,\u0026quot;v2\u0026quot;,\u0026quot;t0\u0026quot;) n.parameters=length(parameter.names) Now i set up a vector of parameter values and a matrix of covariance matrix values. I label these ptm and ptm2;\nptm \u0026lt;- array(dim = n.parameters, dimnames = list(parameter.names)) #an empty array where i will put parameter values pts2 \u0026lt;- array(dim=c(n.parameters,n.parameters), dimnames = list(parameter.names,parameter.names)) #an empty array where i will put the covariance matrix  Fill in model parameters I then give start points for my input parameters. Here, I use increments of 0.2 for the threshold parameters. It is important to consider a number of different values to see if, and how, they affect the simulated data (but you can do this later). For example, I may also want to vary t0 or drift rates (i.e. different models) to see if these parameters are also recoverable.\nNOTE: these values are on the log scale (as the likelihood function takes the exponent of these). This is important to note as PMwG draws values along the real number line.\n ptm[1:n.parameters]=c(0.1,0.3,0.5,0.4,1.2,0.3,-2) exp(ptm) ## b1 b2 b3 A v1 v2 t0 ## 1.1051709 1.3498588 1.6487213 1.4918247 3.3201169 1.3498588 0.1353353  Create covariance matrix Next, I make the variance of the parameters. I do this in a simple way by taking the absolute value for each parameter and divide by 10 - but you can vary them however you like, for example, there may be covariances that are important to the model. In this example with the LBA, it might be better to put larger correlations between b and t0.\nIn this code, SigmaC is a matrix with diagonal of 0.2 (i.e. a correlation of .2 between all parameters), and off diagonal as the variance we just created. I then do sdcor2cov to create a covariance matrix. I do this because, rather than correlation, PMwG expects covariance. You can do the opposite transform to recover these correlations after recovery.\nThis step is the tricky part of simulating in PMwG, as we need this correlation structure (which becomes a covariance structure) to constrain parameters.\n vars = abs(ptm)/10 #off diagonal correlations are done as absolute/10 sigmaC = matrix(.2,nrow=length(ptm),ncol=length(ptm)) ###std dev correlation on diagonal - you might think this should be corr = 1, but it\u0026#39;s actually the standard deviation diag(sigmaC)=sqrt(vars) sigmaC \u0026lt;- sdcor2cov(sigmaC)  Create random effects Finally, I create the subjects random effects. This is a n_parameters * n_subject matrix, so that each subject has a value for each parameter. This is created using rmvnorm, where I do n_subjects of random samples using the mean parameters (ptm) and covariance matrix (sigmaC). These are the values we wish to recover using the model (so it’s often good to save out ptm, sigmaC and subj_random_effects).\n subj_random_effects \u0026lt;- t(mvtnorm::rmvnorm(n.subj,mean=ptm,sigma=sigmaC))  ## [,1] [,2] [,3] [,4] [,5] [,6] ## b1 0.06754572 0.16595634 -0.14890327 0.007714805 0.22501307 0.04926983 ## b2 0.45809512 0.01340640 0.08264425 0.460439178 0.30218297 0.23504860 ## b3 0.58720469 0.37898141 1.13992801 0.573482556 0.32560658 0.63982060 ## A 0.04662982 0.47198462 0.30647877 0.399114046 0.37684673 0.36270810 ## v1 0.81182229 0.72201092 1.25883303 1.574770678 1.08952060 1.14255482 ## v2 0.20903875 0.03129003 0.33744775 0.210704343 0.05004988 0.27938543 ## t0 -1.75064334 -2.78634793 -1.26680060 -1.729459232 -2.60528073 -1.86775101 ## [,7] [,8] [,9] [,10] [,11] [,12] ## b1 0.1242662 0.1685503 0.02285856 0.1965128 0.1384761 -0.1783804 ## b2 0.2630983 0.1563505 0.32340648 0.1680148 0.4791735 0.2025268 ## b3 0.6912657 0.7539752 0.21982543 0.5844972 0.5919369 0.4334701 ## A 0.2050011 0.5426899 0.13960102 0.4180867 0.4450346 0.2092065 ## v1 1.3145116 1.2392363 0.81793618 1.5938252 1.4879329 0.7255356 ## v2 0.5693084 0.4971342 0.32438670 0.3977475 0.3670363 0.1248367 ## t0 -3.1241916 -2.1788509 -2.25873295 -1.3214511 -1.8642588 -2.7759320 ## [,13] [,14] [,15] [,16] [,17] [,18] ## b1 0.2058493 0.009413219 0.2026047 0.09626992 0.16157925 0.03773296 ## b2 0.2836621 -0.071166522 0.1187272 0.26290527 0.17193572 -0.01895990 ## b3 0.4004229 0.301991774 0.1302980 0.61103915 0.24588443 0.34190158 ## A 0.6057064 0.143238731 0.3268137 0.31840193 0.50836559 0.49358757 ## v1 1.6789963 0.999523427 2.0098674 1.39139828 1.31009377 0.66862124 ## v2 0.4274261 0.444679107 0.3446718 0.36219300 0.07455863 0.47500308 ## t0 -2.2730857 -2.881332602 -2.4249129 -1.93661795 -1.87873689 -2.30251849 ## [,19] [,20] ## b1 0.1225545 0.07948995 ## b2 0.1557267 0.32662834 ## b3 0.5413826 0.29090185 ## A 0.4638920 0.54914061 ## v1 1.1878292 1.49936441 ## v2 0.3606136 0.30283934 ## t0 -2.2716049 -1.85730079  Create data Using the likelihood function, I now run a for loop to create data and fill that empty matrix we made at the start! The likelihood returns both rt and resp, so the lower two lines fill in the appropriate columns in the data frame. The for loop runs for each subject, and so I input the subj_random_effect for each participant and fill the appropriate place in the data structure.\nNOTE: It is important to check the data and general trends from the output. This can give an indication of how different parameters and covariance affect the data.\nfor (i in 1:n.subj){ tmp\u0026lt;- log_likelihood(subj_random_effects[,i],sample=TRUE,data=data[data$subject==i,]) data$rt[data$subject==i]=tmp$rt data$resp[data$subject==i]=tmp$response } head(data) ## subject rt resp condition ## 1 1 1.7090777 2 1 ## 2 1 0.9500044 1 2 ## 3 1 1.1687292 2 3 ## 4 1 0.9118316 2 1 ## 5 1 1.2868745 1 2 ## 6 1 1.0794218 2 3 We now have synthetic data which; - is generated using our model - uses theta values we input - uses a covariance structure we created\nThis means that these objects can be recovered.\n  Recovery I now run a typical PMwG sampling exercise. Here the likelihood is unchanged from the PMwG documentation. I use broad priors and do not specify any start points. Following the adaptation stage, i take 1000 samples from the posterior.\nNOTE: Running this on your own machine may be time consuming (and resource consuming). You may want to run it on a grid or dial down the number of cores used in sampling and wait it out.\n library(pmwg) # Specify the log likelihood function ------------------------------------------- lba_loglike \u0026lt;- function(x, data, sample = FALSE) { x \u0026lt;- exp(x) if (any(data$rt \u0026lt; x[\u0026quot;t0\u0026quot;])) { return(-1e10) } if (sample){ data$rt=NA data$resp = NA } bs \u0026lt;- x[\u0026quot;A\u0026quot;] + x[c(\u0026quot;b1\u0026quot;, \u0026quot;b2\u0026quot;, \u0026quot;b3\u0026quot;)][data$condition] if (sample) { out \u0026lt;- rtdists::rLBA(n = nrow(data), A = x[\u0026quot;A\u0026quot;], b = bs, t0 = x[\u0026quot;t0\u0026quot;], mean_v = x[c(\u0026quot;v1\u0026quot;, \u0026quot;v2\u0026quot;)], sd_v = c(1, 1), distribution = \u0026quot;norm\u0026quot;, silent = TRUE) data$rt \u0026lt;- out$rt data$resp \u0026lt;- out$resp } else { out \u0026lt;- rtdists::dLBA(rt = data$rt, response = data$resp, A = x[\u0026quot;A\u0026quot;], b = bs, t0 = x[\u0026quot;t0\u0026quot;], mean_v = list(x[\u0026quot;v1\u0026quot;],x[ \u0026quot;v2\u0026quot;]), sd_v = c(1, 1), distribution = \u0026quot;norm\u0026quot;, silent = TRUE) bad \u0026lt;- (out \u0026lt; 1e-10) | (!is.finite(out)) out[bad] \u0026lt;- 1e-10 out \u0026lt;- sum(log(out)) } if (sample){return(data)} if (!sample){return(out)} } # Specify the parameters and priors ------------------------------------------- # Vars used for controlling the run pars \u0026lt;- c(\u0026quot;b1\u0026quot;,\u0026quot;b2\u0026quot;,\u0026quot;b3\u0026quot;, \u0026quot;A\u0026quot;,\u0026quot;v1\u0026quot;,\u0026quot;v2\u0026quot;,\u0026quot;t0\u0026quot;) priors \u0026lt;- list( theta_mu_mean = rep(0, length(pars)), theta_mu_var = diag(rep(9, length(pars)))) # Create the Particle Metropolis within Gibbs sampler object ------------------ sampler \u0026lt;- pmwgs( data = data, pars = pars, prior = priors, ll_func = lba_loglike ) # start the sampler --------------------------------------------------------- sampler \u0026lt;- init(sampler) # i don\u0026#39;t use any start points here # Sample! ------------------------------------------------------------------- burned \u0026lt;- run_stage(sampler, stage = \u0026quot;burn\u0026quot;,iter = 500, particles = 100, n_cores = 16) #epsion will set automatically to 0.5 adapted \u0026lt;- run_stage(burned, stage = \u0026quot;adapt\u0026quot;, iter = 5000, particles = 100, n_cores = 16) sampled \u0026lt;- run_stage(adapted, stage = \u0026quot;sample\u0026quot;, iter = 1000, particles = 100, n_cores = 16)  We now have samples from our model which is fitted to the simulated data! You should check that the sampling process worked well before continuing on. To check this process, see the guide in the PMwG documentation.\n Checking and Simulating posterior predictive data We now have 1000 posterior samples. From here, there are several steps we want to take to check how well parameters recover. First, we’ll check the mean posterior theta estimates. Then we’ll check the alpha (random effects) estimates, before checking the covariance matrix. For extra checking, I’ll then simulate data from the posterior to see how well the data recovers.\nChecking the theta, alpha and covariance matrix The first check I do is to see whether our ptm values and subj_random_effect recovered following simulation and sampling. I first do this at the group (theta) level;\ntheta \u0026lt;- apply(sampled$samples$theta_mu[,sampled$samples$stage==\u0026quot;sample\u0026quot;],1,mean) #gets the exp(theta) #done for convenience of interpretation ## b1 b2 b3 A v1 v2 t0 ## 0.8832800 1.0173988 1.4128471 1.5383791 3.3117580 1.1322673 0.1540344 exp(ptm) ## b1 b2 b3 A v1 v2 t0 ## 1.1051709 1.3498588 1.6487213 1.4918247 3.3201169 1.3498588 0.1353353 And then at the subject (alpha) level;\nalpha \u0026lt;- apply(sampled$samples$alpha[,,sampled$samples$stage==\u0026quot;sample\u0026quot;],1:2,mean) exp(alpha) ## 1 2 3 4 5 6 7 ## b1 0.7827243 0.9245987 0.7371639 0.8802455 0.9560409 0.8955390 0.82912937 ## b2 1.0776528 0.8313766 0.8328184 1.5171650 1.1111514 1.1415803 0.98725643 ## b3 1.4462429 1.1987399 2.8858160 1.7338225 1.1443454 1.6979257 1.58823062 ## A 1.2372040 1.6414723 1.5708520 1.4744584 1.7530787 1.5733772 1.42484902 ## v1 2.0723960 2.0740149 3.5780296 4.6794195 2.8813526 3.0778612 3.65466091 ## v2 0.9351639 0.7248956 1.0610496 0.9406761 1.1274176 1.0988563 1.23011158 ## t0 0.2202784 0.1229796 0.2960367 0.1865906 0.1174551 0.1609932 0.09479353 ## 8 9 10 11 12 13 14 ## b1 0.8361656 0.8167511 0.7924110 0.9002450 0.69403112 0.9664530 0.88863815 ## b2 0.6926256 1.0854374 0.7986617 1.4237268 0.96883862 1.1169777 0.81015438 ## b3 1.7298796 0.8970793 1.4943264 1.6228472 1.31072072 1.2897463 1.18800982 ## A 1.8387425 1.1814593 1.6525283 1.5502767 1.57883116 1.6410829 1.41700250 ## v1 3.2886217 2.0794062 4.8889854 4.3015547 2.07489764 4.8543457 2.73883101 ## v2 1.1496684 1.1439376 1.1135874 1.1787368 1.11566670 1.1968833 1.63773055 ## t0 0.1869287 0.1638233 0.3126664 0.1922919 0.09447705 0.1224144 0.09564317 ## 15 16 17 18 19 20 ## b1 1.0291379 0.9820875 1.0576247 0.9419080 0.8476923 1.0358966 ## b2 0.7931455 1.2592877 1.0747271 0.9097072 1.0572080 1.2957602 ## b3 0.8598098 1.7561695 1.3005711 1.2929230 1.6198270 1.3100812 ## A 1.3285388 1.4552237 1.6838932 1.7658242 1.5804548 1.8069369 ## v1 6.7084610 4.0459888 3.7720199 2.2243421 3.1182496 4.5875267 ## v2 1.4177665 1.2421040 0.8438141 1.6249084 1.2687774 1.0955937 ## t0 0.1056549 0.1586988 0.1706621 0.1210396 0.1335432 0.1640788 exp(subj_random_effects) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## b1 1.0698792 1.18052156 0.8616525 1.0077446 1.25233908 1.0505038 1.13231721 ## b2 1.5810594 1.01349667 1.0861553 1.5847698 1.35280872 1.2649702 1.30095455 ## b3 1.7989528 1.46079588 3.1265433 1.7744359 1.38487043 1.8961407 1.99624056 ## A 1.0477341 1.60317273 1.3586326 1.4905036 1.45768087 1.4372163 1.22752645 ## v1 2.2520081 2.05856866 3.5213098 4.8296340 2.97284855 3.1347669 3.72293224 ## v2 1.2324928 1.03178471 1.4013664 1.2345473 1.05132353 1.3223169 1.76704460 ## t0 0.1736622 0.06164594 0.2817316 0.1773803 0.07388239 0.1544707 0.04397247 ## [,8] [,9] [,10] [,11] [,12] [,13] [,14] ## b1 1.1835877 1.0231218 1.2171509 1.1485222 0.8366241 1.2285680 1.00945766 ## b2 1.1692359 1.3818269 1.1829541 1.6147392 1.2244929 1.3279842 0.93130680 ## b3 2.1254323 1.2458592 1.7940886 1.8074860 1.5426012 1.4924557 1.35255010 ## A 1.7206290 1.1498150 1.5190524 1.5605442 1.2326995 1.8325463 1.15400527 ## v1 3.4529753 2.2658188 4.9225426 4.4279333 2.0658373 5.3601733 2.71698668 ## v2 1.6440032 1.3831821 1.4884682 1.4434504 1.1329634 1.5333058 1.55998953 ## t0 0.1131715 0.1044828 0.2667479 0.1550111 0.0622914 0.1029939 0.05606001 ## [,15] [,16] [,17] [,18] [,19] [,20] ## b1 1.22458829 1.1010562 1.175366 1.0384539 1.1303807 1.0827347 ## b2 1.12606272 1.3007035 1.187601 0.9812187 1.1685068 1.3862862 ## b3 1.13916786 1.8423449 1.278752 1.4076217 1.7183810 1.3376333 ## A 1.38654314 1.3749288 1.662572 1.6381828 1.5902512 1.7317641 ## v1 7.46232804 4.0204679 3.706521 1.9515447 3.2799533 4.4788415 ## v2 1.41152663 1.4364761 1.077409 1.6080192 1.4342091 1.3536970 ## t0 0.08848583 0.1441908 0.152783 0.1000067 0.1031465 0.1560934 These seem to recover quite accurately. Next though, we need to check the covariance matrix;\n sig\u0026lt;-sampled$samples$theta_sig[,,sampled$samples$stage==\u0026quot;sample\u0026quot;] cov\u0026lt;-apply(sig,3,cov2cor) cov2\u0026lt;-array(cov, c(length(pars),length(pars),1000)) ##1000 is the length of posterior sampling colnames(cov2)\u0026lt;-pars rownames(cov2)\u0026lt;-pars apply(cov2,1:2, mean) ## b1 b2 b3 A v1 v2 ## b1 1.00000000 0.27435761 0.02309705 0.03536069 0.16886207 0.18033331 ## b2 0.27435761 1.00000000 0.13002183 -0.02399570 0.10831885 0.06176004 ## b3 0.02309705 0.13002183 1.00000000 0.08640149 0.07622340 0.02414490 ## A 0.03536069 -0.02399570 0.08640149 1.00000000 0.12468679 0.11164222 ## v1 0.16886207 0.10831885 0.07622340 0.12468679 1.00000000 0.11637253 ## v2 0.18033331 0.06176004 0.02414490 0.11164222 0.11637253 1.00000000 ## t0 -0.25814657 -0.11025466 0.17354185 0.03909197 0.08641464 -0.20071341 ## t0 ## b1 -0.25814657 ## b2 -0.11025466 ## b3 0.17354185 ## A 0.03909197 ## v1 0.08641464 ## v2 -0.20071341 ## t0 1.00000000 cov2cor(sigmaC) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 1.0 0.2 0.2 0.2 0.2 0.2 0.2 ## [2,] 0.2 1.0 0.2 0.2 0.2 0.2 0.2 ## [3,] 0.2 0.2 1.0 0.2 0.2 0.2 0.2 ## [4,] 0.2 0.2 0.2 1.0 0.2 0.2 0.2 ## [5,] 0.2 0.2 0.2 0.2 1.0 0.2 0.2 ## [6,] 0.2 0.2 0.2 0.2 0.2 1.0 0.2 ## [7,] 0.2 0.2 0.2 0.2 0.2 0.2 1.0  Simulate Posterior Following these checks, I simulate posterior predictive data from the posterior model estimates. For simulating from the posterior, I follow a similar method to that shown here. First taking some random posterior samples, and then using our likelihood function to simulate data (similar to the earlier synthetic data step). In this example, I use 20 posterior samples (this is a good start - you may want to use more). This means my posterior predictive data will be 20 times the length of my simulated data.\ngenerate.posterior \u0026lt;- function(sampled, n){ #this function uses sampled$ll_func as the likelihood function to sample from n.posterior=n # Number of samples from posterior distribution. pp.data=list() S = sampled$n_subjects data=sampled$data data$subject\u0026lt;- as.factor(data$subject) sampled_stage = length(sampled$samples$stage[sampled$samples$stage==\u0026quot;sample\u0026quot;]) for (s in 1:S) { cat(s,\u0026quot; \u0026quot;) iterations=round(seq(from=(sampled$samples$idx-sampled_stage) , to=sampled$samples$idx, length.out=n.posterior)) #grab a random iteration of posterior samples for (i in 1:length(iterations)) { #for each iteration, generate data x \u0026lt;- sampled$samples$alpha[,s,iterations[i]] names(x) \u0026lt;- sampled$par_names tmp=sampled$ll_func(x=x,data=data[as.integer(as.numeric(data$subject))==s,],sample=TRUE) #this line relies on your LL being setup to generate data if (i==1) { pp.data[[s]]=cbind(i,tmp) } else { pp.data[[s]]=rbind(pp.data[[s]],cbind(i,tmp)) } } } return(pp.data) } pp.data\u0026lt;-generate.posterior(sampled,20) tmp=do.call(rbind,pp.data) glimpse(tmp)  ## Rows: 120,000 ## Columns: 5 ## $ i \u0026lt;int\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ subject \u0026lt;fct\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ rt \u0026lt;dbl\u0026gt; 0.8109699, 1.3727252, 1.0959211, 0.4747878, 0.9924096, 2.803… ## $ resp \u0026lt;dbl\u0026gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, … ## $ condition \u0026lt;int\u0026gt; 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, … Now we have data from the posterior, we can plot this data against our synthetic data.\n#functions to find the 10th, 30th, 50th, 70th and 90th quantiles q10=function(x) quantile(x,prob=.10,na.rm = TRUE) q30=function(x) quantile(x,prob=.30,na.rm = TRUE) q50=function(x) quantile(x,prob=.50,na.rm = TRUE) q70=function(x) quantile(x,prob=.70,na.rm = TRUE) q90=function(x) quantile(x,prob=.90,na.rm = TRUE) #posterior rt data across quantiles posterior \u0026lt;- tmp %\u0026gt;% group_by(i,condition) %\u0026gt;% summarise(\u0026quot;0.1\u0026quot;=q10(rt), \u0026quot;0.3\u0026quot;=q30(rt), \u0026quot;0.5\u0026quot;=q50(rt), \u0026quot;0.7\u0026quot;=q70(rt), \u0026quot;0.9\u0026quot;=q90(rt))%\u0026gt;% ungroup() #reshape posterior\u0026lt;-posterior %\u0026gt;% pivot_longer( cols = starts_with(\u0026quot;0\u0026quot;), names_to = \u0026quot;quantile\u0026quot;, names_prefix = \u0026quot;\u0026quot;, values_to = \u0026quot;rt\u0026quot;, values_drop_na = TRUE ) #synthetic rt data across quantiles synthetic \u0026lt;- data %\u0026gt;% group_by(condition) %\u0026gt;% summarise(\u0026quot;0.1\u0026quot;=q10(rt), \u0026quot;0.3\u0026quot;=q30(rt), \u0026quot;0.5\u0026quot;=q50(rt), \u0026quot;0.7\u0026quot;=q70(rt), \u0026quot;0.9\u0026quot;=q90(rt))%\u0026gt;% ungroup() #resahpe synthetic\u0026lt;-synthetic %\u0026gt;% pivot_longer( cols = starts_with(\u0026quot;0\u0026quot;), names_to = \u0026quot;quantile\u0026quot;, names_prefix = \u0026quot;\u0026quot;, values_to = \u0026quot;rt\u0026quot;, values_drop_na = TRUE ) synthetic$condition\u0026lt;-as.factor(synthetic$condition) posterior$condition\u0026lt;-as.factor(posterior$condition) #plot - this is only for RT. Also may want to plot accuracy ggplot(posterior, aes(x=as.numeric(quantile), y= rt))+ geom_point(aes(colour = condition))+ xlab(\u0026quot;Quantile\u0026quot;)+ scale_color_manual(values = c(\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;green\u0026quot;))+ geom_line(aes(colour = condition))+ scale_color_manual(values = c(\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;green\u0026quot;))+ geom_point(data=synthetic, aes(x=as.numeric(quantile), y= rt, fill = condition), shape = 24, size=3, alpha = .5)+ theme_bw()+ scale_fill_manual(values = c(\u0026quot;grey\u0026quot;, \u0026quot;black\u0026quot;, \u0026quot;navy\u0026quot;)) This graph shows RT on the y-axis across quantiles (x-axis) for both the simulated (grey triangles) and posterior recovery data (shown as coloured dots for each posterior iteration and lines for the means). Here we see the posterior recovered samples appear to match the synthetic data quite well, as we expected.\n  Conclusion Simulation and recovery practises are important for any modelling exercise. It is important to be familiar with the methods to perform simulation and recovery so you can be confident in your log likelihood function being reliable and valid (in regards to theory). This blog post aimed to explain methods of simulation and recovery for the PMwG package, and provide a framework to allow such practices.\n ","date":1615939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615939200,"objectID":"8fbc0882b97f7a4322140e6736417aa2","permalink":"/post/2021-03-17-recovery/recovery_pmwg/","publishdate":"2021-03-17T00:00:00Z","relpermalink":"/post/2021-03-17-recovery/recovery_pmwg/","section":"post","summary":"This blog post presents a brief guide on how to create synthetic data, and perform parameter recovery, using the PMwG sampler. Parameter recovery is highly important in cognitive modelling procedures as it provides an indication of parameter identifiability (and in a way, model “power”).","tags":["pmwg","modelling"],"title":"Simulation and Recovery with PMwG","type":"post"},{"authors":null,"categories":["R"],"content":"  There has been substantial research and debate over the non-decision time parameter of evidence accumulation models. For LBA and diffusion models, there is often a discrepancy, where diffusion models show longer t0 times, with the LBA known to poorly estimate this parameter. Researchers are aware of this, however, the issue is largely ignored. Research into this parameter has used neural measurement data and muscular activity to provide useful estimates of t0 (i.e. the upper and lower limits of the parameter) to give a sense of how accurate this estimation is.\nRecently, following a paper by Gunawan et al., (2020), a new method of Bayesian hierarchical estimation was outlined which shows more reliable parameter estimation. Following several modelling exercises using this method (known as Particle Metropolis within Gibbs - PMwG), it was found that t0 estimates were much improved from previous DE-MCMC sampling methods. “Improved” being that estimates of t0 often centered around 0.01 seconds, whereas in PMwG, this was closer to 0.1 seconds - much more reasonable and in line with literature. So maybe it wasn’t so much a problem with the model, but rather a problem with the model estimation. In this blog post, i explore why this might be.\nFirst, parameter recovery First of all, I’ll make sure this holds in a parameter recovery exercise - after all, we would like to know if the LBA is still doing a not-so-great job of figuring out t0. In this simulation exercise, I show t0 recovering at 3 different values for a single t0 LBA model (i.e. one t0 for all simulated conditions) and 3 lots of t0 values for a three t0 model (i.e. each condition has a t0 value). In the code below, small, medium and large refer to the input t0 parameters (small = fast t0, large = slow t0).\nSingle t0  Table 1: Large t0   t0    Actual.t0 0.2000000  Recovered 0.2254268     Table 1: Medium t0   t0    Actual.t0 0.1000000  Recovered 0.1715356     Table 1: Small t0   t0    Actual.t0 0.0300000  Recovered 0.0836326    Large  Medium   t0 varies with conds  Table 2: Large t0   t0    Actual.t01 0.1500000  Actual.t02 0.2000000  Actual.t03 0.2500000  Recovered.t01 0.1227897  Recovered.t02 0.1991755  Recovered.t03 0.1987277     Table 2: Medium t0   t0    Actual.t01 0.0500000  Actual.t02 0.1000000  Actual.t03 0.1500000  Recovered.t01 0.0733236  Recovered.t02 0.1138871  Recovered.t03 0.1758666     Table 2: Small t0   t0    Actual.t01 0.0300000  Actual.t02 0.0600000  Actual.t03 0.0900000  Recovered.t01 0.0507706  Recovered.t02 0.0972373  Recovered.t03 0.1111606    So, it looks like t0 recovers relatively well, but maybe overestimates smaller values. This means that the LBA may still not perfectly estimate actual t0 values, but could also come from the variance in the individual subject synthetic parameters. One thing is for sure though, t0 is recovered at reasonable values compared to old DE-MCMC. So what could help this estimation method?\n  Log Transform One answer to this question is the log transformation of the parameter vector. This was proposed in Gunawan et al., (2020) so that values drawn from PMwG, which are on the real number line, could be used with the LBA - which requires positive-definite values. Hence, when using the LBA in PMwG, we take the exponent of the proposal parameters to calculate the likelihood - where we return the log of the likelihood.\nEssentially by doing the log transform of the parameters, all particle values are sampled from the real number line. This makes it starightforward to specify their joint distribution as a multivariate normal (with full covariance matrix structure). Previously, we would’ve assumed that the prior joint distribution of the parameters was an uncorrelated, truncated (at zero) univariate normal. In this new approach, with the log transform (which in practice is just taking the exponential of the values on the real line), there are two key advantages; increasing sampling efficiency (sampling from the real line) and using more informed prior knowledge. further, previous methods could lead to overconfidence in the precision of estimation, and underestimation of the magnitide of individual differences - which could be key in estimating t0. With the covariance matrix able to be estimated, this links to the next section, but also, the log transform is necessary to better estimate this structure.\nTo test the log transform, I use the Forstmann et al., (2008) dataset reported in Gunawan et al., (2020) for a 3 threshold LBA. Using the PMwG sampler, I fit the model twice, using varying likelihood functions - one which takes the exponent of the proposed values and one which returns bad likelihoods for proposed values below 0 - i.e. method one is the log transformed way, method two is the untransformed (but protected against negative values) way.\nThe results are shown below.\n Table 3: t0 estimated values on the exponential scale (normal way) and on the real number line (logged)   t0    Log Transformed 0.1207189  Truncated values 0.1004302    alpha values   covariance matrix In this section, I investigate whether the covariance matrix could be a main cause for better estimates of t0. As mentioned earlier, PMwG samples from the real line, making it easy to specify the parameters (particles) distribution as an unconstrained multivariate normal distribution with full covariance structure. Using the particle approach has an advantage over other MCMC methods as we can jointly estimate the density of parameters, which enables the covariance matrix to be informed, which then constrains proposed particles.\nTo test whether the covariance matrix is a main cause for more accurate t0 estimation, I again ran the above model of the Forstmann et al., (2008) data set (with the log transform) over two different iterations. The first iteration ran PMwG as the R package defaults too. In the second iteration, I changed the v-half parameter, which is the hyperparameter on Σ prior (Half-t degrees of freedom). We tend to use v-half = 1. But to constrain the covariance matrix, I set v-half to 1000, essentially rendering the matrix useless.\nThe following shows the results of this change.\n Table 4: t0 estimated with and without the covariance structure (using a high v-half)   t0    Exponential with Covariance 0.1207189  Without Covariance 0.1058109    alpha values Evidently, removing the restraint from the covariance matrix leads to less reliable sampling and worse t0 estimates. Evidently, it is not directly one reason that t0 estimates more reliably in PMwG, but a combination of things.\n  a comment on hierarchical shrinkage Evidently, although the sampler does a good job in recovering most values of t0, we still see some hierarchical shrinkage, especially with larger t0 values. This is to be expected with hierarchical Bayesian sampling models, however, should still be considered when reporting results.\n References Forstmann, B. U., Dutilh, G., Brown, S., Neumann, J., Von Cramon, D. Y., Ridderinkhof, K. R., \u0026amp; Wagenmakers, E. J. (2008). Striatum and pre-SMA facilitate decision-making under time pressure. Proceedings of the National Academy of Sciences, 105(45), 17538-17542.\nGunawan, D., Hawkins, G. E., Tran, M. N., Kohn, R., \u0026amp; Brown, S. D. (2020). New estimation approaches for the hierarchical Linear Ballistic Accumulator model. Journal of Mathematical Psychology, 96, 102368.\n ","date":1615852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615852800,"objectID":"bd07a9fc0f76be1739f2c7675922d8e2","permalink":"/post/2021-03-16-t0-estimation/why-does-t0-estimate-well-/","publishdate":"2021-03-16T00:00:00Z","relpermalink":"/post/2021-03-16-t0-estimation/why-does-t0-estimate-well-/","section":"post","summary":"There has been substantial research and debate over the non-decision time parameter of evidence accumulation models. For LBA and diffusion models, there is often a discrepancy, where diffusion models show longer t0 times, with the LBA known to poorly estimate this parameter.","tags":["pmwg","modelling"],"title":"Why does t0 estimate well in PMwG?","type":"post"},{"authors":["Reilly Innes"],"categories":null,"content":"","date":1614816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614816000,"objectID":"2263812bb8adfdbe224d91c1897fb965","permalink":"/publication/innesthesis2021/","publishdate":"2021-03-04T00:00:00Z","relpermalink":"/publication/innesthesis2021/","section":"publication","summary":"Everyday we are faced with a myriad of tasks to complete, ranging from the most simple to highly complex. Our ability to complete such tasks is limited by inherent mechanisms which allow us to focus our attention and perceive the optimal amount of information needed to do so. As more information becomes available, and as a result of our propensity to multitask, these cognitive limits are pushed and stretched. In doing so, we often ignore important task relevant information, or our performance is inhibited. To fully understand the interplay of these factors, we need to be able to measure and evaluate workload. In this thesis I investigate the construct of cognitive workload, which is inherently limited by our overall capacity, through a measure used predominantly in applied driver distraction literature. From this, I present a body of work that expands upon theoretical underpinnings and new applications of this measure. In the theoretical stream, I show the usefulness, reliability, and applicability, of this measure in lab-based scenarios, whilst in the applied stream, I show three novel uses of the measure in both theoretical and real-world scenarios, as well as developing analyses applicable to such scenarios. The research in this thesis has implications and applications across a broad range of research areas, ranging from theoretical, in areas such as methodological development, to highly applied, in areas such as aviation environment evaluation. In the interest of openness and replicability, all data (from student cohorts), analysis and further appendices from this thesis can be found at https://osf.io/ayp6d/.","tags":null,"title":"Behavioural Measurement of Cognitive Workload","type":"publication"},{"authors":["Reilly Innes","Jon-Paul Cavallaro","Gavin Cooper","Caroline Kuhne","Guy Hawkins","Scott Brown"],"categories":null,"content":"   ","date":1612346400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612346400,"objectID":"ce8f281203a969efc39ba35f6d5e37c7","permalink":"/talk/ampc2021/","publishdate":"2021-02-03T10:00:00Z","relpermalink":"/talk/ampc2021/","section":"talk","summary":"A brief introduction to PMwG sampling and the associated R package.","tags":[],"title":"Particle Metropolis within Gibbs; An R package for Bayesian Hierarchical Modelling","type":"talk"},{"authors":[],"categories":null,"content":" Download Slides   ","date":1608591600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608591600,"objectID":"b043ba5c56e91dae914a89d89974af1d","permalink":"/talk/amsterdam2/","publishdate":"2020-12-23T00:00:00Z","relpermalink":"/talk/amsterdam2/","section":"talk","summary":"A talk given to colleagues at the University of Amsterdam","tags":[],"title":"PMwG sampler and stuff we've been doing with it","type":"talk"},{"authors":["Alexander Todd","Reilly Innes","Alireza Mazloumi Gavgani","Timothy Whelan","Ebon Baxter","Murray Bennett","Scott Brown","Eugene Nalivaiko","Ami Eidels"],"categories":null,"content":" Download Slides   ","date":1602842400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602842400,"objectID":"d0b6a7051c8f0d6616853d8023697a35","permalink":"/talk/cbmhr2020/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/talk/cbmhr2020/","section":"talk","summary":"Cognitive workload can affect task performance in a number of ways. In occupations where workload is a constant factor, it is important that individuals are selected and trained to cope with high task demands. Literature using physiological measures to assess performance under load has suggested that individuals can enter two states when faced with increasing task demands; threat or challenge. These physiological states reflect mental subprocesses, where individuals see the task as challenging (wanting to increase performance) or threatening (anxiety inducing). The current study aims to draw links between behavioural and physiological outcomes of cognitive workload. Two studies were conducted; an online study -- where participants completed a tracking task and simultaneous behavioural workload measure as well as a pre and post experiment questionnaire; and an in lab study – where participants completed two workload inducing tasks (the Cedar Operator Workload Assessment Tool and a collision detection task) over two sessions. During these sessions, participants were equipped with physiological measurement tools and completed a behavioural workload measure. Results so far have shown that the workload measure captured changes in task difficulty and was related to subjective cognitive workload. Physiological measures also found differences between task difficulty conditions. Participants mental state could be inferred from the physiological measurement. Results so far are promising, with physiological measures successfully differentiating between individuals state of arousal and the workload measures differentiating between levels of cognitive workload as difficulty changed. Going forward, we aim to show a measure of workload and state over time, to observe the effects of state on performance and experienced workload, and further develop a unified measure of task performance. This research has implications and applications for further work with assessment and training of air traffic controllers.","tags":[],"title":"Threat or challenge? Measuring cognitive workload and physiological arousal to infer mental state","type":"talk"},{"authors":["Alexander Thorpe","Reilly Innes","James Townsend","Rachel Heath","Keith Nesbitt","Ami Eidels"],"categories":null,"content":"Supplementary material can be found here.\n","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"52dc1aca81df32df259093a1a3fc51bd","permalink":"/publication/thorpe2020/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/publication/thorpe2020/","section":"publication","summary":"The detection response task (DRT) is a measure of workload that can assess the cognitive demands of real-world multitasking. It can be configured to present simple stimuli of several modalities, including auditory and visual signals. However, the concurrent presentation of the DRT stimuli alongside another task could cause dual-task interference, and the extent of this interference could be different based on the DRT’s configuration. It is necessary to consider the characteristics of the DRT stimulus, such as modality, to identify a minimally intrusive stimulus. Fifty participants completed a computer-based one-dimensional tracking task alongside a DRT. The DRT’s stimuli varied in their modality (visual/auditory), while the tracking task varied in its workload demand (low/high). DRT performance was modelled using a shifted-Wald model, while the tracking task was assessed using systems factorial technology (SFT), a non-parametric methodology capable of capturing a cognitive system’s workload capacity. To allow the latter’s use, we developed a method of transforming continuous tracking data into a discrete form akin to response times. Analysis of DRT data found little evidence that the DRT’s modality affected processing efficiency, while SFT analysis found limited-capacity processing on the tracking task across both DRT modalities. These findings suggest DRT modality had little effect on the level of interference between the two tasks.","tags":null,"title":"Assessing cross-modal interference in the detection response task","type":"publication"},{"authors":["Reilly Innes","Zachary Howard","Alexander Thorpe","Ami Eidels","Scott Brown"],"categories":null,"content":"Supplementary material can be found here.\n","date":1597190400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597190400,"objectID":"3aeb6eaa16de892253c6c00e54b7a7ce","permalink":"/publication/inneshelic/","publishdate":"2020-08-12T00:00:00Z","relpermalink":"/publication/inneshelic/","section":"publication","summary":"To test the effects of enhanced display information (“symbology”) on cognitive workload in a simulated helicopter environment, using the detection response task (DRT). Workload in highly demanding environments can be influenced by the amount of information given to the operator and consequently it is important to limit potential overload. Participants (highly trained military pilots) completed simulated helicopter flights, which varied in visual conditions and the amount of information given. During these flights, participants also completed a DRT as a measure of cognitive workload. With more visual information available, pilots’ landing accuracy was improved across environmental conditions. The DRT is sensitive to changes in cognitive workload, with workload differences shown between environmental conditions. Increasing symbology appeared to have a minor effect on workload, with an interaction effect of symbology and environmental condition showing that symbology appeared to moderate workload. The DRT is a useful workload measure in simulated helicopter settings. The level of symbology-moderated pilot workload. The increased level of symbology appeared to assist pilots’ flight behavior and landing ability. Results indicate that increased symbology has benefits in more difficult scenarios. The DRT is an easily implemented and effective measure of cognitive workload in a variety of settings. In the current experiment, the DRT captures the increased workload induced by varying the environmental conditions, and provides evidence for the use of increased symbology to assist pilots.","tags":null,"title":"The Effects of Increased Visual Information on Cognitive Workload in a Helicopter Simulator","type":"publication"},{"authors":["Reilly Innes","Nathan Evans","Zachary Howard","Ami Eidels","Scott Brown"],"categories":null,"content":"Supplementary material can be found here.\n","date":1596499200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596499200,"objectID":"336d7d5292276802cbd0c9ae8868b805","permalink":"/publication/innesbroad2020/","publishdate":"2020-08-04T00:00:00Z","relpermalink":"/publication/innesbroad2020/","section":"publication","summary":"The present research applied a well-established measure of cognitive workload in driving literature to an in-lab paradigm. We then extended this by comparing the in-lab version of the task to an online version. The accurate and objective measurement of cognitive workload is important in many aspects of psychological research. The detection response task (DRT) is a well-validated method for measuring cognitive workload that has been used extensively in applied tasks, for example, to investigate the effects of phone usage or passenger conversation on driving, but has been used sparingly outside of this field.The study investigated whether the DRT could be used to measure cognitive workload in tasks more commonly used in experimental cognitive psychology and whether this application could be extended to online environments. We had participants perform a multiple object tracking (MOT) task while simultaneously performing a DRT. We manipulated the cognitive load of the MOT task by changing the number of dots to be tracked. Measurements from the DRT were sensitive to changes in the cognitive load, establishing the efficacy of the DRT for experimental cognitive tasks in lab-based situations. This sensitivity continued when applied to an online environment (our code for the online DRT implementation is freely available at https://osf.io/dc39s/), though to a reduced extent compared to the in-lab situation.    The MOT task provides an effective manipulation of cognitive workload. The DRT is sensitive to changes in workload across a range of settings and is suitable to use outside of driving scenarios, as well as via online delivery.   Methodology shows how the DRT could be used to measure sources of cognitive workload in a range of human factors contexts.","tags":null,"title":"A Broader Application of the Detection Response Task to Cognitive Tasks and Online Environments.","type":"publication"},{"authors":["Reilly Innes","Scott Brown"],"categories":null,"content":"   ","date":1595250000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595250000,"objectID":"5c652bfa954cd52b5937558353e290eb","permalink":"/talk/mathpsyc2020/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/talk/mathpsyc2020/","section":"talk","summary":"A joint model for two groups performance across two cognitive tasks.","tags":[],"title":"Joint modelling group differences from military personnel","type":"talk"},{"authors":[],"categories":null,"content":"   ","date":1595250000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595250000,"objectID":"a971f504d05dd14736f683b9f32598f3","permalink":"/talk/3mt/","publishdate":"2020-08-20T00:00:00Z","relpermalink":"/talk/3mt/","section":"talk","summary":"My thesis in 3 minutes.","tags":[],"title":"Juggling your workload - 3MT","type":"talk"},{"authors":["Zachary Howard","Nathan Evans","Reilly Innes","Scott Brown","Ami Eidels"],"categories":null,"content":"Supplementary material can be found here.\n","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"01f904a7bf2d264d6f0ec5706f445d68","permalink":"/publication/howard2020/","publishdate":"2020-05-21T00:00:00Z","relpermalink":"/publication/howard2020/","section":"publication","summary":"With the advancement of technologies like in-car navigation and smartphones, concerns around how cognitive functioning is influenced by “workload” are increasingly prevalent. Research shows that spreading effort across multiple tasks can impair cognitive abilities through an overuse of resources, and that similar overload effects arise in difficult single-task paradigms. We developed a novel lab-based extension of the Detection Response Task, which measures workload, and paired it with a Multiple Object Tracking Task to manipulate cognitive load. Load was manipulated either by changing within-task difficulty or by the addition of an extra task. Using quantitative cognitive modelling we showed that these manipulations cause similar cognitive impairments through diminished processing rates, but that the introduction of a second task tends to invoke more cautious response strategies that do not occur when only difficulty changes. We conclude that more prudence should be exercised when directly comparing multi-tasking and difficulty-based workload impairments, particularly when relying on measures of central tendency.","tags":null,"title":"How is multi-tasking different from increased difficulty?","type":"publication"},{"authors":[],"categories":null,"content":" Download Slides   ","date":1585000800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585000800,"objectID":"e4a76abd4fe9441836a11a635a3893d2","permalink":"/talk/amsterdam/","publishdate":"2020-03-23T00:00:00Z","relpermalink":"/talk/amsterdam/","section":"talk","summary":"A talk given to colleagues at the University of Amsterdam","tags":[],"title":"Cognitive Workload Measurement and New Estimation Methods","type":"talk"},{"authors":["Reilly Innes","Caroline Kuhne"],"categories":null,"content":" Download Slides   ","date":1581674400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581674400,"objectID":"901c42932eb1a510e381097fa82eab87","permalink":"/talk/ampc2020/","publishdate":"2020-02-14T10:00:00Z","relpermalink":"/talk/ampc2020/","section":"talk","summary":"Military group performance on a cognitive task in comparison with students.","tags":[],"title":"Modelling decisions of the multiple object tracking task","type":"talk"},{"authors":["Reilly Innes","Caroline Kuhne"],"categories":null,"content":"Supplementary material can be found here.\n","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"5bff016ba0725c22fe959f3ba6988176","permalink":"/publication/inneslba2020/","publishdate":"2020-02-21T00:00:00Z","relpermalink":"/publication/inneslba2020/","section":"publication","summary":"Decision making is a vital aspect of our everyday functioning, from simple perceptual demands to more complex and meaningful decisions. The strategy adopted to make such decisions is often viewed as balancing elements of speed and caution, i.e. making fast or careful decisions. Using sequential sampling models to analyse decision making data can allow us to tease apart strategic differences, such as being more or less cautious, from processing differences, which would otherwise be indistinguishable in behavioural data. Our study used a multiple object tracking task where student participants and a highly skilled military group were compared on their ability to track several items at once. Using a mathematical model of decision making (the linear ballistic accumulator), we show the underpinnings of how two groups differ in performance. Results showed a large difference between the groups on accuracy, with the Royal Australian Air Force (RAAF) group outperforming students. An interaction effect was observed between groups and level of difficulty in response times, where RAAF response times slowed at a greater rate than the student group as difficulty increased. Model results indicated that the RAAF personnel were more cautious in their decisions than students, and had faster processing in some conditions. Our study shows the strength of sequential sampling models, as well as providing a first attempt at fitting a sequential sampling model to data from a multiple object tracking task.","tags":null,"title":"An LBA account of decisions in the multiple object tracking task?","type":"publication"}]