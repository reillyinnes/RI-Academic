<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Reilly Innes</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 12 May 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu1dc3f56a72840a79504e5212c1d5c006_23068_512x512_fill_lanczos_center_3.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>What do cognitive modellers do?</title>
      <link>/post/2022-05-12-decision-modellers/</link>
      <pubDate>Thu, 12 May 2022 00:00:00 +0000</pubDate>
      <guid>/post/2022-05-12-decision-modellers/</guid>
      <description>
&lt;script src=&#34;/post/2022-05-12-decision-modellers/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In this blog post, I explain what I do as a cognitive modeller - from decision making modelling, to neuroscience, fitting mathematical models to human data and more!&lt;/p&gt;
&lt;!-- more --&gt;
&lt;div id=&#34;i-thought-you-were-a-psychologist&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;I thought you were a psychologist?&lt;/h1&gt;
&lt;p&gt;I hear this too often, and technically, we are a type of psychologist in that we study (‘ology’) cognition (‘psych’). Cognition just means how we think and process things - by reading this now, you’re processing the information, a form of cognition! Cognition includes many processes we use daily, like memory, detecting signals and making decisions (that’s where we come in).&lt;/p&gt;
&lt;div id=&#34;okay-so-you-look-at-decision-making&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Okay so you look at decision making?&lt;/h2&gt;
&lt;p&gt;As a decision modeller, we look at decision making. I’ve had colleagues that have looked at all types of decision making, from the very simple decisions (“is this light red or green?”), to more complex decisions involving memory (“have you seen this image before?”), risk (“would you rather have a 90% chance at &lt;code&gt;$10&lt;/code&gt; or a 10% chance at &lt;code&gt;$90&lt;/code&gt;?) and consumer preference (”would you rather chocolate by brand A or chips by brand B).&lt;/p&gt;
&lt;p&gt;In all of these examples, we often collect lots of data from lots of people. So if you were to do one of our experiments, you’d probably be (bored) asked to make a lot of &lt;strong&gt;simple&lt;/strong&gt;, seemingly repetitive decisions about the thing we’re investigating.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;right-so-what-do-you-find-and-why-is-this-even-important&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Right, so what do you find? And why is this even important?&lt;/h2&gt;
&lt;p&gt;Understanding the decisions people make can have huge consequences for a variety of things - safety, consumer choices, health and medical decisions and even voting behavior.&lt;/p&gt;
&lt;p&gt;See when you make any decision, there are two key outcomes we’re interested in, and this holds for many applications. The &lt;strong&gt;choice&lt;/strong&gt; you make and the &lt;strong&gt;speed&lt;/strong&gt; in which you make that choice. Imagine you’re deciding on what to have for dinner, if you and your partner go back and forth over this for a long time, your partner might not be convinced by your choice. But say you made a choice instantly, this shows some pre-planning and confidence in your decision. These underlying factors that underpin both your choice and speed of making the choice (which we call response time) are what makes the topic important.&lt;/p&gt;
&lt;p&gt;Let’s take another, more consequential decision - deciding on the colour of traffic lights. If the light was difficult to see (late afternoon sun making it hard to make out the colours), you might take longer to make a decision, and in the worst circumstance, you may even make the wrong decision! If it’s night time and easy to see, then you’ll probably make a fast, accurate decision.&lt;/p&gt;
&lt;p&gt;But that’s not all that goes into a decision. What if you’re running late, then you might be biased to see a green light and more likely to make a wrong decision. What if you’re a cautious driver? You might slow down coming to the light and take longer to commit to a decision. All these elements play a part in decision making and, the coolest part, we can disentangle them.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-do-you-do-this&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How do you do this?&lt;/h2&gt;
&lt;p&gt;We do this through mathematical models. See in decision making research, we have some input (i.e., the question “is this light red or green”) and we get some output (a choice and response time), but what we really want to know is what components make up that choice. We want to understand the black box that is the processing of the information to execute a decision.&lt;/p&gt;
&lt;p&gt;So we call these models “process models”, because they help us understand the &lt;em&gt;process&lt;/em&gt; of decision making. In psychological research there are lots of examples of models, from memory models that help us understand how we encode and retrieve information, to learning models that help us understand how we update our knowledge and representations of the world. Then there are decision models which help us break down response times and choices that we make.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;decision-making-modelling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Decision-Making Modelling&lt;/h1&gt;
&lt;p&gt;Luckily, many smart people have worked on improving our models over time. When you think about it, there’s a few things a model should achieve. It should be;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;flexible and able to account for the phenomenon under many conditions&lt;/li&gt;
&lt;li&gt;not too complex - too much complexity means it’s difficult to understand or gain information from&lt;/li&gt;
&lt;li&gt;representative of the system and account for specific things we know about already&lt;/li&gt;
&lt;li&gt;testable (and falsifiable)&lt;/li&gt;
&lt;li&gt;able to produce data that is similar to what we observe.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These last two points are where mathematical models come in. Let’s take a model like this figure below. This model is for decisions in a task where someone is asked to say whether a string of letters is a word or a “non-word”.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;SDT_2.png&#34; width=&#34;386&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we can see a few features. First, there are two &lt;em&gt;distributions&lt;/em&gt; (blue and yellow). These make up the representation of “words” and “non-words” in the persons mind. You’ll notice they overlap - this is because we probably have some instances where we’re not really sure whether an item is a word or not. Secondly, you’ll notice a red line - this is labeled the criterion. If I was to take a random string of letters (say &lt;code&gt;PONK&lt;/code&gt;) and was to sample a point along this line, you would have some form of certainty of how much of a word this was. If you thought (correctly) that it was NOT a word, the value assigned here might be around 0 (middle of the yellow distribution). Because your sample of information falls on the left of the criterion, the model says that you would respond “non-word”. And that’s it!&lt;/p&gt;
&lt;p&gt;So let’s think then, if we had more tricky words, your sample might fall closer to the criterion, so you might be more likely to make a mistake. What if you had just learned the language? Then maybe those distributions would overlap more as it’s harder to decide. In other instance, you might have already heard from your friend that 90% of the items in the test are words. Here, you’ll probably be biased to say “word”, so the model moves your criterion to the left, so there is a greater chance of responding “word” (and making errors when it’s actually a “non-word”)&lt;/p&gt;
&lt;div id=&#34;including-response-times&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Including response times&lt;/h2&gt;
&lt;p&gt;This model is nice and provides an explanation about mental representations of words. But what if we want to include response times? For this, we use response time models.&lt;/p&gt;
&lt;p&gt;These models make a few assumptions about how we make decisions. The theory is that for each decision that we make, we gather &lt;em&gt;evidence&lt;/em&gt; for each option. So when deciding on dinner you might be deciding whether to cook fish or cook burgers. If you have fish in the fridge, this might give you some evidence for choosing “fish”. But burgers taste good, sooo some evidence for burgers. But ultimately you think the fish is probably better for you, so you eventually choose fish.&lt;/p&gt;
&lt;p&gt;This “eventually” point, where you make a decision, is just like the criterion from the last model. We call this a threshold. When you’ve gathered enough evidence and reached this point for a choice option, you execute the choice. The speed at which you gather evidence is called the drift rate. Finally, there is a bit of time that is spent encoding information and executing the decision (very very minute amounts of time around 200 miliseconds) and this is labeled “non-decision time”. All of these components can be seen below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Models.png&#34; width=&#34;420&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this example, we can see another word vs non-word choice. On the left, the evidence for “word” and on the right, the evidence for deciding “non-word”. This time, our stimulus is &lt;code&gt;FOLK&lt;/code&gt;. You’ll notice that the &lt;em&gt;threshold&lt;/em&gt; is lower for “non-word” - this means there is some bias, so we might’ve just seen 5 non-words in a row, or there might be very few words in this test.&lt;/p&gt;
&lt;p&gt;Secondly, you’ll see that drift rate is faster (i.e., the slope is steeper) for “word”. This means we’ve gathered evidence faster. Drift rate is thought to reflect processing speed and urgency, so the difference between drift rates is a good indicator of how easy the task is. If the difference between the correct answer and incorrect answer is really big, then the task is easy, whereas if the difference is small, the task is probably harder (similar to our overlapping distributions in the previous example).&lt;/p&gt;
&lt;p&gt;Next, you’ll also notice the parts at the start and end that are added on for non-decision time. In the model, we simply subtract this time from the response time so that we can asses the time that was spent on actually processing and deciding.&lt;/p&gt;
&lt;p&gt;Now for the maths - and this is really quite simple! The response time is something we know (let’s say it took 600 miliseconds to make this choice). Then the response time (taking away non-decision time) is found by dividing the threshold by the drift rate (slope)! As we get more complex there’s other things to conisder too – such as how high the evidence began accumulating at (it’s about the same here).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;speed-accuracy-tradeoff&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Speed-Accuracy Tradeoff&lt;/h2&gt;
&lt;p&gt;One thing you may have noticed about decision making is that a trade off exists. You can go faster and make faster decisions, but this often means more errors. You could also try and be more accurate, but this often means slowing down. This is called the speed-accuracy tradeoff and is very well studied.&lt;/p&gt;
&lt;p&gt;In our experiments, we might tell people to “be fast” sometimes, and other times to “be accurate”. The results are as you would expect - faster response times and lower accuracy for “be fast” and the opposite for “be accurate”.&lt;/p&gt;
&lt;p&gt;Now, with our models, we can work out what happens here, and the result is very neat. All we do is change our threshold! Looking at this example, it might not be immediately clear how this works, but here’s some more details about the model, which are shown below. The drift rate is actually a distribution, so it’s variable from one decision to the next. This means with a lower threshold, a slightly slow drift rate would lead to an arrow - something that is less likely with a high threshold (where we would wait for more evidence)!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Models2.png&#34; width=&#34;420&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modelling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Modelling&lt;/h2&gt;
&lt;p&gt;You might be wondering how we work out the values in these models. How can we tell whether one threshold is higher than the other? Especially with so much data for so many people!&lt;/p&gt;
&lt;p&gt;For this, we use model estimation techniques and probability theory. Using all the data, we can pick out some values to include in the model - let’s say drift rate = 2, threshold = 0.5, non-decision time = 0.1. These numbers are arbitrary alone, but when combined, give us a response time distribution. The model estimation takes in these guesses, and applies them to our observed data, and then gives us a &lt;em&gt;probability&lt;/em&gt; that these values are the &lt;em&gt;true&lt;/em&gt; values.&lt;/p&gt;
&lt;p&gt;We then repeat this process many many times, each time trying to find better guesses, until we get the &lt;em&gt;most likely parameter estimates&lt;/em&gt;. This just means we’ve found values that are likely to result in the data we observed. And the cool part - we can then put these likely values back into the model and generate new data! If the model works, the new (predicted) data should look just like what we observed!&lt;/p&gt;
&lt;p&gt;From this process, we can test all kinds of things - the different in parameters between people, the difference over different conditions and manipulations, as well as which parameters are affected by different manipulations. For example, we know that harder tasks lead to slower drift rates! From this, we can gain an insight into the process. So when the task is harder, people are slower to process the information! And then vice-versa, if people are slower to process information, it may be too hard to perceive.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;more-examples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;More examples&lt;/h2&gt;
&lt;p&gt;Imagine you’re selling a product on a website, then understanding the consumers behaviour could help you sell more products. If on test A, people clicked “buy” a lot quicker than in test B, then the model might conclude that the information was easier to absorb, or, maybe there was a lower price or more persuasive text that made them more confident in their decision.&lt;/p&gt;
&lt;p&gt;Another interesting aspect is the difference between people. In this online shopping example, we might find that person A is just slow to respond, which is caused by being more cautious compared to other people, where the other people are just slower to take in the information. We overcome this by using &lt;strong&gt;hierarchical models&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hierarchical-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hierarchical Models&lt;/h2&gt;
&lt;p&gt;Hierarchical models have two main levels - each individual has a set of values for their model parameters, and then these help to establish the group’s parameters. In this shopping example, a big company might only care about the group level, because that is the best way to sell more products to more people. But we may also want to know about each individuals behaviour.&lt;/p&gt;
&lt;p&gt;The strength of hierarchical models comes from what we call &lt;em&gt;shrinkage&lt;/em&gt;. Let’s imagine a model that guesses peoples height. These guesses may resemble a normal distribution, just like below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;dist1.png&#34; width=&#34;560&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You’ll notice that most of the guesses are around the mean, right in the middle, which makes this distribution. You’ll also notice that two of the guesses (on the right) are a bit extreme. In hierarchical modelling, shrinkage helps to pull these extreme values back into a normal range, just like below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;dist2.png&#34; width=&#34;530&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So the important point here is that the individuals inform the group, but the group also informs the individuals. This relationship helps us get the best possible estimates for our model parameters as possible, so that we can answer really difficult research questions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;you-mentioned-neuroscience&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;You mentioned neuroscience?&lt;/h1&gt;
&lt;p&gt;Alright, final point. There are now some neuroscience researchers who are using these modelling techniques along with their EEG and fMRI measures! This means that they can look to see how different processes affect the brain - i.e., what parts of the brain light up when I’m being more cautious? And what happens in clinical patients who may display different behaviour? Maybe models can help find improve behavioural therapy by helping us understand the process of the behaviour!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reilly&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Using PMwG for NOT psychology</title>
      <link>/post/2022-04-30-pmwg-fastfood/</link>
      <pubDate>Sat, 30 Apr 2022 00:00:00 +0000</pubDate>
      <guid>/post/2022-04-30-pmwg-fastfood/</guid>
      <description>
&lt;script src=&#34;/post/2022-04-30-pmwg-fastfood/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This blog post uses the &lt;code&gt;pmwg&lt;/code&gt; package for a more generic example than has previously been shown (&lt;a href=&#34;https://newcastlecl.github.io/samplerDoc/&#34;&gt;here&lt;/a&gt; for example).&lt;/p&gt;
&lt;!-- more --&gt;
&lt;div id=&#34;pmwg&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;PMwG&lt;/h1&gt;
&lt;p&gt;The particle metropolis within Gibbs sampling package (&lt;code&gt;pmwg&lt;/code&gt;) was created by a team at the University of Newcastle. The package provides functions to estimate Bayesian hierarchical models. What this means is;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Bayesian; probability based, sampling parameters to form a distribution of parameter values, not a point estimate. Gives us &lt;em&gt;uncertainty&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;hierarchical; multi levels - i.e., a group level and a subject level. Think of a school (the group mean) and classes within the school (each with their own mean)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;model-based sampling; propose lots of values for model parameters and find those which are probabilistically most liekly&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So far, the package has mainly been used for psychological applications, where lots of data is collected per subject, and this data is then fit with complex models (which cannot be fit with standard methods). For an explanation of the full method see our &lt;a href=&#34;https://newcastlecl.github.io/samplerDoc/&#34;&gt;other documentation&lt;/a&gt; or for the full maths, see &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0022249620300389&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pmwg_new&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;PMwG_new&lt;/h1&gt;
&lt;p&gt;Recently, working together with Niek Stevenson, we’ve added functionality to the PMwG package. These are not detailed here, however, I do use a couple of these functions as they are pretty neat and usable. The additional functions can be seen &lt;a href=&#34;https://github.com/niekstevenson/pmwg_new&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-would-we-use-this&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why would we use this?&lt;/h1&gt;
&lt;p&gt;Well there’s a few reasons. For this example, using PMwG is probably overkill, but it makes a nice example for other situations where we might have many observations and many individuals. These bigger data problems represent more realistic scenarios, but importantly, we would want to get a sense of the group level parameter distribution, as well as the individual subjects’ data. For example, if we had a school with many classes (say 50), where each class included 50 students, then we could estimate a model for each class, which would inform the group level model.&lt;/p&gt;
&lt;p&gt;The reason to use PMwG is when we have difficult estimation problems. Many models do not have analytic solutions, and so model estimation is often used to overcome this. Even with an analytic solution, doing hierarchical modelling can be challenging, which is where PMwG shows more usefulness. We could use &lt;code&gt;optim&lt;/code&gt; to find the most likely values for a problem, but this is not reliable and only does calculations at one level (group or individual). Further, we could use another sampler (like stan or jags), however, with PMwG we can achieve greater efficiency, there is no reliance on multiple chains or convergence (due to sampler properties) and we can reliably estimate a more complex group level model – including the multivariate normal we use as default which allows us to estimate parameter correlations. The latter point is particularly useful for models with collinearity between parameters (i.e., as x goes up, so does y). I’ll do a blog in future showing just how bad post-hoc correlations between parameters are, and how PMwG overcomes this.&lt;/p&gt;
&lt;p&gt;So that’s a lot of reasons to use this method, but lets summarise quickly.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We use PMwG to estimate statistical models (often with no analytic solution).&lt;/li&gt;
&lt;li&gt;The models we would estimate are able to have their density calculated/approximated.&lt;/li&gt;
&lt;li&gt;The data we model should have many observations (differs depending on model complexity) for a few different individuals (really want at least 20 to inform the group).&lt;/li&gt;
&lt;li&gt;PMwG allows us to do &lt;em&gt;hierarchical modelling&lt;/em&gt;. This means we have a group and individual level distribution.&lt;/li&gt;
&lt;li&gt;PMwG is Bayesian. This means we can evaluate a level of certainty of our parameter estimates for neater inference.&lt;/li&gt;
&lt;li&gt;PMwG is reliable and efficient.&lt;/li&gt;
&lt;li&gt;PMwG allows us to estimate between parameter correlations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Data&lt;/h1&gt;
&lt;p&gt;In the following example, I use the the &lt;code&gt;fastfood&lt;/code&gt; dataset from the &lt;code&gt;openintro&lt;/code&gt; package. The data includes 515 item menus from 8 different fast food restaurants. Each item has 15 metrics to descirbe the nutritional value. Here we only use the first 11 metrics due to missing data (something I’ll address using PMwG in another blog post).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(openintro)
data &amp;lt;- fastfood[,1:13]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Setup&lt;/h1&gt;
&lt;p&gt;First we need to source the pmwg_new code and set up the environment;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())

# library(pmwg)
source(&amp;quot;pmwg/variants/standard.R&amp;quot;)
library(mvtnorm)
library(openintro)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we need to clean the data. Here i include a &lt;code&gt;subject&lt;/code&gt; column. This is a requirement of the pmwg package - you must include a &lt;code&gt;subject&lt;/code&gt; column. Here, the &lt;code&gt;restaurant&lt;/code&gt; column is labelled as the subject. This is because I’m going to treat each restaurant as an individual entity, where each food item is an “observation” from their menu.&lt;/p&gt;
&lt;p&gt;I also remove the food item label for coding simplicity purposes. Next I’m going to omit any items with missing data, before finally rescaling four metrics for modelling purposes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: These four rescaled metrics don’t need to be normalised, but the values are far greater than the other measures. Here, by dividing them all by 100, I don’t have to set different priors (it’s mainly because I’m being lazy).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
names(data)[1]&amp;lt;-&amp;quot;subject&amp;quot;
data &amp;lt;- data[c(1,3:13)]
data &amp;lt;- na.omit(data)
data[,c(2,3,7,8)]&amp;lt;- data[,c(2,3,7,8)]/100&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we don’t have many observations, which makes it hard to estimate. We also probably should have more restaurants, but this serves as a nice intro example&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;likelihood-function&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Likelihood Function&lt;/h1&gt;
&lt;p&gt;The likelihood function is the engine in PMwG. The nicest part of this engine, is that you can craft it yourself, giving lots of flexibility. The likelihood function takes the proposed parameters and calculates the likelihood of these given the subjects’ (restaurants) data and the model you’re using.&lt;/p&gt;
&lt;p&gt;Before we write this though, we’re first going to check the shapes of the distributions of nutrition metrics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
par(mfrow=c(3,4))
apply(data[,-1], 2, hist, main = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;staticunnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Two things are immediately apparent that need to be accounted for in the likelihood. First, all values are greater than 0. Second, we can see that all distributions are positively skewed. We’re going to make sure we do something about these in the likelihood.&lt;/p&gt;
&lt;div id=&#34;full-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Full Function&lt;/h2&gt;
&lt;p&gt;Shown below is the full likelihood function, but I’ll break it down slowly in this section;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ll &amp;lt;- function(x, data, sample = F ){
  x=exp(x)
  if(sample == FALSE){
    n_pars = length(x)
    tmpData = as.matrix(data[,-1])
    out = sum(TruncatedNormal::dtmvnorm(tmpData, x, diag(1, nrow = n_pars), lb = rep(0,n_pars), log = T, B = 1))
    return(out)
  }else{
    n_pars = length(x)
    Restaurant = data[,1]
    tmpData = as.matrix(data[,-1])
    out = TruncatedNormal::rtmvnorm(nrow(tmpData), x, diag(1, nrow = n_pars), lb = rep(0,n_pars))
    out &amp;lt;- as.data.frame(cbind(out,Restaurant))
    return(out)
  }
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the likelihood function, we need to include three main arguments;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt; is the vector of proposed (named) parameter values&lt;/li&gt;
&lt;li&gt;&lt;code&gt;data&lt;/code&gt; is the data (note that each time using this will only use data for the one subject being estimated)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sample = FALSE&lt;/code&gt; is used to specify whether we are sampling from the model (i.e., &lt;code&gt;TRUE&lt;/code&gt;) or calculating the density ( &lt;code&gt;FALSE&lt;/code&gt; - set as the default)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;transformations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Transformations&lt;/h2&gt;
&lt;p&gt;The first thing I do in the likelihood is transform the input parameter values. Knowing that all the values in the data are positive, and therefore any proposed parameters will be positive (using a multivariate normal model), I need to ensure that the parameters are also positive.&lt;/p&gt;
&lt;p&gt;Taking the exponent of the proposed parameters means these will be strictly positive. This has implications for the implied prior, but I’ll discuss this in the advanced section later on.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Model&lt;/h2&gt;
&lt;p&gt;For this example, I was considering using a multivariate normal model, but I found a truncated multivariate normal function that makes it slightly more interesting and accurate (we’ll do some model comparison below too). When thinking about the “model” we’re using, we’re talking about the model of the data describing the data for &lt;strong&gt;each individual&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Looking at the group level distribution above, we can see a skewed distribution, but it’s worthwhile thinking about this distribution per individual restaurant as well. This means we make the assumption that each individual follows this model, and that informs a population level model (and can allow us to generate or predict other restaurants).&lt;/p&gt;
&lt;p&gt;For the truncated multivariate model I use the &lt;code&gt;dtmvnorm&lt;/code&gt; and &lt;code&gt;rtmvnorm&lt;/code&gt; functions from the &lt;code&gt;TruncatedNormal&lt;/code&gt; package for density calculation and random generation respectively.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;calculating-the-density&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Calculating the Density&lt;/h2&gt;
&lt;p&gt;Here I say that we calculate the likelihood of the data given some parameter values. All I’m doing here is testing the probability of parameters for each persons data. Before I show the functions, I like to do a small thought exercise to show how this works in practice.&lt;/p&gt;
&lt;div id=&#34;thought-exercise-for-likelihoods&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Thought exercise for likelihoods&lt;/h3&gt;
&lt;p&gt;For this example, let’s generate some data from a normal distribution. Here I make some “parameter” values which I’ll save. These parameters are just mean and standard deviation for the normal.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;truePars = c(&amp;quot;mean&amp;quot;=6,&amp;quot;sd&amp;quot;=.3)
exampleData &amp;lt;- rnorm(n=1000, mean = truePars[&amp;quot;mean&amp;quot;], sd= truePars[&amp;quot;sd&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, using this data, I’ll try and input some other parameters to see how likely these are for the data;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# first try
sum(dnorm(exampleData,mean=0,sd=1, log = T))
## [1] -18966.2
# a lot off, so lets shift the mean
sum(dnorm(exampleData,mean=7,sd=1, log = T))
## [1] -1460.141
# what if we move the sd
sum(dnorm(exampleData,mean=6,sd=.5, log = T))
## [1] -394.0612
# now lets try them together
sum(dnorm(exampleData,mean=6.5,sd=.3, log = T))
## [1] -1566.464
# very close
sum(dnorm(exampleData,mean=8,sd=.4, log = T))
## [1] -12754.75
# too far
sum(dnorm(exampleData,mean = truePars[&amp;quot;mean&amp;quot;], sd= truePars[&amp;quot;sd&amp;quot;], log = T))
## [1] -182.3819
# nice&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see how the probability increases as we get closer to the &lt;em&gt;generating values&lt;/em&gt;. In PMwG, this process is taken care of under the hood. Hopefully this shows just what the algorithm is trying to do. This is also a good test for your model, where likelihood should &lt;em&gt;increase&lt;/em&gt; with “better” test values. It’s also a good idea to do simulation and recovery to make sure your model “works”. This means generating values and then fitting the model with the goal of obtaining the generated values. If it doesn’t recover, there could be something wrong with the code, not enough data per condition/individual or the model does not recover.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;density-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Density function&lt;/h3&gt;
&lt;p&gt;The density function is shown below;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if(sample == FALSE){
    n_pars = length(x)
    tmpData = as.matrix(data[,-1])
    out = sum(TruncatedNormal::dtmvnorm(tmpData, x, diag(1, nrow = n_pars), lb = rep(0,n_pars), log = T, B = 1)) #B=1 because otherwise it does many iterations and takes too long
    return(out)
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are four steps in the getting the likelihood. First I grab the number of parameters. Second, I remove the names from the data for coding simplicity. Then I do the important part. Finally, I return a single value (the likelihood).&lt;/p&gt;
&lt;p&gt;For the important part here, I take the sum of all the densities as they are on the log scale (otherwise I would multiply). Using the &lt;code&gt;dtmvnorm&lt;/code&gt; function, I include the data, where each row is an observation and each column is a different metric. The means are given by the parameters (&lt;code&gt;x&lt;/code&gt;). I use an sd of 1 for the covariance. I also include a lower bound of 0 for all metrics.&lt;/p&gt;
&lt;p&gt;Importantly, all I’m doing here is checking how well the parameters fit the data, just as in the thought exercise above. This means that the density is calculated for each data point (i.e., each observation or food item), and then multiplied together (summed here because I use logs for simplicity).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;random-generation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random generation&lt;/h2&gt;
&lt;p&gt;In a very similar sense, to generate random data, I use the inverse functionality.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;else{
    n_pars = length(x)
    Restaurant = data[,1] #remove the name column
    tmpData = as.matrix(data[,-1])
    out = TruncatedNormal::rtmvnorm(nrow(tmpData), x, diag(1, nrow = n_pars), lb = rep(0,n_pars))
    out &amp;lt;- as.data.frame(cbind(out,Restaurant)) #add the name column back on
    return(out)
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What this allows us to do is generate data, given some input parameters for a single subject.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pmwg-functions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;PMwG Functions&lt;/h1&gt;
&lt;p&gt;Now I’ve done all the setup, I just need to prepare the sampler. To do this, first we need to specify the vector of parameter names. These will be used by the sampler.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pars=names(data)[-1]
&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;priors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Priors&lt;/h2&gt;
&lt;p&gt;Next I need to specify some priors. This can be daunting and stressful. But it really doesn’t need to be, especially with PMwG. The prior specification here is for the &lt;strong&gt;group level model&lt;/strong&gt;. PMwG assumes that the group level distribution of parameters (i.e., the means for each restaurant - NOT the truncated model) are distributed according to a multivariate normal distribution. For this prior, we only need to set the mean and the variance.&lt;/p&gt;
&lt;p&gt;The prior influences which parameter values are proposed. Here, I set the mean to 0 and variance to 10. Mean to 0?? Yes, because proposals come from a range of -Inf to Inf. In our likelihood we take care of these by taking the exponential of them, so really, the mean here is &lt;code&gt;exp(0) = 1&lt;/code&gt;. I set the variance to 10 so that a wide range of values are used, but given our exponential transformation, I could probably keep this at 1.&lt;/p&gt;
&lt;p&gt;For the rest of PMwG priors, we don’t need to consider the prior shapes of distributions of group level parameters as many researchers stress about with other fitting methods.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;priors &amp;lt;- list(
  theta_mu_mean = rep(0, length(pars)),
  theta_mu_var = diag(rep(10, length(pars)))) 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-sampler&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The sampler&lt;/h2&gt;
&lt;p&gt;Finally, we can set up the sampler and run the algorithm. To set up the sampler, we use the &lt;code&gt;pmwgs&lt;/code&gt; function, where we need to include &lt;code&gt;data&lt;/code&gt;,&lt;code&gt;pars&lt;/code&gt;,&lt;code&gt;priors&lt;/code&gt; and &lt;code&gt;ll_func&lt;/code&gt;. Then we can intialise the sampler.&lt;/p&gt;
&lt;p&gt;Here, the data must have a subject column so that the data can be split into data for each individual. The pars arguments should include any parameters used in the model to be estimated. Any prior should have a &lt;code&gt;theta_mu_mean&lt;/code&gt; vector and a &lt;code&gt;theta_mu_var&lt;/code&gt; matrix (mean and variance) of length pars and pars * pars respectively.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create the Particle Metropolis within Gibbs sampler object ------------------

sampler &amp;lt;- pmwgs(
  data = data,
  pars = pars,
  prior = priors,
  ll_func = ll
)

sampler &amp;lt;- init(sampler) # i don&amp;#39;t use any start points here&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sampling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sampling&lt;/h2&gt;
&lt;p&gt;When this is done, we can finally do the sampling!! For sampling, there are three stages. The specifics of these can be seen &lt;a href=&#34;https://newcastlecl.github.io/samplerDoc/pmwg-sampler-and-signal-detection-theory.html#sdtPMwG&#34;&gt;here&lt;/a&gt;. the tldr version of this is;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;burn&lt;/code&gt; used for initial burn-in samples (the throwaway samples) to get to the posterior distribution&lt;/li&gt;
&lt;li&gt;&lt;code&gt;adapt&lt;/code&gt; used to create the efficient distribution in sampling. This stage will stop if the efficient distribution can be created.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sample&lt;/code&gt; efficiently sample from the posterior&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that you don’t need to run each stage, you might just run burn-in and throw away the initial samples. However, we’ve seen significant improvements in the quality of samples from the final efficient stage.&lt;/p&gt;
&lt;p&gt;For the &lt;code&gt;run_stage&lt;/code&gt; argument, we need to include the sampler object, the stage we’re running, the number of iterations, particles and computer cores. For iterations and particles, this could take some fine tuning for the user, however, we tend to use 1000 burn-in iterations, 5000 adaptation and then however many desired samples for the final stage. We generally use either 100 or 300 particles depending on the model complexity. Here I use the &lt;code&gt;pmwg_new&lt;/code&gt; function &lt;code&gt;pstar&lt;/code&gt;. This adapts the “acceptance rate” - basically making the sampler more efficient. For more info on all the arguments, see &lt;a href=&#34;https://newcastlecl.github.io/samplerDoc/pmwg-sampler-and-signal-detection-theory.html#sdtPMwG&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this example, the model is simple and easy to estimate, so I dial back some of these tuning parameters. To know the best settings to use, you might start with something like below and then tinker as you see fit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sampled &amp;lt;- run_stage(sampler, stage = &amp;quot;burn&amp;quot;,iter = 100, particles = 100, n_cores = 4, pstar=.6) #epsion will set automatically to 0.5
sampled &amp;lt;- run_stage(sampled, stage = &amp;quot;adapt&amp;quot;,iter = 500, particles = 100, n_cores = 4, pstar=.6) 
sampled &amp;lt;- run_stage(sampled, stage = &amp;quot;sample&amp;quot;,iter = 100, particles = 50, n_cores = 4, pstar=.6) 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;NOTE: This will take a few minutes to run. Increasing the number of particles or iterations will slow the sampler down. However, we need to reach the posterior (i.e., the good values for the parameters), and to do this we need good information. This means that we may need more particles or iterations. Ideally, for efficiency, we should aim for between 30 and 80% acceptance rate. Here, &lt;code&gt;pstar&lt;/code&gt; helps as I can specify a desired acceptance rate.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;output&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Output&lt;/h1&gt;
&lt;p&gt;Once our sampler is finished, we can check the output.&lt;/p&gt;
&lt;div id=&#34;checking&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Checking&lt;/h2&gt;
&lt;p&gt;First, we should check that we are in the posterior. To do this, we can check the chain plots of the samples. Here we show two plots. The first is the group level parameter chains and the second is the individual subject likelihood chains. For the parameter chains, we’re looking for “stationarity” in the samples. This means that the sampled values will show a flat line with a bit of wiggle (like a hairy caterpillar). For the subject likelihood, we’re looking for flat lines as well, however, we should see an increase in these from the start to the end. These will be less wiggly. See &lt;a href=&#34;https://newcastlecl.github.io/samplerDoc/pmwg-sampler-and-signal-detection-theory.html#check-the-sampling-process&#34;&gt;here&lt;/a&gt; for more details on this output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmp &amp;lt;- sampled
matplot(t(tmp$samples$theta_mu),type=&amp;quot;l&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;staticunnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;matplot(t(tmp$samples$subj_ll),type=&amp;quot;l&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;staticunnamed-chunk-15-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we can see that some parameters only reach the posterior in the final sampling stage (x-axis is iterations of sampling). We can see however, that stationarity is reached. Secondly, we can see that the likelihood increases and then stabilizes at the highest value for each restaurant.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;posterior-inference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Posterior Inference&lt;/h2&gt;
&lt;p&gt;Next, we can look at the sampled values. Here I show group level and inidvidual restaurant level parameter values. Remember that by model sampling, we generate distributions of possible parameter values, so here, I take the median of these values. I also transform these as I did in the likelihood.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;groupMedian &amp;lt;- apply(tmp$samples$theta_mu[,tmp$samples$stage==&amp;quot;sample&amp;quot;],1,median)
round(exp(groupMedian),3)
##    calories     cal_fat   total_fat     sat_fat   trans_fat cholesterol 
##       4.842       0.001      23.322       6.782       0.000       0.816 
##      sodium  total_carb       fiber       sugar     protein 
##      12.661      41.485       3.470       6.421      28.770

restaurantMedian &amp;lt;- apply(tmp$samples$alpha[,,tmp$samples$stage==&amp;quot;sample&amp;quot;],1:2, median)
round(exp(restaurantMedian),3)
##             Mcdonalds Chick Fil-A  Sonic  Arbys Burger King Dairy Queen Subway
## calories        5.904       3.870  5.600  4.872       5.672       5.107  4.952
## cal_fat         0.000       0.006  0.000  0.006       0.000       0.003  0.000
## total_fat      31.947      14.475 37.683 26.708      34.755      29.038 18.584
## sat_fat         8.204       3.186 10.982  8.050      10.414      10.415  5.950
## trans_fat       0.000       0.000  0.000  0.004       0.000       0.000  0.002
## cholesterol     1.541       0.596  1.180  0.900       0.884       0.819  0.737
## sodium         14.550      10.735 13.711 15.157      11.673      11.413 12.802
## total_carb     48.858      25.488 47.153 44.797      37.188      38.741 54.857
## fiber           3.394       2.687  2.972  3.022       2.358       3.418  6.606
## sugar          11.041       3.834  6.320  7.477       7.713       6.219 10.094
## protein        40.386      30.841 29.280 29.165      29.285      24.947 30.244
##             Taco Bell
## calories        4.597
## cal_fat         0.000
## total_fat      20.861
## sat_fat         6.409
## trans_fat       0.000
## cholesterol     0.506
## sodium         10.164
## total_carb     46.599
## fiber           5.707
## sugar           3.721
## protein        17.390&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see the mean values overall, but that’s not overly meaningful. The more meaningful estimates are at the individual level.&lt;/p&gt;
&lt;p&gt;We can also look at the distribution of the group parameter values and see how restaurants compare to the group distribution. I’ve only shown two parameters here, but this is a nice example.&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;library(tidyr)
library(ggplot2)

eg &amp;lt;- apply(tmp$samples$alpha[c(1,10),,tmp$samples$stage==&amp;quot;sample&amp;quot;],1:2, median)
eg&amp;lt;-round(exp(eg),3)
group &amp;lt;- exp(tmp$samples$theta_mu[c(1,10),tmp$samples$stage==&amp;quot;sample&amp;quot;])
group &amp;lt;- as.data.frame(t(group))
eg &amp;lt;- as.data.frame(t(eg))
eg$restaurant &amp;lt;- row.names(eg)
group &amp;lt;- pivot_longer(group, cols = everything(), names_to = &amp;quot;metric&amp;quot;, values_to = &amp;quot;value&amp;quot;)
eg &amp;lt;- pivot_longer(eg, cols = c(&amp;quot;calories&amp;quot;,&amp;quot;sugar&amp;quot;), names_to = &amp;quot;metric&amp;quot;, values_to = &amp;quot;value&amp;quot;)

# add vertical mean line to density plot with geom_vline()
ggplot(group, aes(x=value)) +
  geom_density( fill=&amp;quot;dodgerblue&amp;quot;, alpha=0.5)+
  geom_vline(data=eg,aes(xintercept=value,colour = restaurant), size=.5)+
  geom_text(data=eg,aes(x=value,label = restaurant,y=1), angle=90)+  
  theme_bw()+
  facet_grid(~metric, scales = &amp;quot;free&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;staticunnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Anything unexpected here?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Correlations&lt;/h2&gt;
&lt;p&gt;Finally, one strength of PMwG is that it assumes a group level multivariate model, with variance given by a covariance matrix. This means we can see correlations between parameters of the model - something that can be very useful with models with multiple components. An example of this is shown below;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov&amp;lt;-apply(tmp$samples$theta_var[,,tmp$samples$stage==&amp;quot;sample&amp;quot;],1:2, median)
colnames(cov)&amp;lt;-tmp$par_names
rownames(cov)&amp;lt;-tmp$par_names
cor&amp;lt;-cov2cor(cov) #correlation matrix
library(corrplot)
## corrplot 0.84 loaded
corrplot(cor, method=&amp;quot;circle&amp;quot;, type = &amp;quot;lower&amp;quot;, title = &amp;quot;Parameter Correlations&amp;quot;, tl.col = &amp;quot;black&amp;quot;,mar=c(0,0,2,0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;staticunnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model-comparison&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model Comparison&lt;/h1&gt;
&lt;p&gt;Let’s say we wanted to compare our model (the truncated normal) to something else (standard multivariate normal). How could we do this??&lt;/p&gt;
&lt;p&gt;First, let’s write the likelihood function;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ll &amp;lt;- function(x, data, sample = F ){
  x=exp(x)
  if(sample == FALSE){
    n_pars = length(x)
    tmpData = as.matrix(data[,-1])
    out = sum(mvtnorm::dmvnorm(tmpData, x, diag(1, nrow = n_pars), log = T)) # only change is here to the model
    return(out)
  }else{
    n_pars = length(x)
    Restaurant = data[,1]
    tmpData = as.matrix(data[,-1])
    out = mvtnorm::rmvnorm(nrow(tmpData), x, diag(1, nrow = n_pars), log = T)
    out &amp;lt;- as.data.frame(cbind(out,Restaurant))
    return(out)
  }
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I ran this model using the exact same code (apart from the likelihood), but I’ve included it below;&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;rm(list=ls())

# library(pmwg)

source(&amp;quot;pmwg/variants/standard.R&amp;quot;)
library(TruncatedNormal)
library(openintro)

data &amp;lt;- fastfood[,1:13]
names(data)[1]&amp;lt;-&amp;quot;subject&amp;quot;
data &amp;lt;- data[c(1,3:13)]
data &amp;lt;- na.omit(data)
data[,c(2,3,7,8)]&amp;lt;- data[,c(2,3,7,8)]/100


## plots

par(mfrow=c(4,3))
apply(data[,-1], 2, hist, main = NULL)
dev.off()


### likelihood
ll &amp;lt;- function(x, data, sample = F ){
  x=exp(x)
  if(sample == FALSE){
    n_pars = length(x)
    tmpData = as.matrix(data[,-1])
    out = sum(mvtnorm::dmvnorm(tmpData, x, diag(1, nrow = n_pars), log = T)) # only change is here to the model
    return(out)
  }else{
    n_pars = length(x)
    Restaurant = data[,1]
    tmpData = as.matrix(data[,-1])
    out = mvtnorm::rmvnorm(nrow(tmpData), x, diag(1, nrow = n_pars), log = T)
    out &amp;lt;- as.data.frame(cbind(out,Restaurant))
    return(out)
  }
  
}

pars=names(data)[-1]


priors &amp;lt;- list(
  theta_mu_mean = rep(0, length(pars)),
  theta_mu_var = diag(rep(10, length(pars)))) 


# Create the Particle Metropolis within Gibbs sampler object ------------------

sampler &amp;lt;- pmwgs(
  data = data,
  pars = pars,
  prior = priors,
  ll_func = ll
)

sampler &amp;lt;- init(sampler) # i don&amp;#39;t use any start points here

sampled &amp;lt;- run_stage(sampler, stage = &amp;quot;burn&amp;quot;,iter = 100, particles = 100, n_cores = 4, pstar=.6) #epsion will set automatically to 0.5
sampled &amp;lt;- run_stage(sampled, stage = &amp;quot;adapt&amp;quot;,iter = 500, particles = 100, n_cores = 4, pstar=.6) 
sampled &amp;lt;- run_stage(sampled, stage = &amp;quot;sample&amp;quot;,iter = 200, particles = 50, n_cores = 4, pstar=.6) 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have our two sampled objects, we can do model comparison. For some advanced model comparison methods, see &lt;a href=&#34;https://newcastlecl.github.io/samplerDoc/estimating-the-marginal-likelihood-via-importance-sampling-is2.html&#34;&gt;here&lt;/a&gt;. For now though, we’re going to compare using a criterion method. Here, the smaller value represents the more likely model. The function for the deviance information criterion (DIC) is below&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;pmwg.DIC=function(sampled,pD=FALSE){
  nsubj=length(unique(sampled$data$subject))
  
  # the mean likelihood of the overall (sampled-stage) model, separately for each subject
  mean.like &amp;lt;- apply(sampled$samples$subj_ll[,sampled$samples$stage==&amp;quot;sample&amp;quot;],1,mean)
  
  # the mean of each parameter across iterations. Keep dimensions for parameters and subjects
  mean.params &amp;lt;- t(apply(sampled$samples$alpha[,,sampled$samples$stage==&amp;quot;sample&amp;quot;],1:2,mean))
  
  # i name mean.params here so it can be used by the log_like function
  colnames(mean.params)&amp;lt;-sampled$par_names
  
  # log-likelihood for each subject using their mean parameter vector
  mean.params.like &amp;lt;- numeric(ncol(mean.params))
  data &amp;lt;- transform(sampled$data, subject=match(subject, unique(subject)))
  for (j in 1:nsubj) {
    mean.params.like[j] &amp;lt;- sampled$ll_func(mean.params[j,], data=data[data$subject==j,], sample=FALSE)
  }
  
  # Effective number of parameters
  pD &amp;lt;- sum(-2*mean.like + 2*mean.params.like)
  
  # Deviance Information Criterion
  DIC &amp;lt;- sum(-4*mean.like + 2*mean.params.like)
  
  if (pD){
    return(c(&amp;quot;DIC&amp;quot;=DIC,&amp;quot;effective parameters&amp;quot;=pD))
  }else{
    return(DIC)
  }
    
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now all we need to do is load our two sampled objects and compare!&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Warning in -2 * mean.like + 2 * mean.params.like: longer object length is not a
## multiple of shorter object length
## Warning in -4 * mean.like + 2 * mean.params.like: longer object length is not a
## multiple of shorter object length
##                  DIC effective parameters 
##            1105994.4             231328.4
## Warning in -2 * mean.like + 2 * mean.params.like: longer object length is not a
## multiple of shorter object length

## Warning in -2 * mean.like + 2 * mean.params.like: longer object length is not a
## multiple of shorter object length
##                  DIC effective parameters 
##            1115781.3             231675.7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# First load in the sampled object for the truncated multivariate normal model
load(&amp;quot;fastFood_samples_tmvn.RData&amp;quot;)
# test
pmwg.DIC(sampled) 

# First load in the sampled object for the truncated multivariate normal model
load(&amp;quot;fastFood_samples_mvn.RData&amp;quot;)
# test
pmwg.DIC(sampled)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Evidently, we can see that the smaller DIC value was for our truncated normal model - so we made a good choice picking this one first ;)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-word&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Final Word&lt;/h1&gt;
&lt;p&gt;In this data set, there are only 8 restaurants. Usually for a hierarchical model, we would want at least 20 individuals. Additionally, there are very few observations per restaurant. Similar to the group level, we want a well informed individual level model. The model still works, but because of the low amount of data, there is greater uncertainty.&lt;/p&gt;
&lt;p&gt;So it’s not the best data set, but hey, it makes for a nice example. I’ve also been brief with a few explanations on specifics, the algorithm and some function arguments, but this is for brevity (and because they’re generally not super important). If you want to know more about specifics, please see the &lt;a href=&#34;https://newcastlecl.github.io/samplerDoc/&#34;&gt;bookdown&lt;/a&gt;. Hopefully for users outside of psychology (or even outside of decision making modelling), this example can help contextualize the bookdown.&lt;/p&gt;
&lt;p&gt;Happy coding!&lt;/p&gt;
&lt;p&gt;Reilly&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>PMwG Log-Likelihood Functions</title>
      <link>/post/2021-05-12-ll/writing-ll/</link>
      <pubDate>Wed, 12 May 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-05-12-ll/writing-ll/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;A blog post to help with writing log likelihood functions to be used in PMwG. The examples shown in the &lt;a href=&#34;https://newcastlecl.github.io/samplerDoc/&#34;&gt;PMwG Sampler Doc&lt;/a&gt; are quite specific, and so this blog aims to outline the general process of constructing a likelihood function. This blog is probably most useful for anyone who is looking for more info on how to make these functions for different RT models, or any general probability based hierarchical models. Probability based modelling involves working out the probability of some data given a model (and certain model parameter values). This could be evidence accumulation models, signal detection theory models, categorisation models and many, many more.&lt;/p&gt;
&lt;p&gt;To get started with PMwG, you’ll need some data - which can be any shape and can take any values, as long as there is a &lt;code&gt;subject&lt;/code&gt; column which identifies the separate subjects - and a likelihood function - which calculates the likelihood of data given some parameter values. The data can be small or large, but should have multiple entries per person. Further, the data should be able to be split be subject column (i.e. &lt;code&gt;subject_i &amp;lt;- data[data$subject==i,]&lt;/code&gt;) and will link closely to the structure of the likelihood function. For example, in the SDT example in &lt;a href=&#34;https://newcastlecl.github.io/samplerDoc/pmwg-sampler-and-signal-detection-theory.html&#34;&gt;Chapter 2&lt;/a&gt;, the data for the fast likelihood shows 16 possible values for each participant, which counts the number of responses in each cell of the design. In the likelihood function in Chapter 2, probability is calculated for each response type and is multiplied by the number of these responses (from the column &lt;code&gt;n&lt;/code&gt;). This shows that the data can take any form, as long as it is accounted for in the function.&lt;/p&gt;
&lt;div id=&#34;writing-log-likelihood-functions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Writing Log-Likelihood Functions&lt;/h1&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;PMwG is a Bayesian hierarchical model sampling method proposed by Gunawan et al., (2020). In our PMwG sampler documentation, there are several examples of (and countless references to) writing your log likelihood function. For anyone new to modelling, or just new to this style of modelling, this step is the equivalent of STAN and JAGS model.text files. The way these operate though is vastly different.&lt;/p&gt;
&lt;p&gt;The main purpose of the log likelihood function, which I’ll refer to as LL from here on, is to return a single logged likelihood value. This value can be thought of as a probability, or marginal likelihood, of some data given some model parameters under this model. If this stuff and modelling is all pretty familiar to you, you can skip straight to the “How to Make Your LL function” section.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: For people new to this kind of modelling, by sampling, here I mean “training” your model on the data. This is done in a Markov Chain Monte-Carlo (MCMC) kind of way for PMwG, where values are selected when they are &lt;em&gt;good&lt;/em&gt; on each iteration, until the model is trained to be in the best place. We call it “sampling” as we are sampling the parameter space (i.e. all possible values of parameters), of which we need many plausible values at the end so that we can process these in post (i.e. essentially if we only get one “sample” of parameters, we restrict ourselves, sampling lots of good values allows us to see the variance and do more accurate posterior calculations). Later on, after the model is &lt;em&gt;trained&lt;/em&gt;, we may do anpother type of sampling where we sample “posterior predictive data” - i.e. we use the trained model parameters to generate data. For this second kind of sampling, I’ll try and refer to it as “generating”.&lt;/p&gt;
&lt;p&gt;Usually, for data, we can easily return the density of a value given some input. For example, if I’d like to know the probability of a response coming from a certain normal distribution, i could do;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;value &amp;lt;- 3
m &amp;lt;- 4 #mean
s &amp;lt;- 0.5 #standard deviation
dnorm(x=value, mean=m,sd=s, log=FALSE) #density function
## [1] 0.1079819&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which returns a probability value. Plotting this shows us a little bit more;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = data.frame(x = c(2, 6)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 4, sd = 0.5)) + ylab(&amp;quot;&amp;quot;) +
  theme_bw()+geom_vline(xintercept=3, colour=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;staticnormalcurve-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And so this looks about right. There’s a 10% chance this response is under this distribution. So now we know that we can get the density of data for values. If i change log to &lt;code&gt;TRUE&lt;/code&gt;, i get the log likelihood for these &lt;em&gt;parameters&lt;/em&gt; and the data point I have. Lets see what this would look like;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
dnorm(x=value, mean=m,sd=s, log=TRUE)
## [1] -2.225791&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For PMwG we need to return the log-likelihood so that the values are useful to the algorithm. This also means we can add values together rather than multiplying probabilities together - and this protects against tiny values.&lt;/p&gt;
&lt;p&gt;In a modelling scenario, we would repeat this density calculation many times (i.e. for each data point). But what if there were two conditions, which differed for the mean of the distribution? The distributions might be hypothesized to look like this for example;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;staticnormalcurve2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we can check different data points for different conditions. Lets imagine a person’s responses in condition 1 were 4, 3 and 3.5 and in condition 2 were 2, 2.1 and 4. Using the same method as above, we can calculate the probability of these responses (data) given some parameters (here, the parameters are the mean and standard deviation, but we only vary the mean).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;values_1 &amp;lt;- c(4,3,3.5) #data for condition 1
values_2 &amp;lt;- c(2,2.1,4) # data for condition 2
m1 &amp;lt;- 4
m2 &amp;lt;- 2
s &amp;lt;- 0.5
cond1 &amp;lt;- dnorm(x=values_1, mean=m1,sd=s, log=FALSE)
cond2 &amp;lt;- dnorm(x=values_2, mean=m2,sd=s, log=FALSE)
cond1
## [1] 0.7978846 0.1079819 0.4839414
cond2
## [1] 0.7978845608 0.7820853880 0.0002676605&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we can see here that the two means we’ve proposed are pretty good guesses for the probability of these responses. In the PMwG algorithm, each particle proposes new guesses for the parameters for each subjects’ data. These values are then input into the model (similar to above), and then the sum of these logged values is returned. The particle with the highest log likelihood is chosen on each iteration so that eventually, the best parameter values are chosen for each subjects’ data.&lt;/p&gt;
&lt;p&gt;The model above is a model that assumes that responses come from normal distributions which may differ across conditions. If there was no difference in conditions, then PMwG would likely return equivalent mean parameter values (m1 and m2). This is a simple model, and so there is much more complexity to add before reaching a full drift diffusion or LBA model. In the next section I outline some considerations for modelling and then steps to creating your likelihood function.&lt;/p&gt;
&lt;div id=&#34;linking-models-and-lls&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Linking Models and LLs&lt;/h3&gt;
&lt;p&gt;It’s important to note here about how LLs and models link. You may think the LL &lt;em&gt;is&lt;/em&gt; the model, and it kind of is. We use the LL function to &lt;em&gt;estimate the likelihood under the model&lt;/em&gt; - that’s pretty clear from above. So how do I fit my model in? Well this comes in the call to the density function (like dnorm or dLBA). Many density functions for probability models already exist in a variety of packages. These functions are often detailed in papers, and so if a density function does not exist, you can still easily calculate the density for models given these equations in papers – although it is likely someone has already done this for you. If a density function does exist, this is usually as easy as inputting the observations (data) and the parameter values. For some examples of this see the &lt;a href=&#34;https://cran.r-project.org/web/packages/rtdists/rtdists.pdf&#34;&gt;rtdists package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;whats-the-purpose-of-modelling-your-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What’s the purpose of modelling your data?&lt;/h2&gt;
&lt;p&gt;The first question to ask is “what’s the purpose of modelling my data?” or “what should the modelling tell me about the data?”. There are many different flavours of answers to this question which should be considered from the outset of any experiment/modelling exercise. These ‘flavours’ fall into several main categories.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;selective influence&lt;/li&gt;
&lt;li&gt;model comparison&lt;/li&gt;
&lt;li&gt;comparing groups&lt;/li&gt;
&lt;li&gt;finding the ‘best’ model&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;selective-influence&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Selective influence&lt;/h3&gt;
&lt;p&gt;Selective influence is a manipulation used to check whether a parameter maps to the appropriate parameter. For example, in a model where a parameter was thought to map to memory strength, then values for this parameter should be higher in conditions where memories are stronger or more deeply encoded (and not other parameters of the model). This is a type of experiment we do to test the validity and reliability of our models to make sure that they capture the effects we are interested in.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-comparison&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model comparison&lt;/h3&gt;
&lt;p&gt;Model comparison is a highly common practice in statistical methods which involves comparing different model accounts of the data. In the PMwG documentation, we briefly touch on how to compare models and different comparison methods. For the purposes of writing an LL however, this often involves writing several different LLs which vary (in parameters or models used) and running them separately. The results from these are then compared in model comparison methods. An example of a model comparison could be from the above example, where we might compare the descriptive adequacy of normal distributions to truncated normal, uniform distributions, log normal and exponential distributions to decide which best describes the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-groups&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comparing groups&lt;/h3&gt;
&lt;p&gt;Comparing groups is highly important for psychology and other sciences, especially in modelling exercises, as we may wish to discover &lt;em&gt;how&lt;/em&gt; groups differ in underlying parameters or even &lt;em&gt;if&lt;/em&gt; they differ. There are many varying ways of doing this. In PMwG we can do between-subjects model fits or we may run the models separately for the different data sets and compare parameter outputs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;finding-the-best-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Finding the ‘best’ model&lt;/h3&gt;
&lt;p&gt;Similar to model comparison, finding the best model is often done when researchers aim to describe certain behavior or data. In finding the best model, one may avoid fitting the full model space so as to reduce the complexity (and not limit flexibility) of models. After this, complexity may be added in to capture more of the data. This is often done by fitting a model, checking the model to see how well it did, and then adding in parameters or complexity to see if this can help improve the model fit. This can also grow into a model compariosn exercise, where the models to compare do not cover the full model space.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;so-whats-the-point&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;So what’s the point?&lt;/h3&gt;
&lt;p&gt;Evidently, there are many different methods for modelling that achieve different outcomes, and this can influence how we construct our LLs. For now though, we’re going to stick with the “finding the best model” approach.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;which-model-will-capture-the-effects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Which model will capture the effects?&lt;/h2&gt;
&lt;p&gt;So now you have the purpose of your model, you need to ask what kind of model will capture the effects. In most of the research I do, I’m working with response times and decision making, so I work with evidence accumulation models. For these models, I’m lucky because the density functions are already written. For many applications this is the case, although in some, you may have to put in some extra effort to write your own density functions.&lt;/p&gt;
&lt;p&gt;The effects you’re looking at are important for considering what models you’ll use. Being in your field, you’ll probably already know an array of different models that exist and are evidence based that you will want to fit. But for now, lets go through some key considerations.&lt;/p&gt;
&lt;div id=&#34;what-type-of-model-is-needed-to-answer-the-purpose&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What type of model is needed to answer the purpose?&lt;/h4&gt;
&lt;p&gt;In the example I gave above about model comparison, I talked about comparing different distributions. For the purpose of this question (i.e. what distribution do these responses come from), then these models (i.e. different distributions) answer the purpose. For a question where I wanted to know where people set their mean and sd given I think the distribution is normal - this would require only a normal distribution and looking at the PMwG parameter estimates.&lt;/p&gt;
&lt;p&gt;In an evidence accumulation model, I may want to know about start point variability or within trial drift variability, and so it is better for me to use the DDM rather than LBA, as these parameters are not evaluated by the LBA. Put simply, make sure the model you use answers the purpose you set out to achieve.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;is-this-model-flexible-for-my-effects&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Is this model flexible for my effects?&lt;/h4&gt;
&lt;p&gt;Secondly, it’s important to consider the flexibility of the model and whether the model will allow you to capture effects. For example, fitting an exponential curve to data could really restrict the space I’m sampling (i.e. positive only and exponential values), and may bias models towards inflated values which don’t capture the true tails of the distributions of my responses. Obviously this is an extreme example, but this needs to be considered when planning which model to use.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;is-your-data-suitable&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Is your data suitable?&lt;/h4&gt;
&lt;p&gt;The next consideration is with your data. This should probably come earlier, or even before the experiment has been done. Remembering that PMwG is for hierarchical modelling, which means we get both group and individual level parameter estimates. If we wanted to estimate means and standard deviations for the top example, this would work fine - we could collect lots of data from a large sample and then see how individuals set their means (and sd’s) for the two conditions. This would give us both individual estimates, but also will allow us to see group level estimates.&lt;/p&gt;
&lt;p&gt;However, if we take only one data point per person and randomly allocate people to conditions, this causes trouble. First of all, at the individual level, we will not have enough data to fit models. Secondly, for the m1 and m2 parameters, people would be missing values, and so this estimated value would revert to the prior, thus being uninformative at individual subject level and less useful at the group level. Evidently, we need to collect enough data to fit our models, but also consider our experimental manipulations - ensuring we have enough data in each cell of the design. This extends to the &lt;em&gt;type&lt;/em&gt; of design, where between-subjects effects become difficult to account for. This can be done, but experimental rigor is sacrificed as it either requires separate fitting or uninformed random effects.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-will-the-models-priors-be&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What will the models priors be?&lt;/h4&gt;
&lt;p&gt;Finally, it’s important to consider priors. PMwG uses a multivariate normal prior (and Huang and Wand’s (2013) prior on the variance). This isn’t super important for &lt;em&gt;using&lt;/em&gt; the sampler, but is important to the methodology. What’s important for using the sampler, is that you specify the prior mean and mean variance (remembering this is on the real number line). For most models we do, we generally set the mean at 0 and the variance at 1, but this may be too restrictive for some models. Further, you should consider parameter transformations you may undertake and how these parameters will appear &lt;strong&gt;before&lt;/strong&gt; they are transformed. For example we usually take the exponenital of input parameters in LBA modelling to ensure they’re strictly positive, this means the exp of 0 is 1, and such our prior within the model is 1. This is also the case for variance, where the exponential of -1 is 0.37 and the exponential of 1 is 2.72 which would be the case for one standard deviation either side of the mean in this example.&lt;/p&gt;
&lt;p&gt;An example of these priors are shown below;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
priors &amp;lt;- list(
  theta_mu_mean = rep(0, length(pars)),
  theta_mu_var = diag(rep(3, length(pars)))) 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;what-are-the-important-effects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What are the important effects?&lt;/h2&gt;
&lt;p&gt;So we now know our purpose and we’ve chosen a model to fit. Next things to consider are the effects. Most of the time, you’ll know the effects you’re interested in because these will form the research questions &lt;em&gt;and&lt;/em&gt; will be evident from descriptive statistics. These are effects like condition 1 vs condition 2 from our top example, but could also be related to stimuli, responses and more. Lets go through some key considerations when designing the LL and the experiment.&lt;/p&gt;
&lt;div id=&#34;experimental-manipulations&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Experimental manipulations?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;What were the manipulations in the experiment?&lt;/li&gt;
&lt;li&gt;Were these between or within subjects?&lt;/li&gt;
&lt;li&gt;What would we actually be looking for in the data (i.e. differences in response times, accuracy, choice proportions etc)?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;how-do-manipulations-manifest&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;How do manipulations manifest?&lt;/h4&gt;
&lt;p&gt;From these experimental manipulations, we need to consider how these might manifest in the cognitive model. In a cognitive model, the parameters relate to underlying cognitive processes or representations. Consequently, we should propose models, and parameterisations, that relate to our experimental conditions. This might mean that in a 3 parameter model for a 3 condition experiment, we could have up to 9 parameters (or more if there was added model complexity), where for each condition, the model has 3 unique parameters. We also might hypothesize that in the 3 parameter model for the 3 condition experiment, that 1 parameter is fixed across conditions, while the other two vary. Comparing the 9 parameter model to the 7 parameter model (and other combinations) becomes a model comparison question.&lt;/p&gt;
&lt;p&gt;Working out how these manipulations may manifest is highly important in modelling – both going forward and looking back. For example, when planning an experiment that involves modelling, one should consider the type of modelling question and the best experimental manipulation to implement to answer this question. When planning LLs, one should consider how the experimental manipulation would logically, and rationally, map to parameters. This is not only important for the model, but is vital for drawing clear conclusions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;group-differences&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Group differences&lt;/h4&gt;
&lt;p&gt;It’s also important to consider potential group differences when writing LLs. Where group differences exist, we may see the model under-fitting the data, however, including these effects in the LL could lead to uninformed cells in the model. If this is the case, it may be best to run the model separately for the different groups (but this method also has a weakness in that the fits are not informed by one another).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;important-effects-are-these-present-in-descriptives-what-does-the-literature-say&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Important effects? Are these present in descriptives? What does the literature say?&lt;/h4&gt;
&lt;p&gt;If we do expect to see effects of manipulations in the model, we should first check if the effects of manipulations can be seen in the descriptives. It’s important that the manipulation actually worked before we start explaining how/why it worked. This can also be useful to see main and interaction effects which could be important to the model.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;considerations-of-writing-the-ll&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Considerations of writing the LL&lt;/h2&gt;
&lt;p&gt;Finally, we’ve made it to the LL function! You’ve now thought about all components of your data and your experiment (probably). But, before writing this, lets take some time for final considerations.&lt;/p&gt;
&lt;div id=&#34;consider-the-stimuli-and-responses&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Consider the stimuli and responses&lt;/h4&gt;
&lt;p&gt;When considering your stimuli and responses, consider how these may vary across conditions, how these might vary with response types/proportions/biases and what effects these may have on the underlying cognitive process. It is often the case that certain stimuli or responses can lead to inconsistencies in the model (and in the descriptives). Consequently, consider doing a deep dive into the descriptives first up to check for any weirdness - then decide what to do with these. It might also be an idea to run a full model (in addition to other models) to ensure nothing strange is going on in certain conditions or with certain parameters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;consider-your-data&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Consider your data&lt;/h4&gt;
&lt;p&gt;Following on from the above, when doing a deep dive into the data, consider cut points for outliers and standardization. As PMwG fits hierarchical models, it is often unlikely that corrections are required (such as logging RTs or minimizing the impacts of extreme data), however, occasionally data may need to be removed, such as for bad participants or lapses of attention (where these aren’t fit by the model).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-make-your-ll&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to make your LL&lt;/h2&gt;
&lt;p&gt;In this section, I’ll detail the methods I generally use to make my LL function, and in the next section, I’ll test it. There are six main steps to making a LL.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Prepare the parameters to be used&lt;/li&gt;
&lt;li&gt;Ensure no bad parameter values&lt;/li&gt;
&lt;li&gt;Map parameters to conditions&lt;/li&gt;
&lt;li&gt;Make the ‘sample’ component&lt;/li&gt;
&lt;li&gt;Make the ‘likelihood’ component&lt;/li&gt;
&lt;li&gt;Return&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;PMwG samples random values on the real line for the parameters of the model. These values are taken from a distribution of values centered on the group mean or the individual mean, with some variance, in a multivariate normal distribution. So for example, for 3 true parameter values in a model (say -1, 2 and 9), PMwG will propose 3 values as a particle (where we have n particles) for n iterations, constantly returning the most likely combination of parameters. On iteration 1, three values are selected at random (say 0,0 and 0) and a likelihood is returned. On the second step, n particles more values will be proposed using the previous particle as a reference point (from both the group and individuals previous particle) and the winner will be selected (say 0,1 and 5). This process happens over and over until the posterior space is reached.&lt;/p&gt;
&lt;div id=&#34;prepare-the-parameters-to-be-used&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Prepare the parameters to be used&lt;/h4&gt;
&lt;p&gt;As PMwG returns values on the real number line, we first have to prepare our parameters to be used properly in the function. This may require corrections, such as taking the exponent of these values (as is done by Gunawan et al., 2020, JMP), converting a number to between 0 and 1 (for probability parameters), or may require a mathematical function, for example, certain parameters may become 1/parameter. This should be done early in the LL.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: if there are any transformations or corrections carried out, it is important to remember and account for these when evaluating the output and generating posterior predictive data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
ll &amp;lt;- function(x, data, sample=FALSE){
  x = exp(x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, and those shown in the sampler documentation, we use an exponential transformation. This is to ensure parameter values are always greater than 0 (as the model doesn’t work if values are below 0). Evidence for this transformation comes from the main PMwG paper by David Gunawan and colleagues (Gunawan, et al., 2020, JMP). It is not always necessary to carry out transformations, however, it is important to consult the literature for advice on general practice (especially keeping in mind that PMwG samples from the real line).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ensure-no-bad-parameter-values&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Ensure no bad parameter values&lt;/h4&gt;
&lt;p&gt;Secondly, to save computation time and protect against bad values, I often include code to exclude ‘bad’ parameters. For example, if specific parameters should be non-negative and a negative value is proposed from the PMwG algorithm, the LL automatically returns -1e10. Example of this is included below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  if (!sample){
    if (any(data$RT &amp;lt; x[&amp;quot;t0&amp;quot;])) {  ### for this example, i need to make sure t0 is not below any RT, if there are RTs below t0, this is bad for the model
    return(-1e10)
  }


if (x[&amp;quot;b&amp;quot;] &amp;lt;= any(abs(c(x[&amp;quot;b.1&amp;quot;],x[&amp;quot;b.2&amp;quot;],x[&amp;quot;b.3&amp;quot;])))) {  # for this example, i need to add b.n to b, and so if there are negative b.n&amp;#39;s this would make b below 0, which again is bad for the model
    return(-1e10)
  }
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, x is the parameter vector, and the parameters have names such as “b”, “t0”, “b.1” etc. To refer to parameters, I use x[“b”] naming convention. You can also refer to these numerically, such as x[1], however, this is risky and can lead to potential errors. Naming is safer.&lt;/p&gt;
&lt;p&gt;I only use this when &lt;code&gt;sample = FALSE&lt;/code&gt;, as this can mess up posterior predictive sampling.&lt;/p&gt;
&lt;p&gt;It’s important that if the density function cannot handle specific values (i.e. dLBA can’t work with negative values), then this safety check should be put in, so that NAs aren’t returned (which would break the sampling).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;map-parameters-to-conditions&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Map parameters to conditions&lt;/h4&gt;
&lt;p&gt;Next up, I need to map parameters to conditions. I referred to this earlier when discussing experimental manipulation effects on the data/model. When I refer to mapping parameters to conditions, this means that parameters which refer to conditions are associated with the correct data. i.e. parameter “b.1” is associated with condition 1 and so fourth. All this step does is &lt;strong&gt;ensure that the correct parameter values are input into the model for each data point evaluated.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: In process models, parameters, like A and B used in the examples below, often refer to specific components. For example, in cognitive modelling, parameter A may refer to the individuals bias in responding and parameter B could refer to the sensitivity of responding. Many models used in cognitive modelling link to specific cognitive processes or structures, and links to these processes are made through selective influence studies. There is often a significant amount of literature justifying parameters in a model and underlying processes these relate to.&lt;/p&gt;
&lt;p&gt;When writing your LL you could do parameter:condition associations line by line, and I recommend this at first, or through vectorising or other methods. The line by line method is slow, but also more accurate. Later on you can move to more advanced methods which quickly loops over conditions. Note that the more calls you make to the density function, and the more loops in the function, lead to more time.&lt;/p&gt;
&lt;p&gt;I’ve included two examples below of mapping parameters to conditions, in both slow and fast ways. The slow way is computationally inefficient, but is quite safe. The fast way is fast, but risks parameters not mapping as effectively. You should write your function in both ways, then compare them to ensure the output is the same and correctly mapping. For more info on this, see &lt;a href=&#34;https://newcastlecl.github.io/samplerDoc/forstmannChapter.html#writellFunchttps://newcastlecl.github.io/samplerDoc/forstmannChapter.html#writellFunc&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;slow&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Slow&lt;/h5&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
parameter.b = parameter.A = numeric(nrow(data)) # first i create a vector to store the values in
parameter.A = x[&amp;quot;A&amp;quot;] #here A is the same across conditions, so i store A in here
for (i in 1: nrow(data)){   #here i loop across rows to see which condition the data relates to and store the appropriate parameter for that row - either b.1 or b.2
 if(data$condition[i]==1)
   parameter.b[i] &amp;lt;- x[&amp;quot;b.1&amp;quot;]
} else if(data$condition[i]==2) {
  parameter.b[i]&amp;lt;- x[&amp;quot;b.2&amp;quot;]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fast&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Fast&lt;/h5&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
parameter.b = parameter.A = numeric(nrow(data)) # first i create a vector to store the values in
parameter.A &amp;lt;- x[&amp;quot;A&amp;quot;] #here A is the sham across conditions, so i store A in here
parameter.b &amp;lt;- x[c(&amp;quot;b1&amp;quot;, &amp;quot;b2&amp;quot;)][data$condition] #here i do the same as above, in a faster way. 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Evidently, there are many ways of doing this to make this operation more computationally efficient. You may also find you need to call the density function several times (i.e. density for one subset of data, and then for another or even in a line by line way), however, the less calls the better (faster). Also, you may find that you have main and interaction effects in the model, these are fine too, and there are many ways of implementing this, but need to be closely checked. Examples of this can be seen below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In sum, for each trial, we need to ensure that the call to the density function is referring to the correct parameter values. &lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;main-and-interaction-effects&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;main and interaction effects&lt;/h4&gt;
&lt;p&gt;In this example code snippet below, I show slow and fast ways for mapping out parameters across conditions. Here, this model includes all types of effects (similar to that in an ANOVA), where each cell of a design contains a parameter. This could also only look at main effects (i.e. only b.1 and b.2 and only slow and fast without having the interaction) or could include some interactions in addition to the main effect. This becomes particularly important in designs with more experimental factors.&lt;/p&gt;
&lt;div id=&#34;slow-1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Slow&lt;/h5&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
parameter.b = parameter.A = numeric(nrow(data)) # first i create a vector to store the values in
parameter.A = x[&amp;quot;A&amp;quot;] #here A is the same across conditions, so i store A in here
for (i in 1: nrow(data)){   #here i loop across rows to see which condition the data relates to and store the appropriate parameter for that row - either b.1 or b.2
 if(data$condition[i]==1)
   if(data$speed[i]==&amp;quot;slow&amp;quot;){
      parameter.b[i] &amp;lt;- x[&amp;quot;b.1.slow&amp;quot;]
   } else if(data$speed[i]==&amp;quot;fast&amp;quot;)
      parameter.b[i] &amp;lt;- x[&amp;quot;b.1.fast&amp;quot;]
} else if(data$condition[i]==2) {
     if(data$speed[i]==&amp;quot;slow&amp;quot;){
      parameter.b[i]&amp;lt;- x[&amp;quot;b.2.slow&amp;quot;]
     } else if(data$speed[i]==&amp;quot;fast&amp;quot;){
       parameter.b[i] &amp;lt;- x[&amp;quot;b.2.fast&amp;quot;]
     }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fast-1&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Fast&lt;/h5&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
parameter.b = parameter.A = numeric(nrow(data)) # first i create a vector to store the values in
parameter.A &amp;lt;- x[&amp;quot;A&amp;quot;] #here A is the same across conditions, so i store A in here

for (cond in unique(data$condition)){
  for (speed in unique(data$speed)){
    parameter.b &amp;lt;- x[paste0(&amp;quot;b&amp;quot;,cond,speed)]
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, this can be done in a variety of ways, but does require thorough checking to ensure parameters map correctly and effects carry.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pointintercept-method&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Point/intercept method&lt;/h5&gt;
&lt;p&gt;Another method for dealing with main and interaction effects is with a “point-intercept” method of parameter mapping. We often use this method to limit the number of parameters when the parameter space is growing. This method is similar to a treatment effect method used in many ANOVA analyses. This is referred to above when I allude to main and interaction effect parameters.&lt;/p&gt;
&lt;p&gt;For the point intercept method, there is a grand mean. For each specific condition, the parameter becomes the difference from the grand mean parameter. This means in our parameters, we have a grand mean parameter and a difference parameter. In the example above, the parameterisation is too simple, so lets look at two examples from the PMwG Doc in Chapters &lt;a href=&#34;https://newcastlecl.github.io/samplerDoc/pmwg-sampler-and-signal-detection-theory.html&#34;&gt;2&lt;/a&gt; and &lt;a href=&#34;https://newcastlecl.github.io/samplerDoc/forstmannChapter.html#the-speed-accuracy-tradeoff-in-perceptual-decisions&#34;&gt;3&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In Chapter 2, an SDT model is used. For the parameterisation, the d’ is the difference between parameters. We could estimate a mean for non-targets, a mean for targets and a mean for lures. However, these values are arbitrary and so could move along the scale unconstrained. Instead, we set the mean of non-targets to 0. We then only need to calculate the distance (d’) from non-targets to lures and from non-targets to targets. So lets start with targets, we call this parameter d’. Then, all we need to do is add a d’ increment to the lures parameter (i.e. d’ + d’ increment) to get the mean. These parameters are all on the real line, and so the d’ increment could be negative, allowing the mean for lures to fall anywhere on the line.&lt;/p&gt;
&lt;p&gt;Secondly, in chapter 3, we use an LBA model. In this example, we could parameterise in a way where we have a grand mean for threshold with differences for the conditions. This means that threshold is constrained by the mean parameter, and then for the other parameters, we add an increment to see differences between conditions. This is especially useful if we had bias in responses, where threshold for left was different than threshold for right responses. In this example, we could do this;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
bL=bR=numeric(nrow(data))
for (cond in unique(data$condition)){
bL&amp;lt;-x[&amp;quot;b.mean&amp;quot;]+x[paste0(&amp;quot;b.&amp;quot;), cond]
bR&amp;lt;-x[&amp;quot;b.mean&amp;quot;]-x[paste0(&amp;quot;b.&amp;quot;), cond]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, the x[“b.i”] parameters need to be on the real line so that they can move the bias up or down for either responses (in each condition). This means that we should not take the exponential of these specific parameters at the start of the LL.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: This kind of function would also require a check so that the overall threshold parameter (after adding/subtracting the difference) couldn’t be negative, and so something like the below is needed;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if(!sample){
  if (x[&amp;quot;b&amp;quot;] &amp;lt;= any(abs(c(x[&amp;quot;b.1&amp;quot;],x[&amp;quot;b.2&amp;quot;],x[&amp;quot;b.3&amp;quot;])))) {
      return(-1e10)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;make-the-sample-component&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Make the ‘sample’ component&lt;/h4&gt;
&lt;p&gt;Next up, we make the sample function for when sample = TRUE. This is not essential for PMwG to work, but does make post-processing much easier. In this step, we use the parameters above to create data with random generation functions (such as rnorm, rbinom or rLBA). This generally requires one call to a random generation function with the parameters created above. We usually do this as a loop over rows to protect against any weirdness from the generating function. This is slower, but safer (and is only called a handful of times).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;    data$rt&amp;lt;-NA
    for (i in 1:nrow(data)){
      tmp &amp;lt;- rpackage::rfunction(n=1, A = parameter.A[i], b = parameter.b[i])  ## rpackage::rfunction could be for example rtdists::rLBA
      data$rt[i]&amp;lt;-tmp
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;make-the-likelihood-component&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Make the ‘likelihood’ component&lt;/h4&gt;
&lt;p&gt;Next up, we make the density function for when sample = FALSE. This &lt;strong&gt;IS&lt;/strong&gt; essential for PMwG to work. In this step, we use the parameters above to obtain the probability of each data point for the given parameters. This means that following the parameter specifications above, the density for data from trial i is calculated under the correct parameter inputs (i.e. parameter.A[i] and parameter.b[i]). So on each trial, we should see density[i] (of data[i]) = model_function(b=parameter.b[i], A=parameter.A[i]).&lt;/p&gt;
&lt;p&gt;Evidently, you could do this as a loop over rows, however, most density functions allow you to specify vectors of data and parameters (which is what we created above). The likelihood is generally obtained through density functions (such as dnorm, dbinom or dLBA), however, you may need to specify your own depending on the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;    out &amp;lt;- numeric(nrow(data))
    out &amp;lt;- rpackage::dfunction(data=data$rt, A = parameter.A[i], b = parameter.b[i]) ## rpackage::dfunction could be for example rtdists::dLBA
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Usually, density functions already exist or are pre-specified - many R packages are available for these. If a density function does not already exist, you probably have a mathematical model in mind. It’s important that this is tractable and able to recover effectively.&lt;/p&gt;
&lt;!-- To check recovery, see next blog post.  --&gt;
&lt;/div&gt;
&lt;div id=&#34;return&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Return&lt;/h4&gt;
&lt;p&gt;Finally, you need to return the likelihood (or randomly generated data). If you have multiple probabilities, this will require combining these. We generally do this through;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if(!sample){
return(sum(log(out)))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although occasionally, you may want to protect against really small values, and so we do;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
bad &amp;lt;- (out &amp;lt; 1e-10) | (!is.finite(out))
    out[bad] &amp;lt;- 1e-10
    out &amp;lt;- sum(log(out))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
return(sum(log(pmax(out,1e-10)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For returning data when sample = TRUE, you should write your sample function similar to above to ensure that the generated data is returned (not the original data). Then, we just need to return data;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if(sample){
return(data)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: if you return data, you need to ensure that this is only when sample = TRUE and - for a posterior generating function to work - must replace the values in the actual data. For more on simulating from the posterior, see &lt;a href=&#34;https://newcastlecl.github.io/samplerDoc/forstmannChapter.html#genppdatafunc&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;full-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Full function&lt;/h3&gt;
&lt;p&gt;Shown below is how your full likelihood function will look;&lt;/p&gt;
&lt;p&gt;(NOTE: this won’t work because it’s all example text)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
LL &amp;lt;- function(x,data,sample=FALSE){
  #note, x is a vector of named parameter values
  # data is data - but in pmwg will be data for each subject
  #sample = FALSE should be default. if true, uses x to randomly generate. If false, uses x for density. 
  
  #transform vector of parameters
  x&amp;lt;-exp(x)

 
  #map
  parameter.b = parameter.A = numeric(nrow(data)) # first i create a vector to store the values in
  parameter.A &amp;lt;- x[&amp;quot;A&amp;quot;] #here A is the same across conditions, so i store A in here
  for (cond in unique(data$condition)){
    for (speed in unique(data$speed)){
      parameter.b &amp;lt;- x[paste0(&amp;quot;b&amp;quot;,cond,speed)]
    }
  }

  #generate
  if(sample){
    data$rt&amp;lt;-NA
    for (i in 1:nrow(data)){
      tmp &amp;lt;- rpackage::rfunction(n=1, A = parameter.A[i], b = parameter.b[i])  ## i.e. rtdists::rLBA
      data$rt[i]&amp;lt;-tmp
    }
 
  #return
  return(data)
 
  } else{
     #checks
    if(!sample){
     if (any(data$RT &amp;lt; x[&amp;quot;t0&amp;quot;])) {  
       return(-1e10)
     }
    }
    out &amp;lt;- numeric(nrow(data))
    out &amp;lt;- rpackage::dfunction(data=data$rt, A = parameter.A[i], b = parameter.b[i]) ## i.e. rtdists::dLBA
    return(sum(log(pmax(out,1e-10)))    
     }
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;testing-your-ll-function.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Testing your LL function.&lt;/h2&gt;
&lt;p&gt;Finally, it’s &lt;strong&gt;REALLY&lt;/strong&gt; important to test your likelihood function. In this section, I’ll go through a couple of methods I’ve given titles to.&lt;/p&gt;
&lt;div id=&#34;line-by-line-testing&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Line by line testing&lt;/h4&gt;
&lt;p&gt;For this method, first make some x values, for example;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pars&amp;lt;- c(&amp;quot;A&amp;quot;,&amp;quot;b.1&amp;quot;,&amp;quot;b.2&amp;quot;) #names of params
x &amp;lt;- c(1,0.5,0.7) #make sure you give different conditions different values so they don&amp;#39;t all look the same and its confusing. 
names(x)&amp;lt;-pars
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From here, use this x and your data (or a subset) to work through each line of the function. What you should see is values for the correct parameters mapping to the correct conditions in your vector (for example x[“b.1”] matches with data that is from condition 1 etc). In our example above, parameter.A will be a vector of 1’s (x[“A’]=1) and parameter.b will be a vector of 0.5’s and 0.7’s which match to data$condition.&lt;/p&gt;
&lt;p&gt;From here, work through your LL running each line (or loop) of the function to ensure there are no errors, values map correctly, and the density function works correctly. This means that parameter vectors should use the correct conditions and values and output as expected.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;alternate-input-values&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Alternate input values&lt;/h4&gt;
&lt;p&gt;On the topic of density functions working correctly, this is the next step. For this step, again, create some parameter values. Then just run your LL with these values and data. Then, make some new parameter vectors, and repeat. What we should see is that;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;the likelihood changes with different input values&lt;/li&gt;
&lt;li&gt;the likelihood changes in the right direction (i.e. for silly parameter values, a low likelihood is returned)&lt;/li&gt;
&lt;li&gt;the likelihood should quickly return bad likelihood for bad parameter values (as discussed above)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is a quick and easy test that can be done many times and should give you some idea of whether your likelihood is ‘working’. There are more thorough and robust checks shown in &lt;a href=&#34;https://newcastlecl.github.io/samplerDoc/forstmannChapter.html#checking-the-lba-log-likelihood-function&#34;&gt;Chapter 3&lt;/a&gt; of the sampler doc, but this is a good first pass.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;profile-plots&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Profile Plots&lt;/h4&gt;
&lt;p&gt;Another strong way to test the likelihood is with profile plots. This is a basic kind of simulation and recovery exercise where we first generate some data using the sample = TRUE argument and then test different values using the sample = FALSE argument. Here, I use a function from the &lt;a href=&#34;https://github.com/NewcastleCL/pmwg_toolkit&#34;&gt;PMwG Toolkit&lt;/a&gt; to make my profile plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#### profile plot function
#### Contributed by Reilly Innes
#### Use pwmg_profilePlot on an initiated PMwG object to ensure your likelihood functions correctly
#### This function takes in the initial theta_mu estimates and simulates a small amount of data based on these
#### plots are returned which show how the likelihood changes as the generating value changes
#### For the different generating values, we use small increments (which can be negative or positive)
#### We expect to see inverse U shaped curves, where the likelihood is most likely at the generating value and falls away as we get further from the generating value

#### NOTE: The likelihood function needs both sample = TRUE and sample = FALSE arguments to function correctly
#### Also, avoid putting protective statements (like if(any(data$rt)&amp;lt;t0) etc) at the beginning of the function
#### These statements should go in the if(sample=FALSE) part
require(ggplot2)
require(tidyr)
## Loading required package: tidyr
require(pmwg)
## Loading required package: pmwg

pmwg_profilePlot = function(sampler, generating_values=NULL){
  if(is.null(generating_values)){
    #create generating values based on theta_mu
    generating_values &amp;lt;- sampler$samples$theta_mu
    names(generating_values)&amp;lt;- sampler$par_names
  } else{
    names(generating_values)&amp;lt;-sampler$par_names
  }
  #make the test data set. here I use a tenth of the total data for speed
  test &amp;lt;- sampler$ll_func(x = generating_values,
                  data = sampler$data[c(1:(nrow(sampler$data)/10)),],
                  sample = TRUE)
  # this is the number of values to test and plot. 
  n_values &amp;lt;- 9
  tmp &amp;lt;- array(dim=c(n_values,sampler$n_pars))
  #here i make the increment, however, you may wish to make this smaller or larger.
  #the increment here goes from theta_mu - .2 to theta_mu + .2, with n_values length
  increment&amp;lt;- seq(from=-.2, to=.2, length.out = n_values)
  
  for (i in 1:sampler$n_pars){
    for (j in 1:n_values){
      #need to use all generating values except the current parameter being tested
      test_values &amp;lt;- generating_values
      #here we change the current parameter by adding the increment
      test_values[i] &amp;lt;- generating_values[i] + increment[j]
      #test the likelihood given these new values and the test data
      tmp[j, i] &amp;lt;- sampler$ll_func(x = test_values, data=test, sample=F)
    }
  }
  #prepare output for plotting
  colnames(tmp)&amp;lt;-sampler$par_names
  tmp&amp;lt;-as.data.frame(tmp)
  tmp &amp;lt;- tidyr::pivot_longer(tmp, everything(), names_to = &amp;quot;pars&amp;quot;, values_to = &amp;quot;likelihood&amp;quot;)
  tmp$increment &amp;lt;- rep(increment, each=sampler$n_pars)
  ### next, plot these values for each parameter
  ggplot2::ggplot(tmp, aes(x=increment, y= likelihood))+
    geom_point()+
    geom_line()+
    facet_wrap(~pars, scales = &amp;quot;free&amp;quot;)+
    theme_bw()+
    geom_vline(xintercept = 0, color=&amp;quot;red&amp;quot;, alpha = 0.3)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this function, it is essential that both components of the likelihood (sample=&lt;code&gt;TRUE&lt;/code&gt; and &lt;code&gt;FALSE&lt;/code&gt;) work correctly. This means that parameters are first transformed and mapped before being used by each component. It also means any protective statements are limited to the sample = &lt;code&gt;FALSE&lt;/code&gt; section. The function then takes in an initialised PMwG object (from the init function in PMwG - although you can also use a sampled object). Now let’s test this out with a simple LBA example from Forstmann (2008).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
data&amp;lt;- forstmann
data$condition &amp;lt;- as.factor(data$condition)

lba_loglike &amp;lt;- function(x, data, sample = FALSE) {
  x &amp;lt;- exp(x)

  bs &amp;lt;- x[&amp;quot;A&amp;quot;] + x[c(&amp;quot;b1&amp;quot;, &amp;quot;b2&amp;quot;, &amp;quot;b3&amp;quot;)][data$condition]
  t0 &amp;lt;- x[c(&amp;quot;t0.1&amp;quot;,&amp;quot;t0.2&amp;quot;,&amp;quot;t0.3&amp;quot;)][data$condition]
  out &amp;lt;- nrow(data)
  if (sample) {
    data$rt=NA
    data$resp = NA
    for (i in 1:nrow(data)){
    out &amp;lt;- rtdists::rLBA(n = 1,
                         A = x[&amp;quot;A&amp;quot;],
                         b = bs[i],
                         t0 = t0[i],
                         mean_v = x[c(&amp;quot;v1&amp;quot;, &amp;quot;v2&amp;quot;)],
                         sd_v = c(1, 1),
                         distribution = &amp;quot;norm&amp;quot;,
                         silent = TRUE)
    data$rt[i] &amp;lt;- out$rt
    data$resp[i] &amp;lt;- out$resp
    }
    return(data)
  } else {
    if (any(min(data$rt) &amp;lt; c(x[&amp;quot;t0.1&amp;quot;], x[&amp;quot;t0.2&amp;quot;], x[&amp;quot;t0.3&amp;quot;]))  ) {
      return(-1e10)
    }
    out &amp;lt;- rtdists::dLBA(rt = data$rt,
                         response = data$resp,
                         A = x[&amp;quot;A&amp;quot;],
                         b = bs,
                         t0 = t0,
                         mean_v = x[c(&amp;quot;v1&amp;quot;, &amp;quot;v2&amp;quot;)],
                         sd_v = c(1, 1),
                         distribution = &amp;quot;norm&amp;quot;,
                         silent = TRUE)
    bad &amp;lt;- (out &amp;lt; 1e-10) | (!is.finite(out))
    out[bad] &amp;lt;- 1e-10
    out &amp;lt;- sum(log(out))
    return(out)
  }
  }


# Specify the parameters and priors -------------------------------------------

# Vars used for controlling the run
pars &amp;lt;- c(&amp;quot;b1&amp;quot;, &amp;quot;b2&amp;quot;, &amp;quot;b3&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;v1&amp;quot;, &amp;quot;v2&amp;quot;, &amp;quot;t0.1&amp;quot;,&amp;quot;t0.2&amp;quot;,&amp;quot;t0.3&amp;quot;)
priors &amp;lt;- list(
  theta_mu_mean = c(0,0,0,0,1,1,-3,-3,-3),
  theta_mu_var = diag(c(1,1,1,1,2,2,0.1,0.1,0.1))
)

# Create the Particle Metropolis within Gibbs sampler object ------------------

sampler &amp;lt;- pmwgs(
  data = data,
  pars = pars,
  prior = priors,
  ll_func = lba_loglike
)


start_points &amp;lt;- list(
  mu = log(c(1,1,1,1,2,3,0.07,0.08,0.09)),
  sig2 = diag(rep(.1, length(pars)))
)


sampler &amp;lt;- init(sampler, start_mu = start_points$mu,
                start_sig = start_points$sig2, display_progress = FALSE, particles = 250)

pmwg_profilePlot(sampler)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;staticforstProfile-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From these plots we can see that for most of the parameters, there is a nice inverse U curve, which centers at 0 increment. This means that the generating value is also the most likely value for that parameter. This indicates that the parameter values simulate and recover well and our likelihood seems to function correctly. If you see different patterns, try using sensible start points or check line by line what could cause the function to return weird values (this could be due to one of the protective statements or could be due to parameters not mapping correctly).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Gunawan, D., Hawkins, G. E., Tran, M. N., Kohn, R., &amp;amp; Brown, S. D. (2020). New estimation approaches for the hierarchical Linear Ballistic Accumulator model. &lt;em&gt;Journal of Mathematical Psychology, 96&lt;/em&gt;, 102368.&lt;/p&gt;
&lt;p&gt;Huang, A., &amp;amp; Wand, M. P. (2013). Simple marginally noninformative prior distributions for covariance matrices. &lt;em&gt;Bayesian Analysis, 8(2)&lt;/em&gt;, 439-452.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How to Posterior</title>
      <link>/post/2021-05-04-posterior/posterior_inference/</link>
      <pubDate>Tue, 04 May 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-05-04-posterior/posterior_inference/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;So you’ve made the PMwG sampler work and now you have a &lt;code&gt;sampled&lt;/code&gt; object, what’s next? Well now is where the fun stuff happens - checking, inference, plots and posteriors. In this short blog post, I’ll show several user contributed functions to make plots and output that is a great starting step for analysing your posterior model estimates.&lt;/p&gt;
&lt;p&gt;To start with, we’ll go through some checks to make sure the sampler has run as we expected. Next, we’ll go through analysing posterior parameter and random effects (and convariance) estimates. Finally, I’ll provide code to create posterior predictive data so that you can compare the model estimates to the real data.&lt;/p&gt;
&lt;div id=&#34;checking&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Checking&lt;/h2&gt;
&lt;p&gt;The first thing we should do with a PMwG object is check that it sampled correctly. For this, there are two main plots I look at - the parameter ‘chains’ and the subject likelihood ‘chains’. These are shown in the sampler documentation and are great ways to quickly visualise the sampling process.&lt;/p&gt;
&lt;div id=&#34;chain-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Chain plots&lt;/h3&gt;
&lt;p&gt;From the parameter chains, we would expect to see stationarity of parameters. By this I mean that the parameter chains are flat and not trending up or down. If they are moving up or down, we may not have yet reached the posterior. Further, the chains should be thin and not filling all of the prior space. If they are quite wide, this could indicate that the parameter is not informed. In the acceptance rates, if this is happening, it is likely that you would see very high acceptance as well. Often this happens when the likelihood isn’t specified correctly (for example, those pesky factors being read in the wrong way) and as a consequence, the likelihood returned on each step doesn’t change. This will be evident in the chain plot.&lt;/p&gt;
&lt;p&gt;Secondly, another good plot to look at is the subject likelihood plot. In this plot, similar to the parameter chains, we are looking for stationarity in the likelihood estimates for each subject (each line). It is likely that the subject likelihood plot will rapidly jump from a low value to a high one and remain in this region. If all estimates are the same for all subjects, this can also indicate a problem in the likelihood function, similar to the one above.&lt;/p&gt;
&lt;p&gt;Shown below is code and example outputs for the chain plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
pmwg_chainPlots &amp;lt;- function(samples, subjectParPlot = F, parameterPlot = T, subjectLLPlot = T){
  if (subjectParPlot){
    par(mfrow = c(2, ceiling(samples$n_pars/2)))
    for (par in samples$par_names){
      matplot(t(samples$samples$alpha[par,,]),type=&amp;quot;l&amp;quot;, main = par, xlab = &amp;quot;samples&amp;quot;, ylab = &amp;quot;ParameterValue&amp;quot;)
    } 
  }
  par(mfrow=c(1,1))
  if(parameterPlot) matplot(t(samples$samples$theta_mu), type=&amp;quot;l&amp;quot;, main = &amp;quot;Paramater chains&amp;quot;, ylab = &amp;quot;Parameter Value&amp;quot;, xlab = &amp;quot;samples&amp;quot;)
  if(subjectLLPlot) matplot(t(samples$samples$subj_ll), type=&amp;quot;l&amp;quot;, main = &amp;quot;LogLikelihood chains per subject&amp;quot;, ylab = &amp;quot;LogLikelihood&amp;quot;, xlab = &amp;quot;samples&amp;quot;)
  if(sum(subjectParPlot, parameterPlot, subjectLLPlot) &amp;gt; 1) print(&amp;#39;plots presented behind eachother&amp;#39;)
  
}

pmwg_chainPlots(sampled)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;staticchains-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;staticchains-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;plots presented behind eachother&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From these outputs we can see that the parameter plots show thin stationary lines. Secondly, we can see the likelihood plots show a rapid rise and then stationary, separated lines. These are indicative that sampling worked as intended.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameter-histograms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Parameter histograms&lt;/h3&gt;
&lt;p&gt;Secondly, it’s important to look at the posterior density of parameter estimates. To do this, I use a function that plots a historgram of each parameter. Additionally, I overlay the prior density to see if our priors were too restrictive or if we haven’t learnt anything new (i.e. prior = posterior).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;

pmwg_parHist &amp;lt;- function(samples, bins =30, prior = FALSE ){
  if (!prior){
    chains &amp;lt;- as.array(as_mcmc(samples))
    mcmc_hist(chains)
  } else{
    theta &amp;lt;- t(sampled$samples$theta_mu)
    theta&amp;lt;-as.data.frame(theta)
    long &amp;lt;- sum(sampled$samples$stage==&amp;quot;sample&amp;quot;)
    theta &amp;lt;- theta[c((length(theta[,1])-long+1):length(theta[,1])),]
    theta &amp;lt;- pivot_longer(theta, cols = everything(), names_to = &amp;quot;pars&amp;quot;, values_to = &amp;quot;estimate&amp;quot; )
    prior_mean &amp;lt;- sampled$prior$theta_mu_mean
    prior_var &amp;lt;- diag(sampled$prior$theta_mu_var)
    priors = NULL
    for (i in 1:sampled$n_pars){
      tmp &amp;lt;- rnorm(n=long, mean=prior_mean[i], sd=prior_var[i])
      tmp &amp;lt;- as.data.frame(tmp)      
      priors&amp;lt;-  c(priors, tmp[1:long,])
    }
    priors&amp;lt;-as.data.frame(priors)
    y &amp;lt;- as.factor(sampled$par_names)
    theta&amp;lt;-theta[order(factor(theta$pars, levels = y)),]
    theta$prior &amp;lt;- priors$priors
    theta$pars&amp;lt;- as.factor(theta$pars)
    
    
    ggplot(theta, aes(estimate))+
      geom_histogram(aes(y =..density..), bins = bins)+
      geom_density(aes(prior))+
      facet_wrap(~pars, scales = &amp;quot;free_y&amp;quot;)+
      theme_bw()
  }
}

pmwg_parHist(sampled, bins=50, prior =T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;staticparamHist-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this example, we can see that for t0, the prior may be too restrictive, however, for other estimates, all seems as expected, with posterior samples falling in a thin range.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;posterior-estimates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Posterior Estimates&lt;/h2&gt;
&lt;p&gt;Now we know the sampler worked as intended, we can look at the posterior parameter estimates. For this part, it is crucial to remember the parameter transformations. For example, in the LBA, all input parameters must be positive, so we take the exponent of the proposed values on each iteration in the likelihood function. This means that when we interpret posterior parameters, we should also take the exponent of the value. For example, t0 in the example above is centered around -2 – an impossible value– and so we should take the exponent of it (0.135) as this is what the model is actually using.&lt;/p&gt;
&lt;div id=&#34;parameter-histograms-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Parameter histograms&lt;/h3&gt;
&lt;p&gt;From the previous section, we can also use this plot to see where the posterior parameters lie. As mentioned above, we need to take the exponent of these LBA values, so this time I’ll include the transformations in the function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;

pmwg_parHist &amp;lt;- function(samples, bins =30, prior = FALSE ){
  if (!prior){
    chains &amp;lt;- as.array(as_mcmc(samples))
    mcmc_hist(chains)
  } else{
    theta &amp;lt;- exp(t(sampled$samples$theta_mu)) ### exp here
    theta&amp;lt;-as.data.frame(theta)
    long &amp;lt;- sum(sampled$samples$stage==&amp;quot;sample&amp;quot;)
    theta &amp;lt;- theta[c((length(theta[,1])-long+1):length(theta[,1])),]
    theta &amp;lt;- pivot_longer(theta, cols = everything(), names_to = &amp;quot;pars&amp;quot;, values_to = &amp;quot;estimate&amp;quot; )
    prior_mean &amp;lt;- exp(sampled$prior$theta_mu_mean) ## exp here
    prior_var &amp;lt;- exp(diag(sampled$prior$theta_mu_var)) ## exp here
    priors = NULL
    for (i in 1:sampled$n_pars){
      tmp &amp;lt;- rnorm(n=long, mean=prior_mean[i], sd=prior_var[i])
      tmp &amp;lt;- as.data.frame(tmp)      
      priors&amp;lt;-  c(priors, tmp[1:long,])
    }
    priors&amp;lt;-as.data.frame(priors)
    y &amp;lt;- as.factor(sampled$par_names)
    theta&amp;lt;-theta[order(factor(theta$pars, levels = y)),]
    theta$prior &amp;lt;- priors$priors
    theta$pars&amp;lt;- as.factor(theta$pars)
    
    
    ggplot(theta, aes(estimate))+
      geom_histogram(aes(y =..density..), bins = bins)+
      geom_density(aes(prior))+
      facet_wrap(~pars, scales = &amp;quot;free_y&amp;quot;)+
      theme_bw()
  }
}

pmwg_parHist(sampled, bins=50, prior =T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;staticparamHist2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, we can see that t0 is just above 0, v1 is greater than v2 (correct &amp;gt; error drift rates) and there are three separable threshold values.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;output-tables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Output tables&lt;/h3&gt;
&lt;p&gt;Whilst the above plot is useful, we may wish to get some more fine grained analysis. For this, again looking at the group level (theta) values, we can create some output tables.&lt;/p&gt;
&lt;p&gt;First we look at mean parameter estimates (and variance using 95% credible intervals)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
qL=function(x) quantile(x,prob=.05,na.rm = TRUE) #for high and low quantiles
qH=function(x) quantile(x,prob=.95,na.rm = TRUE)

tmp &amp;lt;- exp(apply(sampled$samples$theta_mu[,sampled$samples$stage==&amp;quot;sample&amp;quot;],1,mean))
lower &amp;lt;- exp(apply(sampled$samples$theta_mu[,sampled$samples$stage==&amp;quot;sample&amp;quot;],1,qL))
upper &amp;lt;- exp(apply(sampled$samples$theta_mu[,sampled$samples$stage==&amp;quot;sample&amp;quot;],1,qH))
tmp &amp;lt;- t(rbind(tmp,lower,upper))
kable(tmp)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;tmp&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;lower&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;upper&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;b1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8832800&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7672251&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0082430&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;b2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0173988&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8724713&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1971625&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;b3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.4128471&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.2075098&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.6497759&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.5383791&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.3499831&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.7429653&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;v1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.3117580&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.7409280&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.9455836&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;v2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1322673&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9322809&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.3405226&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;t0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1540344&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1243581&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1915416&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This is a nice way to view the posterior parameter means and ranges. Next, we can look at the mean covariance structure, which we will transform into a correlation matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov&amp;lt;-apply(sampled$samples$theta_sig[,,sampled$samples$idx-1000:sampled$samples$idx] ,1:2, mean)
colnames(cov)&amp;lt;-pars
rownames(cov)&amp;lt;-pars
cor&amp;lt;-cov2cor(cov) #transforms covariance to correlation matrix
kable(cor)&lt;/code&gt;&lt;/pre&gt;
&lt;table style=&#34;width:100%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;3%&#34; /&gt;
&lt;col width=&#34;14%&#34; /&gt;
&lt;col width=&#34;14%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;14%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;14%&#34; /&gt;
&lt;col width=&#34;14%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;b1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;b2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;b3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;A&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;v1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;v2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;b1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2261599&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0722276&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0124526&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2234376&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1664706&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.2142574&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;b2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2261599&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1250495&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0768053&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1306218&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0276819&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0462468&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;b3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0722276&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1250495&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0710326&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1006157&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0548435&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1736373&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0124526&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0768053&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0710326&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1164379&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1028593&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0674493&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;v1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2234376&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1306218&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1006157&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1164379&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0823956&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0483617&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;v2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1664706&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0276819&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0548435&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1028593&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0823956&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.1596716&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;t0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.2142574&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0462468&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1736373&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0674493&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0483617&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.1596716&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This is a good first summary, but now lets plot this with corrplot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
corrplot(cor, method=&amp;quot;color&amp;quot;, type = &amp;quot;lower&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;staticcorplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This covariance analysis looks at the correlations between parameter values. Here we can see that b1 and v1 are positively correlated, whereas t0 and b1 are negatively correlated. PMwG is great at dealing with models with high autocorrelation of parameters, and so it is important to check these kinds of outputs to see where correlations exist in the model and what could underpin these (or whether these should NOT be correlated).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;inference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Inference&lt;/h3&gt;
&lt;p&gt;Importantly for psych research, we might wish to do inference on the parameter estimates. Here, we can use any kind of classic inference test to look for differences between parameter values. We can also test this by looking at whether the difference between posterior parameter estimates crosses zero.&lt;/p&gt;
&lt;p&gt;First though, lets look to see if there’s a difference between the b parameters using a bayesian anova.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(BayesFactor)
## Loading required package: coda
## ************
## Welcome to BayesFactor 0.9.12-4.2. If you have questions, please contact Richard Morey (richarddmorey@gmail.com).
## 
## Type BFManual() to open the manual.
## ************
tmp &amp;lt;- as.data.frame(apply(sampled$samples$theta_mu[,sampled$samples$stage==&amp;quot;sample&amp;quot;],1,exp))

tmp &amp;lt;- tmp[,c(1:3)]
tmp &amp;lt;- tmp %&amp;gt;% pivot_longer(everything() , names_to = &amp;quot;pars&amp;quot;, values_to = &amp;quot;estimate&amp;quot;)
tmp &amp;lt;- as.data.frame(tmp)
tmp$pars&amp;lt;-as.factor(tmp$pars)

anovaBF(estimate ~ pars, data=tmp)
## Bayes factor analysis
## --------------
## [1] pars : 7.809663e+1090 ±0%
## 
## Against denominator:
##   Intercept only 
## ---
## Bayes factor type: BFlinearModel, JZS&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From this output, there is strong evidence for a difference between these conditions. Next we can plot these estimates to see the difference between b values visually.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
plot.data &amp;lt;- tmp %&amp;gt;% group_by(pars) %&amp;gt;% summarise(avg = mean(estimate),
                                                  lower = qL(estimate),
                                                  upper = qH(estimate)) %&amp;gt;% ungroup()


ggplot(plot.data, aes(x=pars, y=avg)) + 
  geom_point(size=3) +
  geom_errorbar(aes(ymin=lower,ymax=upper),width=0.2) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;staticplotsB-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another method we can use to look for a difference between parameters is to check if the difference between posterior parameters crosses 0. If it does not, then one posterior estimate is reliably greater than the other. Lets check this for b1 and b2. Here, we can set a frequentist style criterion (lets say 95%), and if 95% of the difference between samples are on one side of zero, we can conclude that there is a difference between these parameters. Alternatively, we could run a separate model where these parameters were not separated and then do model comparison methods for these models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmp &amp;lt;- as.data.frame(apply(sampled$samples$theta_mu[,sampled$samples$stage==&amp;quot;sample&amp;quot;],1,exp))

tmp &amp;lt;- tmp[,c(1:2)]
tmp$diff &amp;lt;- tmp$b1-tmp$b2
mean(tmp$diff&amp;gt;0)
## [1] 0.062&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we can see that 93.8% of the samples are greater than zero, so there is not conclusive evidence that these parameters vary at the group level.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;posterior-predictive-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Posterior Predictive Data&lt;/h2&gt;
&lt;p&gt;Finally, an important part of post-model processing is generating posterior predictive data. Using this generated data, we can check to see if the posterior data ‘fits’ the data. To generate posterior predictive data, we first need to randomly sample some individual random effects. We then use these in the log-likelihood function (using &lt;code&gt;sample=TRUE&lt;/code&gt;) to generate data. For this part, it is important that your likelihood function is set up correctly to generate data. However, if this is not the case, you can restructure your likelihood function and change the function below so that it still operates correctly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
pmwg_generatePosterior &amp;lt;- function(sampled, n){
  n.posterior=n # Number of parameter samples from posterior distribution.
  pp.data=list()
  S = sampled$n_subjects
  data=sampled$data
  sampled_stage = length(sampled$samples$stage[sampled$samples$stage==&amp;quot;sample&amp;quot;])
  for (s in 1:S) {
    cat(s,&amp;quot; &amp;quot;)
    iterations=round(seq(from=(sampled$samples$idx-sampled_stage) , to=sampled$samples$idx, length.out=n.posterior))
    for (i in 1:length(iterations)) {
      x &amp;lt;- sampled$samples$alpha[,s,iterations[i]]
      names(x) &amp;lt;- sampled$par_names
      tmp=sampled$ll_func(x=x,data=data[as.integer(as.numeric(data$subject))==s,],sample=TRUE) ##change here to your own sampled function if your likelihood was not set up correctly
      if (i==1) {
        pp.data[[s]]=cbind(i,tmp)
      } else {
        pp.data[[s]]=rbind(pp.data[[s]],cbind(i,tmp))
      }
    }
  }
  return(pp.data)
}

pp.data&amp;lt;-generate.posterior(sampled,20) #i do 20 because that seems fine
tmp=do.call(rbind,pp.data) #binds together
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   i subject        rt resp condition
## 1 1       1 0.8109699    1         1
## 2 1       1 1.3727252    1         2
## 3 1       1 1.0959211    2         3
## 4 1       1 0.4747878    1         1
## 5 1       1 0.9924096    1         2
## 6 1       1 2.8030712    1         3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here you can see that I’ve generated 20 samples for each person. The column &lt;code&gt;i&lt;/code&gt; shows the iteration. This does take a little bit of time, but not too long (it depends on your likelihood). When writing your likelihood, we often try and make this step as safe as possible (rather than fast) as it is only called several times.&lt;/p&gt;
&lt;p&gt;Now we have posterior predictive data, we can plot this against the real data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;

summ &amp;lt;- tmp %&amp;gt;% group_by(i,condition) %&amp;gt;%
  summarise(MeanRT = mean(rt,na.rm=TRUE),
            ACC = mean(resp),
            qL = qL(rt),
            qH = qH(rt),
            qLA = qL(resp),
            qHA = qH(resp))%&amp;gt;%
  ungroup()
## `summarise()` has grouped output by &amp;#39;i&amp;#39;. You can override using the `.groups` argument.

summ$source &amp;lt;- &amp;quot;model&amp;quot;
summD &amp;lt;- data %&amp;gt;% group_by(condition) %&amp;gt;%
  summarise(MeanRT = mean(rt,na.rm=TRUE),
            ACC = mean(resp),
            qL = qL(rt),
            qH = qH(rt),
            qLA = qL(resp),
            qHA = qH(resp))%&amp;gt;%
  ungroup()

summD$source &amp;lt;- &amp;quot;data&amp;quot;

ggplot(summ, aes(x=condition, y=ACC, color=source, group=source))+geom_point()+geom_col(data=summD, aes(x=condition, y=ACC), alpha=0.2)+theme_bw() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;staticplotPost-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(summ, aes(x=condition, y=MeanRT, color=source, group=source))+geom_point()+geom_col(data=summD, aes(x=condition, y=MeanRT), alpha=0.2)+theme_bw() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;staticplotPost-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In these plots, I simply show the mean rt and accuracy for both the data (bars) and each posterior predictive iteration (20 dots). There are a variety of alternative methods to showing the descriptive adequacy of the model, such as Q-Q plots and plots across subjects, however, this is a quick way to check the fit of the model and a stepping stone to better analysis and graphs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-comparison&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Comparison&lt;/h2&gt;
&lt;p&gt;Finally, an important part of post modelling is often model comparison. There are many methods of model comparison, and they all have advantages and disadvantages - which I’m not going to go into. Here, I show two methods of model comparison - DIC and IS2. If you have the time and computer resources, we recommend using IS2 with PMwG objects. This comes from the paper by Minh-Ngoc Tran and colleagues for &lt;a href=&#34;https://link.springer.com/article/10.3758/s13428-020-01348-w&#34;&gt;robustly estimating the marginal likelihood by importance sampling&lt;/a&gt;. An important caveat of this method however, is that it relies heavily on the settings of the prior (which are input &lt;em&gt;before&lt;/em&gt; sampling). If the prior is too restrictive, too broad or just wrong, then estimates will be biased. This means that more complex models will be more penalized when the prior is set incorrectly (or uninformed), and why it is crucial to check the posterior parameter estimates (with functions like that above - e.g. pmwg_ParHist) before doing this kind of model comparison.&lt;/p&gt;
&lt;div id=&#34;dic&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;DIC&lt;/h3&gt;
&lt;p&gt;DIC is a commonly used information criteria for comparing models. The function below takes in a sampled pmwg object and returns the DIC value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;

pmwg_DIC=function(sampled,pD=FALSE){
  nsubj=length(unique(sampled$data$subject))
  
  # the mean likelihood of the overall (sampled-stage) model, separately for each subject
  mean.like &amp;lt;- apply(sampled$samples$subj_ll[,sampled$samples$stage==&amp;quot;sample&amp;quot;],1,mean)
  
  # the mean of each parameter across iterations. Keep dimensions for parameters and subjects
  mean.params &amp;lt;- t(apply(sampled$samples$alpha[,,sampled$samples$stage==&amp;quot;sample&amp;quot;],1:2,mean))
  
  # i name mean.params here so it can be used by the log_like function
  colnames(mean.params)&amp;lt;-sampled$par_names
  
  # log-likelihood for each subject using their mean parameter vector
  mean.params.like &amp;lt;- numeric(ncol(mean.params))
  data &amp;lt;- transform(sampled$data, subject=match(subject, unique(subject)))
  for (j in 1:nsubj) {
    mean.params.like[j] &amp;lt;- sampled$ll_func(mean.params[j,], data=data[data$subject==j,], sample=FALSE)
  }
  
  # Effective number of parameters
  pD &amp;lt;- sum(-2*mean.like + 2*mean.params.like)
  
  # Deviance Information Criterion
  DIC &amp;lt;- sum(-4*mean.like + 2*mean.params.like)
  
  if (pD){
    return(c(&amp;quot;DIC&amp;quot;=DIC,&amp;quot;effective parameters&amp;quot;=pD))
  }else{
    return(DIC)
  }
  
}

pmwg_DIC(sampled)
##                  DIC effective parameters 
##             2135.809              124.878&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A DIC value on it’s own is relatively meaningless unless compared against competing models, so we could fit another model where threshold did not vary between condition 1 and condition 2 and then check which of the models had the lower DIC.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;is2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;IS2&lt;/h3&gt;
&lt;p&gt;Importance sampling squared is another method of model comparison. IS2 allows for robust and unbiased estimation of the marginal likelihood. The script below will run IS2 on a sampled object once loaded in, however, it can take some time. Typically, we generate around 10,000 IS samples, with 250 particles. The IS2 method works by first generating an importance distribution for the fixed parameters. This importance distribution is constructed by fitting a mixture of normal or Student t distributions to these MCMC samples. We then construct conditional proposal parameters - called particles - for each subject. The marginal likelihood is then estimated unbiasedly which is combined with the importance distribution. From this method, the importance sampling procedure is in itself an importance sampling procedure which can be used to estimate the likelihood.&lt;/p&gt;
&lt;p&gt;IS2 Script;&lt;/p&gt;
&lt;pre class=&#34;r fold-hide&#34;&gt;&lt;code&gt;
#######       IS2 t-distribution Code       #########

## set up environment and packages 
rm(list=ls())
library(mvtnorm)
library(MCMCpack)
library(rtdists)
library(invgamma)
library(mixtools)
library(condMVNorm)
library(parallel)
load(&amp;quot;sampled.Rdata&amp;quot;)

cpus = 20
###### set up variables #####
# number of particles, samples, subjects, random effects etc
n_randeffect=sampled$n_pars
n_subjects = sampled$n_subjects
n_iter = length(sampled$samples$stage[sampled$samples$stage==&amp;quot;sample&amp;quot;])
length_draws = sampled$samples$idx #length of the full transformed random effect vector and/or parameter vector
IS_samples = 10000 #number of importance samples
n_particles = 250 #number of particles
v_alpha = 2  #?
pars = sampled$pars


# grab the sampled stage of PMwG
# store the random effects
alpha &amp;lt;- sampled$samples$alpha[,,sampled$samples$stage==&amp;quot;sample&amp;quot;]
# store the mu
theta &amp;lt;- sampled$samples$theta_mu[,sampled$samples$stage==&amp;quot;sample&amp;quot;]
# store the cholesky transformed sigma
sig &amp;lt;- sampled$samples$theta_sig[,,sampled$samples$stage==&amp;quot;sample&amp;quot;]
# the a-hlaf is used in  calculating the Huang and Wand (2013) prior. 
# The a is a random sample from inv gamma which weights the inv wishart. The mix of inverse wisharts is the prior on the correlation matrix
a_half &amp;lt;- log(sampled$samples$a_half[,sampled$samples$stage==&amp;quot;sample&amp;quot;])



unwind=function(x,reverse=FALSE) {

  if (reverse) {
    ##        if ((n*n+n)!=2*length(x)) stop(&amp;quot;Wrong sizes in unwind.&amp;quot;)
    n=sqrt(2*length(x)+0.25)-0.5 ## Dim of matrix.
    out=array(0,dim=c(n,n))
    out[lower.tri(out,diag=TRUE)]=x
    diag(out)=exp(diag(out))
    out=out%*%t(out)
  } else {
    y=t(chol(x))
    diag(y)=log(diag(y))
    out=y[lower.tri(y,diag=TRUE)]
  }
  return(out)
}

robust_diwish = function (W, v, S) 
{
  if (!is.matrix(S)) 
    S &amp;lt;- matrix(S)
  if (nrow(S) != ncol(S)) {
    stop(&amp;quot;S not square in diwish().\n&amp;quot;)
  }
  if (!is.matrix(W)) 
    W &amp;lt;- matrix(W)
  if (nrow(W) != ncol(W)) {
    stop(&amp;quot;W not square in diwish().\n&amp;quot;)
  }
  if (nrow(S) != ncol(W)) {
    stop(&amp;quot;W and X of different dimensionality in diwish().\n&amp;quot;)
  }
  if (v &amp;lt; nrow(S)) {
    stop(&amp;quot;v is less than the dimension of S in  diwish().\n&amp;quot;)
  }
  p &amp;lt;- nrow(S)
  gammapart &amp;lt;- sum(lgamma((v + 1 - 1:p)/2))
  ldenom &amp;lt;- gammapart + 0.5 * v * p * log(2) + 0.25 * p * (p - 
                                                             1) * log(pi)
  cholS &amp;lt;- chol(S)
  #cholW &amp;lt;- chol(W)
  
  
  cholW &amp;lt;- tryCatch(chol(W),error= return(1e-10))   
  halflogdetS &amp;lt;- sum(log(diag(cholS)))
  halflogdetW &amp;lt;- sum(log(diag(cholW)))
  invW &amp;lt;- chol2inv(cholW)
  exptrace &amp;lt;- sum(S * invW)
  lnum &amp;lt;- v * halflogdetS - (v + p + 1) * halflogdetW - 0.5 * 
    exptrace
  lpdf &amp;lt;- lnum - ldenom
  return(exp(lpdf))
}




#unwound sigma
pts2.unwound = apply(sig,3,unwind)

n.params&amp;lt;- nrow(pts2.unwound)+n_randeffect+n_randeffect
all_samples=array(dim=c(n_subjects,n.params,n_iter))
mu_tilde=array(dim = c(n_subjects,n.params))
sigma_tilde=array(dim = c(n_subjects,n.params,n.params))


for (j in 1:n_subjects){
  all_samples[j,,] = rbind(alpha[,j,],theta[,],pts2.unwound[,])
  # calculate the mean for re, mu and sigma
  mu_tilde[j,] =apply(all_samples[j,,],1,mean)
  # calculate the covariance matrix for random effects, mu and sigma
  sigma_tilde[j,,] = cov(t(all_samples[j,,]))
}



X &amp;lt;- cbind(t(theta),t(pts2.unwound),t(a_half)) 

muX&amp;lt;-apply(X,2,mean)
sigmaX&amp;lt;-var(X)

# generates the IS proposals
prop_theta=mvtnorm::rmvt(IS_samples,sigma = sigmaX, df=1, delta=muX)
#prop_theta_compare=rmvnorm(IS_samples,muX,sigmaX)




### main functions

group_dist = function(random_effect = NULL, parameters, sample = FALSE, n_samples = NULL, n_randeffect){
  param.theta.mu &amp;lt;- parameters[1:n_randeffect]
  param.theta.sig.unwound &amp;lt;- parameters[(n_randeffect+1):(length(parameters)-n_randeffect)] 
  param.theta.sig2 &amp;lt;- unwind(param.theta.sig.unwound, reverse = TRUE)
  if (sample){
    return(mvtnorm::rmvnorm(n=n_samples, mean=param.theta.mu,sigma=param.theta.sig2))
  }else{
    logw_second&amp;lt;-mvtnorm::dmvnorm(random_effect, param.theta.mu,param.theta.sig2,log=TRUE)
    return(logw_second)
  }
}



prior_dist = function(parameters, prior_parameters = sampled$prior, n_randeffect){ ###mod notes: the sampled$prior needs to be fixed/passed in some other time
  param.theta.mu &amp;lt;- parameters[1:n_randeffect]
  param.theta.sig.unwound &amp;lt;- parameters[(n_randeffect+1):(length(parameters)-n_randeffect)] ##scott would like it to ask for n(unwind)
  param.theta.sig2 &amp;lt;- unwind(param.theta.sig.unwound, reverse = TRUE)
  param.a &amp;lt;- exp(parameters[((length(parameters)-n_randeffect)+1):(length(parameters))])
  v_alpha=2
  
  log_prior_mu=mvtnorm::dmvnorm(param.theta.mu, mean = prior_parameters$theta_mu_mean, sigma = prior_parameters$theta_mu_var, log =TRUE)
  
  log_prior_sigma = log(robust_diwish(param.theta.sig2, v=v_alpha+ n_randeffect-1, S = 2*v_alpha*diag(1/param.a)))  #exp of a-half -&amp;gt; positive only
  log_prior_a = sum(invgamma::dinvgamma(param.a,scale = 0.5,shape=1,log=TRUE))
  
  logw_den2 &amp;lt;- sum(log(1/param.a)) # jacobian determinant of transformation of log of the a-half
  logw_den3 &amp;lt;- log(2^n_randeffect)+sum((n_randeffect:1+1)*log(diag(param.theta.sig2))) #jacobian determinant of cholesky factors of cov matrix
  
  return(log_prior_mu + log_prior_sigma + log_prior_a + logw_den3 - logw_den2)
}





get_logp=function(prop_theta,data,n_subjects,n_particles,n_randeffect,mu_tilde,sigma_tilde,i, group_dist=group_dist){
  #make an array for the density
  logp=array(dim=c(n_particles,n_subjects))
  # for each subject, get 1000 IS samples (particles) and find log weight of each
  for (j in 1:n_subjects){
    #generate the particles from the conditional MVnorm AND mix of group level proposals
    wmix &amp;lt;- 0.95
    n1=rbinom(n=1,size=n_particles,prob=wmix)
    if (n1&amp;lt;2) n1=2
    if (n1&amp;gt;(n_particles-2)) n1=n_particles-2 ## These just avoid degenerate arrays.
    n2=n_particles-n1
    # do conditional MVnorm based on the proposal distribution
    conditional = condMVNorm::condMVN(mean=mu_tilde[j,],sigma=sigma_tilde[j,,],dependent.ind=1:n_randeffect,
                          given.ind=(n_randeffect+1):n.params,X.given=prop_theta[i,1:(n.params-n_randeffect)])
    particles1 &amp;lt;- mvtnorm::rmvnorm(n1, conditional$condMean,conditional$condVar)
    # mix of proposal params and conditional
    particles2 &amp;lt;- group_dist(n_samples=n2, parameters = prop_theta[i,],sample=TRUE, n_randeffect=n_randeffect)
    particles &amp;lt;- rbind(particles1,particles2)
    
    for (k in 1:n_particles){
      x &amp;lt;-particles[k,]
      #names for ll function to work
      #mod notes: this is the bit the prior effects
      names(x)&amp;lt;-sampled$par_names
      #   do lba log likelihood with given parameters for each subject, gets density of particle from ll func
      logw_first=sampled$ll_func(x,data = data[as.numeric(factor(data$subject))==j,]) #mod notes: do we pass this in or the whole sampled object????
      # below gets second part of equation 5 numerator ie density under prop_theta
      # particle k and big vector of things
      logw_second&amp;lt;-group_dist(random_effect = particles[k,], parameters = prop_theta[i,], sample= FALSE, n_randeffect = n_randeffect) #mod notes: group dist
      # below is the denominator - ie mix of density under conditional and density under pro_theta
      logw_third &amp;lt;- log(wmix*dmvnorm(particles[k,], conditional$condMean,conditional$condVar)+(1-wmix)*exp(logw_second)) #mod notes: fine?
      #does equation 5
      logw=(logw_first+logw_second)-logw_third
      #assign to correct row/column
      logp[k,j]=logw 
    }
  }
  #we use this part to centre the logw before addign back on at the end. This avoids inf and -inf values
  sub_max = apply(logp,2,max)
  logw = t(t(logp) - sub_max)
  w = exp(logw)
  subj_logp = log(apply(w,2,mean))+sub_max #means
  
# sum the logp and return
  if(is.nan(sum(subj_logp))){
    return(1e-10)
  }else{
  return(sum(subj_logp))
  }
}

compute_lw=function(prop_theta,data,n_subjects,n_particles,n_randeffect,mu_tilde,sigma_tilde,i, prior_dist=prior_dist, sampled=sampled){
  
  logp.out &amp;lt;- get_logp(prop_theta,data,n_subjects,n_particles,n_randeffect,mu_tilde,sigma_tilde,i, group_dist=group_dist)
  ##do equation 10
  logw_num &amp;lt;- logp.out[1]+prior_dist(parameters = prop_theta[i,], prior_parameters = sampled$prior, n_randeffect)
  logw_den &amp;lt;- mvtnorm::dmvt(prop_theta[i,], delta=muX, sigma=sigmaX,df=1, log = TRUE) #density of proposed params under the means
  logw     &amp;lt;- logw_num-logw_den #this is the equation 10
  return(c(logw))
  #NOTE: we should leave a note if variance is shit - variance is given by the logp function (currently commented out)
}



##### make it work

#makes an array to store the IS samples
tmp&amp;lt;-array(dim=c(IS_samples))

#do the sampling
if (cpus&amp;gt;1){
  tmp &amp;lt;- mclapply(X=1:IS_samples,mc.cores = cpus, FUN = compute_lw, prop_theta = prop_theta,data = data,n_subjects= n_subjects,n_particles = n_particles,
                  n_randeffect = n_randeffect,mu_tilde=mu_tilde,sigma_tilde = sigma_tilde, prior_dist=prior_dist, sampled=sampled)
} else{
  for (i in 1:IS_samples){
    cat(i)
    tmp[i]&amp;lt;-compute_lw(prop_theta,data,n_subjects,n_particles, n_randeffect,mu_tilde,sigma_tilde,i,prior_dist=prior_dist, sampled=sampled)
  }
}


# get the ML value
finished &amp;lt;- tmp
tmp&amp;lt;-unlist(tmp)
max.lw &amp;lt;- max(tmp)
mean.centred.lw &amp;lt;- mean(exp(tmp-max.lw)) #takes off the max and gets mean (avoids infs)
MLE &amp;lt;-log(mean.centred.lw)+max.lw #puts max back on to get the lw
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This script returns the maximum likelihood estimate for a model. Similar to DIC, this is relatively meaningless without something to compare to, where the model with the higher MLE is chosen. Further, we can also compute Bayes factors with MLE’s to show the weight of evidence for one model over another.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Ultimately, post sampling analysis is up to the user, but hopefully this blog has provided some insight into what sort of analysis is typically done, some ways of checking the sampling process, methods of posterior analysis and generating posterior data, as well as some model comparison insights. For more info on the PMwG sampler, see the &lt;a href=&#34;https://newcastlecl.github.io/samplerDoc/&#34;&gt;documentation&lt;/a&gt; and for some more useful functions, see the &lt;a href=&#34;https://github.com/NewcastleCL/pmwg_toolkit&#34;&gt;PMwG toolkit github&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Tran, M. N., Scharth, M., Gunawan, D., Kohn, R., Brown, S. D., &amp;amp; Hawkins, G. E. (2020). Robustly estimating the marginal likelihood for cognitive models via importance sampling. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, 1-18.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Simulation and Recovery with PMwG</title>
      <link>/post/2021-03-17-recovery/recovery_pmwg/</link>
      <pubDate>Wed, 17 Mar 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-03-17-recovery/recovery_pmwg/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This blog post presents a brief guide on how to create synthetic data, and perform parameter recovery, using the PMwG sampler. Parameter recovery is highly important in cognitive modelling procedures as it provides an indication of parameter identifiability (and in a way, model “power”). Parameter recovery involves creating synthetic data using a likelihood function, checking the synthetic data, performing a sampling regime and then checking the posterior estimates (as well as posterior predictive data). It is an essential step in the modelling process.&lt;/p&gt;
&lt;p&gt;Parameter recovery through simulation allows us to be confident in our models for three reasons. First, it gives us confidence that our model &lt;em&gt;actually works&lt;/em&gt;, i.e. it runs, it relates to data, it does what it’s supposed to do. Secondly, parameter recovery allows us to see whether the parameter values we input are recovered at the end, and this can give us confidence in the reliability of our model. Finally, parameter recovery allows us to test a variety of parameter combinations and models, which can test theory and help us build hypotheses.&lt;/p&gt;
&lt;div id=&#34;synthetic-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Synthetic data&lt;/h2&gt;
&lt;p&gt;Synthetic data is data generated from your model. When simulating this synthetic data, we make a data set that looks just like real data (i.e. same structure), with the aim of later replacing it with real data. We also want this synthetic data to reflect real data - i.e. somewhere in the ballpark of means and variance. When generating synthetic data, we input parameter values that we decide (or could be informed by prior research) and then use a random generator function (like rnorm, rlba or rddm), that takes in these values to create data.&lt;/p&gt;
&lt;p&gt;By creating synthetic data, we are able to control the input parameters, and so we are able to better understand how parameters affect synthetic data - i.e. whether changing given parameters changes output. Secondly, we are able to see if the original values are recovered following sampling, therefore showing how well the model identifies the actual effects. Finally, we can see how well the posterior predictive data fits the synthetic data. Often different parameters may lead to similar patterns in the data (i.e. higher thresholds and higher t0 can show similar increases in rt for accumulator models), so by comparing to the posterior predictive, this can give us an indication of whether our model can tease apart parameter differences.&lt;/p&gt;
&lt;p&gt;Within PMwG, there is an assumed hierarchical structure, with group level (theta) and individual participant level (alpha) values. On each iteration of the sampling process, n particles density are compared (given the data). These particles are drawn from a multivariate normal distribution centered around each participants current particle (with added noise). To create this multivariate normal distribution, the sampler also uses a covariance matrix, which gives insight into the level of covariance between individual parameters. This makes simulating synthetic data for the PMwG sampler slightly more complex, as simulated parameters should also have a covariance structure constraining them. This is shown below.&lt;/p&gt;
&lt;p&gt;In this blog post I will;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;construct the data structure&lt;/li&gt;
&lt;li&gt;construct a likelihood function with sampling and density calculation arguments.&lt;/li&gt;
&lt;li&gt;create some parameters for the model&lt;/li&gt;
&lt;li&gt;fill in the values for these parameters&lt;/li&gt;
&lt;li&gt;constrain by the covariance matrix&lt;/li&gt;
&lt;li&gt;create data&lt;/li&gt;
&lt;li&gt;perform parameter recovery.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulation&lt;/h2&gt;
&lt;div id=&#34;starting-out&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Starting out&lt;/h4&gt;
&lt;p&gt;First I load in the packages used for this exercise;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rtdists)
library(mvtnorm) ## For the multivariate normal.
library(MASS) ## For matrix inverse.
library(MCMCpack)
library(lme4)
library(dplyr)
library(ggplot2)
library(tidyr)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-structure&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Data structure&lt;/h4&gt;
&lt;p&gt;Next, we set up the structure of the data frame to be filled with data. For this exercise, i use 3 conditions, each with 100 trials for 20 participants. You can create whatever data structure you wish, however, it should map to your likelihood function, should be the same as your real data (or at least the parts of your real data used in the likelihood function) and can be any length (but more is better). I do this setup step first as this often assists with writing the likelihood function (i.e. for correct column names and data structure).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n.trials = 100      #number trials per subject per conditions
n.subj = 20         #number of subjects
n.cond = 3          #number of conditions


names=c(&amp;quot;subject&amp;quot;,&amp;quot;rt&amp;quot;,&amp;quot;resp&amp;quot;,&amp;quot;condition&amp;quot;) #names of columns
data = data.frame(matrix(NA, ncol = length(names), nrow = (n.trials*n.subj*n.cond))) #empty data frame
names(data)=names
data$condition = rep(1:n.cond,times = n.trials) #filling in condition
data$subject = rep(1:n.subj, each = n.trials*n.cond) #filling in subjects&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;construct-the-model&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Construct the model&lt;/h4&gt;
&lt;p&gt;Now that the data structure has been created, I need to establish my model. The model I use here is similar to that used in &lt;a href=&#34;https://newcastlecl.github.io/samplerDoc/forstmannChapter.html#writellFunc&#34;&gt;Chapter 3&lt;/a&gt; of the PMwG Documention. This model is a LBA model which includes 3 different thresholds (differing across conditions).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
log_likelihood=function(x,data,sample=TRUE) {
  x=exp(x)
  bs=x[&amp;quot;A&amp;quot;]+x[c(&amp;quot;b1&amp;quot;,&amp;quot;b2&amp;quot;,&amp;quot;b3&amp;quot;)][data$condition]
  if (sample) { #for sampling
    out=rLBA(n=nrow(data),A=x[&amp;quot;A&amp;quot;],b=bs,t0=x[&amp;quot;t0&amp;quot;],mean_v=x[c(&amp;quot;v1&amp;quot;,&amp;quot;v2&amp;quot;)],sd_v=c(1,1),distribution=&amp;quot;norm&amp;quot;,silent=TRUE)
  } else { #for calculating density
    out=dLBA(rt=data$rt,response=data$correct,A=x[&amp;quot;A&amp;quot;],b=bs,t0=x[&amp;quot;t0&amp;quot;],mean_v=x[c(&amp;quot;v1&amp;quot;,&amp;quot;v2&amp;quot;)],sd_v=c(1,1),distribution=&amp;quot;norm&amp;quot;,silent=TRUE)
    bad=(out&amp;lt;1e-10)|(!is.finite(out))
    out[bad]=1e-10
    out=sum(log(out))
  }
  out
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, I use responses “1” and “2”, but you can use whatever responses you would prefer (i.e. correct or error etc). Further, you could also use a different model type (i.e. diffusion).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-model-parameters&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Create model parameters&lt;/h4&gt;
&lt;p&gt;Next, I create my parameter vector;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
parameter.names=c(&amp;quot;b1&amp;quot;,&amp;quot;b2&amp;quot;,&amp;quot;b3&amp;quot;, &amp;quot;A&amp;quot;,&amp;quot;v1&amp;quot;,&amp;quot;v2&amp;quot;,&amp;quot;t0&amp;quot;)
n.parameters=length(parameter.names)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now i set up a vector of parameter values and a matrix of covariance matrix values. I label these &lt;code&gt;ptm&lt;/code&gt; and &lt;code&gt;ptm2&lt;/code&gt;;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ptm &amp;lt;- array(dim = n.parameters, dimnames = list(parameter.names)) #an empty array where i will put parameter values
pts2 &amp;lt;- array(dim=c(n.parameters,n.parameters), dimnames = list(parameter.names,parameter.names)) #an empty array where i will put the covariance matrix&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fill-in-model-parameters&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Fill in model parameters&lt;/h4&gt;
&lt;p&gt;I then give start points for my input parameters. Here, I use increments of 0.2 for the threshold parameters. It is important to consider a number of different values to see if, and how, they affect the simulated data (but you can do this later). For example, I may also want to vary t0 or drift rates (i.e. different models) to see if these parameters are also recoverable.&lt;/p&gt;
&lt;p&gt;NOTE: these values are on the log scale (as the likelihood function takes the exponent of these). This is important to note as PMwG draws values along the real number line.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
ptm[1:n.parameters]=c(0.1,0.3,0.5,0.4,1.2,0.3,-2) 
exp(ptm)
##        b1        b2        b3         A        v1        v2        t0 
## 1.1051709 1.3498588 1.6487213 1.4918247 3.3201169 1.3498588 0.1353353&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;create-covariance-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Create covariance matrix&lt;/h4&gt;
&lt;p&gt;Next, I make the variance of the parameters. I do this in a simple way by taking the absolute value for each parameter and divide by 10 - but you can vary them however you like, for example, there may be covariances that are important to the model. In this example with the LBA, it might be better to put larger correlations between b and t0.&lt;/p&gt;
&lt;p&gt;In this code, SigmaC is a matrix with diagonal of 0.2 (i.e. a correlation of .2 between all parameters), and off diagonal as the variance we just created. I then do sdcor2cov to create a covariance matrix. I do this because, rather than correlation, PMwG expects covariance. You can do the opposite transform to recover these correlations after recovery.&lt;/p&gt;
&lt;p&gt;This step is the tricky part of simulating in PMwG, as we need this correlation structure (which becomes a covariance structure) to constrain parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;

vars = abs(ptm)/10 #off diagonal correlations are done as absolute/10
sigmaC = matrix(.2,nrow=length(ptm),ncol=length(ptm)) ###std dev correlation on diagonal - you might think this should be corr = 1, but it&amp;#39;s actually the standard deviation 
diag(sigmaC)=sqrt(vars)
sigmaC &amp;lt;- sdcor2cov(sigmaC)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;create-random-effects&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Create random effects&lt;/h4&gt;
&lt;p&gt;Finally, I create the subjects random effects. This is a n_parameters * n_subject matrix, so that each subject has a value for each parameter. This is created using rmvnorm, where I do n_subjects of random samples using the mean parameters (ptm) and covariance matrix (sigmaC). &lt;strong&gt;These are the values we wish to recover using the model&lt;/strong&gt; (so it’s often good to save out ptm, sigmaC and subj_random_effects).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
subj_random_effects &amp;lt;- t(mvtnorm::rmvnorm(n.subj,mean=ptm,sigma=sigmaC))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]        [,2]        [,3]         [,4]        [,5]        [,6]
## b1  0.06754572  0.16595634 -0.14890327  0.007714805  0.22501307  0.04926983
## b2  0.45809512  0.01340640  0.08264425  0.460439178  0.30218297  0.23504860
## b3  0.58720469  0.37898141  1.13992801  0.573482556  0.32560658  0.63982060
## A   0.04662982  0.47198462  0.30647877  0.399114046  0.37684673  0.36270810
## v1  0.81182229  0.72201092  1.25883303  1.574770678  1.08952060  1.14255482
## v2  0.20903875  0.03129003  0.33744775  0.210704343  0.05004988  0.27938543
## t0 -1.75064334 -2.78634793 -1.26680060 -1.729459232 -2.60528073 -1.86775101
##          [,7]       [,8]        [,9]      [,10]      [,11]      [,12]
## b1  0.1242662  0.1685503  0.02285856  0.1965128  0.1384761 -0.1783804
## b2  0.2630983  0.1563505  0.32340648  0.1680148  0.4791735  0.2025268
## b3  0.6912657  0.7539752  0.21982543  0.5844972  0.5919369  0.4334701
## A   0.2050011  0.5426899  0.13960102  0.4180867  0.4450346  0.2092065
## v1  1.3145116  1.2392363  0.81793618  1.5938252  1.4879329  0.7255356
## v2  0.5693084  0.4971342  0.32438670  0.3977475  0.3670363  0.1248367
## t0 -3.1241916 -2.1788509 -2.25873295 -1.3214511 -1.8642588 -2.7759320
##         [,13]        [,14]      [,15]       [,16]       [,17]       [,18]
## b1  0.2058493  0.009413219  0.2026047  0.09626992  0.16157925  0.03773296
## b2  0.2836621 -0.071166522  0.1187272  0.26290527  0.17193572 -0.01895990
## b3  0.4004229  0.301991774  0.1302980  0.61103915  0.24588443  0.34190158
## A   0.6057064  0.143238731  0.3268137  0.31840193  0.50836559  0.49358757
## v1  1.6789963  0.999523427  2.0098674  1.39139828  1.31009377  0.66862124
## v2  0.4274261  0.444679107  0.3446718  0.36219300  0.07455863  0.47500308
## t0 -2.2730857 -2.881332602 -2.4249129 -1.93661795 -1.87873689 -2.30251849
##         [,19]       [,20]
## b1  0.1225545  0.07948995
## b2  0.1557267  0.32662834
## b3  0.5413826  0.29090185
## A   0.4638920  0.54914061
## v1  1.1878292  1.49936441
## v2  0.3606136  0.30283934
## t0 -2.2716049 -1.85730079&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;create-data&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Create data&lt;/h4&gt;
&lt;p&gt;Using the likelihood function, I now run a for loop to create data and fill that empty matrix we made at the start! The likelihood returns both rt and resp, so the lower two lines fill in the appropriate columns in the data frame. The &lt;code&gt;for&lt;/code&gt; loop runs for each subject, and so I input the subj_random_effect for each participant and fill the appropriate place in the data structure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; It is important to check the data and general trends from the output. This can give an indication of how different parameters and covariance affect the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (i in 1:n.subj){
  tmp&amp;lt;- log_likelihood(subj_random_effects[,i],sample=TRUE,data=data[data$subject==i,])
  data$rt[data$subject==i]=tmp$rt
  data$resp[data$subject==i]=tmp$response
}
head(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   subject        rt resp condition
## 1       1 1.7090777    2         1
## 2       1 0.9500044    1         2
## 3       1 1.1687292    2         3
## 4       1 0.9118316    2         1
## 5       1 1.2868745    1         2
## 6       1 1.0794218    2         3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have synthetic data which;
- is generated using our model
- uses theta values we input
- uses a covariance structure we created&lt;/p&gt;
&lt;p&gt;This means that these objects can be recovered.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;recovery&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Recovery&lt;/h2&gt;
&lt;p&gt;I now run a typical PMwG sampling exercise. Here the likelihood is unchanged from the PMwG documentation. I use broad priors and do not specify any start points. Following the adaptation stage, i take 1000 samples from the posterior.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Running this on your own machine may be time consuming (and resource consuming). You may want to run it on a grid or dial down the number of cores used in sampling and wait it out.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
library(pmwg)

# Specify the log likelihood function -------------------------------------------

lba_loglike &amp;lt;- function(x, data, sample = FALSE) {
  x &amp;lt;- exp(x)
  if (any(data$rt &amp;lt; x[&amp;quot;t0&amp;quot;])) {
    return(-1e10)
  }
  if (sample){
    data$rt=NA
    data$resp = NA
  }
  
  bs &amp;lt;- x[&amp;quot;A&amp;quot;] + x[c(&amp;quot;b1&amp;quot;, &amp;quot;b2&amp;quot;, &amp;quot;b3&amp;quot;)][data$condition]
  
  if (sample) {
    out &amp;lt;- rtdists::rLBA(n = nrow(data),
                         A = x[&amp;quot;A&amp;quot;],
                         b = bs,
                         t0 = x[&amp;quot;t0&amp;quot;],
                         mean_v = x[c(&amp;quot;v1&amp;quot;, &amp;quot;v2&amp;quot;)],
                         sd_v = c(1, 1),
                         distribution = &amp;quot;norm&amp;quot;,
                         silent = TRUE)
    data$rt &amp;lt;- out$rt
    data$resp &amp;lt;- out$resp
    
  } else {
    out &amp;lt;- rtdists::dLBA(rt = data$rt,
                         response = data$resp,
                         A = x[&amp;quot;A&amp;quot;],
                         b = bs,
                         t0 = x[&amp;quot;t0&amp;quot;],
                         mean_v = list(x[&amp;quot;v1&amp;quot;],x[ &amp;quot;v2&amp;quot;]),
                         sd_v = c(1, 1),
                         distribution = &amp;quot;norm&amp;quot;,
                         silent = TRUE)
    bad &amp;lt;- (out &amp;lt; 1e-10) | (!is.finite(out))
    out[bad] &amp;lt;- 1e-10
    out &amp;lt;- sum(log(out))
  }
  if (sample){return(data)}
  if (!sample){return(out)}
}




# Specify the parameters and priors -------------------------------------------

# Vars used for controlling the run
pars &amp;lt;- c(&amp;quot;b1&amp;quot;,&amp;quot;b2&amp;quot;,&amp;quot;b3&amp;quot;, &amp;quot;A&amp;quot;,&amp;quot;v1&amp;quot;,&amp;quot;v2&amp;quot;,&amp;quot;t0&amp;quot;)

priors &amp;lt;- list(
  theta_mu_mean = rep(0, length(pars)),
  theta_mu_var = diag(rep(9, length(pars)))) 


# Create the Particle Metropolis within Gibbs sampler object ------------------

sampler &amp;lt;- pmwgs(
  data = data,
  pars = pars,
  prior = priors,
  ll_func = lba_loglike
)


# start the sampler ---------------------------------------------------------

sampler &amp;lt;- init(sampler) # i don&amp;#39;t use any start points here


# Sample! -------------------------------------------------------------------

burned &amp;lt;- run_stage(sampler, stage = &amp;quot;burn&amp;quot;,iter = 500, particles = 100, n_cores = 16) #epsion will set automatically to 0.5

adapted &amp;lt;- run_stage(burned, stage = &amp;quot;adapt&amp;quot;, iter = 5000, particles = 100, n_cores = 16)

sampled &amp;lt;- run_stage(adapted, stage = &amp;quot;sample&amp;quot;, iter = 1000, particles = 100, n_cores = 16)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have samples from our model which is fitted to the simulated data! You should check that the sampling process worked well before continuing on. To check this process, see the guide in the &lt;a href=&#34;https://newcastlecl.github.io/samplerDoc/pmwg-sampler-and-signal-detection-theory.html#check-sampling-process&#34;&gt;PMwG documentation&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;checking-and-simulating-posterior-predictive-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Checking and Simulating posterior predictive data&lt;/h2&gt;
&lt;p&gt;We now have 1000 posterior samples. From here, there are several steps we want to take to check how well parameters recover. First, we’ll check the mean posterior theta estimates. Then we’ll check the alpha (random effects) estimates, before checking the covariance matrix. For extra checking, I’ll then simulate data from the posterior to see how well the data recovers.&lt;/p&gt;
&lt;div id=&#34;checking-the-theta-alpha-and-covariance-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Checking the theta, alpha and covariance matrix&lt;/h4&gt;
&lt;p&gt;The first check I do is to see whether our &lt;code&gt;ptm&lt;/code&gt; values and &lt;code&gt;subj_random_effect&lt;/code&gt; recovered following simulation and sampling. I first do this at the group (theta) level;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theta &amp;lt;- apply(sampled$samples$theta_mu[,sampled$samples$stage==&amp;quot;sample&amp;quot;],1,mean) #gets the 
exp(theta) #done for convenience of interpretation
##        b1        b2        b3         A        v1        v2        t0 
## 0.8832800 1.0173988 1.4128471 1.5383791 3.3117580 1.1322673 0.1540344
exp(ptm)
##        b1        b2        b3         A        v1        v2        t0 
## 1.1051709 1.3498588 1.6487213 1.4918247 3.3201169 1.3498588 0.1353353&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then at the subject (alpha) level;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;alpha &amp;lt;- apply(sampled$samples$alpha[,,sampled$samples$stage==&amp;quot;sample&amp;quot;],1:2,mean)
exp(alpha)
##            1         2         3         4         5         6          7
## b1 0.7827243 0.9245987 0.7371639 0.8802455 0.9560409 0.8955390 0.82912937
## b2 1.0776528 0.8313766 0.8328184 1.5171650 1.1111514 1.1415803 0.98725643
## b3 1.4462429 1.1987399 2.8858160 1.7338225 1.1443454 1.6979257 1.58823062
## A  1.2372040 1.6414723 1.5708520 1.4744584 1.7530787 1.5733772 1.42484902
## v1 2.0723960 2.0740149 3.5780296 4.6794195 2.8813526 3.0778612 3.65466091
## v2 0.9351639 0.7248956 1.0610496 0.9406761 1.1274176 1.0988563 1.23011158
## t0 0.2202784 0.1229796 0.2960367 0.1865906 0.1174551 0.1609932 0.09479353
##            8         9        10        11         12        13         14
## b1 0.8361656 0.8167511 0.7924110 0.9002450 0.69403112 0.9664530 0.88863815
## b2 0.6926256 1.0854374 0.7986617 1.4237268 0.96883862 1.1169777 0.81015438
## b3 1.7298796 0.8970793 1.4943264 1.6228472 1.31072072 1.2897463 1.18800982
## A  1.8387425 1.1814593 1.6525283 1.5502767 1.57883116 1.6410829 1.41700250
## v1 3.2886217 2.0794062 4.8889854 4.3015547 2.07489764 4.8543457 2.73883101
## v2 1.1496684 1.1439376 1.1135874 1.1787368 1.11566670 1.1968833 1.63773055
## t0 0.1869287 0.1638233 0.3126664 0.1922919 0.09447705 0.1224144 0.09564317
##           15        16        17        18        19        20
## b1 1.0291379 0.9820875 1.0576247 0.9419080 0.8476923 1.0358966
## b2 0.7931455 1.2592877 1.0747271 0.9097072 1.0572080 1.2957602
## b3 0.8598098 1.7561695 1.3005711 1.2929230 1.6198270 1.3100812
## A  1.3285388 1.4552237 1.6838932 1.7658242 1.5804548 1.8069369
## v1 6.7084610 4.0459888 3.7720199 2.2243421 3.1182496 4.5875267
## v2 1.4177665 1.2421040 0.8438141 1.6249084 1.2687774 1.0955937
## t0 0.1056549 0.1586988 0.1706621 0.1210396 0.1335432 0.1640788
exp(subj_random_effects)
##         [,1]       [,2]      [,3]      [,4]       [,5]      [,6]       [,7]
## b1 1.0698792 1.18052156 0.8616525 1.0077446 1.25233908 1.0505038 1.13231721
## b2 1.5810594 1.01349667 1.0861553 1.5847698 1.35280872 1.2649702 1.30095455
## b3 1.7989528 1.46079588 3.1265433 1.7744359 1.38487043 1.8961407 1.99624056
## A  1.0477341 1.60317273 1.3586326 1.4905036 1.45768087 1.4372163 1.22752645
## v1 2.2520081 2.05856866 3.5213098 4.8296340 2.97284855 3.1347669 3.72293224
## v2 1.2324928 1.03178471 1.4013664 1.2345473 1.05132353 1.3223169 1.76704460
## t0 0.1736622 0.06164594 0.2817316 0.1773803 0.07388239 0.1544707 0.04397247
##         [,8]      [,9]     [,10]     [,11]     [,12]     [,13]      [,14]
## b1 1.1835877 1.0231218 1.2171509 1.1485222 0.8366241 1.2285680 1.00945766
## b2 1.1692359 1.3818269 1.1829541 1.6147392 1.2244929 1.3279842 0.93130680
## b3 2.1254323 1.2458592 1.7940886 1.8074860 1.5426012 1.4924557 1.35255010
## A  1.7206290 1.1498150 1.5190524 1.5605442 1.2326995 1.8325463 1.15400527
## v1 3.4529753 2.2658188 4.9225426 4.4279333 2.0658373 5.3601733 2.71698668
## v2 1.6440032 1.3831821 1.4884682 1.4434504 1.1329634 1.5333058 1.55998953
## t0 0.1131715 0.1044828 0.2667479 0.1550111 0.0622914 0.1029939 0.05606001
##         [,15]     [,16]    [,17]     [,18]     [,19]     [,20]
## b1 1.22458829 1.1010562 1.175366 1.0384539 1.1303807 1.0827347
## b2 1.12606272 1.3007035 1.187601 0.9812187 1.1685068 1.3862862
## b3 1.13916786 1.8423449 1.278752 1.4076217 1.7183810 1.3376333
## A  1.38654314 1.3749288 1.662572 1.6381828 1.5902512 1.7317641
## v1 7.46232804 4.0204679 3.706521 1.9515447 3.2799533 4.4788415
## v2 1.41152663 1.4364761 1.077409 1.6080192 1.4342091 1.3536970
## t0 0.08848583 0.1441908 0.152783 0.1000067 0.1031465 0.1560934&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These seem to recover quite accurately. Next though, we need to check the covariance matrix;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
sig&amp;lt;-sampled$samples$theta_sig[,,sampled$samples$stage==&amp;quot;sample&amp;quot;]

cov&amp;lt;-apply(sig,3,cov2cor)
cov2&amp;lt;-array(cov, c(length(pars),length(pars),1000)) ##1000 is the length of posterior sampling 
colnames(cov2)&amp;lt;-pars
rownames(cov2)&amp;lt;-pars
apply(cov2,1:2, mean)
##             b1          b2         b3           A         v1          v2
## b1  1.00000000  0.27435761 0.02309705  0.03536069 0.16886207  0.18033331
## b2  0.27435761  1.00000000 0.13002183 -0.02399570 0.10831885  0.06176004
## b3  0.02309705  0.13002183 1.00000000  0.08640149 0.07622340  0.02414490
## A   0.03536069 -0.02399570 0.08640149  1.00000000 0.12468679  0.11164222
## v1  0.16886207  0.10831885 0.07622340  0.12468679 1.00000000  0.11637253
## v2  0.18033331  0.06176004 0.02414490  0.11164222 0.11637253  1.00000000
## t0 -0.25814657 -0.11025466 0.17354185  0.03909197 0.08641464 -0.20071341
##             t0
## b1 -0.25814657
## b2 -0.11025466
## b3  0.17354185
## A   0.03909197
## v1  0.08641464
## v2 -0.20071341
## t0  1.00000000
cov2cor(sigmaC)
##      [,1] [,2] [,3] [,4] [,5] [,6] [,7]
## [1,]  1.0  0.2  0.2  0.2  0.2  0.2  0.2
## [2,]  0.2  1.0  0.2  0.2  0.2  0.2  0.2
## [3,]  0.2  0.2  1.0  0.2  0.2  0.2  0.2
## [4,]  0.2  0.2  0.2  1.0  0.2  0.2  0.2
## [5,]  0.2  0.2  0.2  0.2  1.0  0.2  0.2
## [6,]  0.2  0.2  0.2  0.2  0.2  1.0  0.2
## [7,]  0.2  0.2  0.2  0.2  0.2  0.2  1.0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;simulate-posterior&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Simulate Posterior&lt;/h4&gt;
&lt;p&gt;Following these checks, I simulate posterior predictive data from the posterior model estimates. For simulating from the posterior, I follow a similar method to that shown &lt;a href=&#34;https://newcastlecl.github.io/samplerDoc/pmwg-sampler-and-signal-detection-theory.html#simulating-posterior-data&#34;&gt;here&lt;/a&gt;. First taking some random posterior samples, and then using our likelihood function to simulate data (similar to the earlier synthetic data step). In this example, I use 20 posterior samples (this is a good start - you may want to use more). This means my posterior predictive data will be 20 times the length of my simulated data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;generate.posterior &amp;lt;- function(sampled, n){  #this function uses sampled$ll_func as the likelihood function to sample from
  n.posterior=n # Number of samples from posterior distribution.
  pp.data=list()
  S = sampled$n_subjects
  data=sampled$data
  data$subject&amp;lt;- as.factor(data$subject)
  sampled_stage = length(sampled$samples$stage[sampled$samples$stage==&amp;quot;sample&amp;quot;])
  for (s in 1:S) {
    cat(s,&amp;quot; &amp;quot;)
    iterations=round(seq(from=(sampled$samples$idx-sampled_stage) , to=sampled$samples$idx, length.out=n.posterior)) #grab a random iteration of posterior samples
    for (i in 1:length(iterations)) { #for each iteration, generate data
      x &amp;lt;- sampled$samples$alpha[,s,iterations[i]]
      names(x) &amp;lt;- sampled$par_names
      tmp=sampled$ll_func(x=x,data=data[as.integer(as.numeric(data$subject))==s,],sample=TRUE) #this line relies on your LL being setup to generate data
      if (i==1) {
        pp.data[[s]]=cbind(i,tmp)
      } else {
        pp.data[[s]]=rbind(pp.data[[s]],cbind(i,tmp))
      }
    }
    
  }
  return(pp.data)
}

pp.data&amp;lt;-generate.posterior(sampled,20)
tmp=do.call(rbind,pp.data)
glimpse(tmp)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 120,000
## Columns: 5
## $ i         &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …
## $ subject   &amp;lt;fct&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …
## $ rt        &amp;lt;dbl&amp;gt; 0.8109699, 1.3727252, 1.0959211, 0.4747878, 0.9924096, 2.803…
## $ resp      &amp;lt;dbl&amp;gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, …
## $ condition &amp;lt;int&amp;gt; 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, …&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have data from the posterior, we can plot this data against our synthetic data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#functions to find the 10th, 30th, 50th, 70th and 90th quantiles
q10=function(x) quantile(x,prob=.10,na.rm = TRUE)
q30=function(x) quantile(x,prob=.30,na.rm = TRUE)
q50=function(x) quantile(x,prob=.50,na.rm = TRUE)
q70=function(x) quantile(x,prob=.70,na.rm = TRUE)
q90=function(x) quantile(x,prob=.90,na.rm = TRUE)

#posterior rt data across quantiles
posterior &amp;lt;- tmp %&amp;gt;% group_by(i,condition) %&amp;gt;%
  summarise(&amp;quot;0.1&amp;quot;=q10(rt),
            &amp;quot;0.3&amp;quot;=q30(rt),
            &amp;quot;0.5&amp;quot;=q50(rt),
            &amp;quot;0.7&amp;quot;=q70(rt),
            &amp;quot;0.9&amp;quot;=q90(rt))%&amp;gt;%
  ungroup()

#reshape
posterior&amp;lt;-posterior %&amp;gt;%
  pivot_longer(
    cols = starts_with(&amp;quot;0&amp;quot;),
    names_to = &amp;quot;quantile&amp;quot;,
    names_prefix = &amp;quot;&amp;quot;,
    values_to = &amp;quot;rt&amp;quot;,
    values_drop_na = TRUE
  )

#synthetic rt data across quantiles
synthetic &amp;lt;- data %&amp;gt;% group_by(condition) %&amp;gt;%
  summarise(&amp;quot;0.1&amp;quot;=q10(rt),
            &amp;quot;0.3&amp;quot;=q30(rt),
            &amp;quot;0.5&amp;quot;=q50(rt),
            &amp;quot;0.7&amp;quot;=q70(rt),
            &amp;quot;0.9&amp;quot;=q90(rt))%&amp;gt;%
  ungroup()

#resahpe
synthetic&amp;lt;-synthetic %&amp;gt;%
  pivot_longer(
    cols = starts_with(&amp;quot;0&amp;quot;),
    names_to = &amp;quot;quantile&amp;quot;,
    names_prefix = &amp;quot;&amp;quot;,
    values_to = &amp;quot;rt&amp;quot;,
    values_drop_na = TRUE
  )

synthetic$condition&amp;lt;-as.factor(synthetic$condition)
posterior$condition&amp;lt;-as.factor(posterior$condition)

#plot - this is only for RT. Also may want to plot accuracy
ggplot(posterior, aes(x=as.numeric(quantile), y= rt))+
  geom_point(aes(colour = condition))+
  xlab(&amp;quot;Quantile&amp;quot;)+
  scale_color_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;, &amp;quot;green&amp;quot;))+
  geom_line(aes(colour = condition))+
  scale_color_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;, &amp;quot;green&amp;quot;))+
  geom_point(data=synthetic, aes(x=as.numeric(quantile), y= rt, fill = condition), shape = 24, size=3, alpha = .5)+
  theme_bw()+
  scale_fill_manual(values = c(&amp;quot;grey&amp;quot;, &amp;quot;black&amp;quot;, &amp;quot;navy&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;staticgraph-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This graph shows RT on the y-axis across quantiles (x-axis) for both the simulated (grey triangles) and posterior recovery data (shown as coloured dots for each posterior iteration and lines for the means). Here we see the posterior recovered samples appear to match the synthetic data quite well, as we expected.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Simulation and recovery practises are important for any modelling exercise. It is important to be familiar with the methods to perform simulation and recovery so you can be confident in your log likelihood function being reliable and valid (in regards to theory). This blog post aimed to explain methods of simulation and recovery for the PMwG package, and provide a framework to allow such practices.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Why does t0 estimate well in PMwG?</title>
      <link>/post/2021-03-16-t0-estimation/why-does-t0-estimate-well-/</link>
      <pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-03-16-t0-estimation/why-does-t0-estimate-well-/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;There has been substantial research and debate over the non-decision time parameter of evidence accumulation models. For LBA and diffusion models, there is often a discrepancy, where diffusion models show longer t0 times, with the LBA known to poorly estimate this parameter. Researchers are aware of this, however, the issue is largely ignored. Research into this parameter has used neural measurement data and muscular activity to provide useful estimates of t0 (i.e. the upper and lower limits of the parameter) to give a sense of how accurate this estimation is.&lt;/p&gt;
&lt;p&gt;Recently, following a paper by Gunawan et al., (2020), a new method of Bayesian hierarchical estimation was outlined which shows more reliable parameter estimation. Following several modelling exercises using this method (known as Particle Metropolis within Gibbs - PMwG), it was found that t0 estimates were much improved from previous DE-MCMC sampling methods. “Improved” being that estimates of t0 often centered around 0.01 seconds, whereas in PMwG, this was closer to 0.1 seconds - much more reasonable and in line with literature. So maybe it wasn’t so much a problem with the model, but rather a problem with the model estimation. In this blog post, i explore why this might be.&lt;/p&gt;
&lt;div id=&#34;first-parameter-recovery&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;First, parameter recovery&lt;/h2&gt;
&lt;p&gt;First of all, I’ll make sure this holds in a parameter recovery exercise - after all, we would like to know if the LBA is still doing a not-so-great job of figuring out t0. In this simulation exercise, I show t0 recovering at 3 different values for a single t0 LBA model (i.e. one t0 for all simulated conditions) and 3 lots of t0 values for a three t0 model (i.e. each condition has a t0 value). In the code below, small, medium and large refer to the input t0 parameters (small = fast t0, large = slow t0).&lt;/p&gt;
&lt;div id=&#34;single-t0&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Single t0&lt;/h3&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:singlet0recover&#34;&gt;Table 1: &lt;/span&gt;Large t0&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Actual.t0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Recovered&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2254268&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:singlet0recover&#34;&gt;Table 1: &lt;/span&gt;Medium t0&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Actual.t0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Recovered&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1715356&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:singlet0recover&#34;&gt;Table 1: &lt;/span&gt;Small t0&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Actual.t0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0300000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Recovered&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0836326&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;large&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Large&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;staticalphasB-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;medium&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Medium&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;staticalphasM-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;staticalphasS-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;t0-varies-with-conds&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;t0 varies with conds&lt;/h3&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:doubltt0recover&#34;&gt;Table 2: &lt;/span&gt;Large t0&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Actual.t01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1500000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Actual.t02&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Actual.t03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2500000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Recovered.t01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1227897&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Recovered.t02&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1991755&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Recovered.t03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1987277&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:doubltt0recover&#34;&gt;Table 2: &lt;/span&gt;Medium t0&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Actual.t01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0500000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Actual.t02&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Actual.t03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1500000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Recovered.t01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0733236&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Recovered.t02&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1138871&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Recovered.t03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1758666&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:doubltt0recover&#34;&gt;Table 2: &lt;/span&gt;Small t0&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Actual.t01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0300000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Actual.t02&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0600000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Actual.t03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0900000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Recovered.t01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0507706&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Recovered.t02&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0972373&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Recovered.t03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1111606&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So, it looks like t0 recovers relatively well, but maybe overestimates smaller values. This means that the LBA may still not perfectly estimate actual t0 values, but could also come from the variance in the individual subject synthetic parameters. One thing is for sure though, t0 is recovered at reasonable values compared to old DE-MCMC. So what could help this estimation method?&lt;/p&gt;
&lt;!-- WHY?????? espeically for small values, it actually is bigger???? Check other params --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;log-transform&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Log Transform&lt;/h2&gt;
&lt;p&gt;One answer to this question is the log transformation of the parameter vector. This was proposed in Gunawan et al., (2020) so that values drawn from PMwG, which are on the real number line, could be used with the LBA - which requires positive-definite values. Hence, when using the LBA in PMwG, we take the exponent of the proposal parameters to calculate the likelihood - where we return the log of the likelihood.&lt;/p&gt;
&lt;p&gt;Essentially by doing the log transform of the parameters, all particle values are sampled from the real number line. This makes it starightforward to specify their joint distribution as a multivariate normal (with full covariance matrix structure). Previously, we would’ve assumed that the prior joint distribution of the parameters was an uncorrelated, truncated (at zero) univariate normal. In this new approach, with the log transform (which in practice is just taking the exponential of the values on the real line), there are two key advantages; increasing sampling efficiency (sampling from the real line) and using more informed prior knowledge. further, previous methods could lead to overconfidence in the precision of estimation, and underestimation of the magnitide of individual differences - which could be key in estimating t0. With the covariance matrix able to be estimated, this links to the next section, but also, the log transform is necessary to better estimate this structure.&lt;/p&gt;
&lt;p&gt;To test the log transform, I use the Forstmann et al., (2008) dataset reported in Gunawan et al., (2020) for a 3 threshold LBA. Using the PMwG sampler, I fit the model twice, using varying likelihood functions - one which takes the exponent of the proposed values and one which returns bad likelihoods for proposed values below 0 - i.e. method one is the log transformed way, method two is the untransformed (but protected against negative values) way.&lt;/p&gt;
&lt;p&gt;The results are shown below.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:thetaExpLogComparison&#34;&gt;Table 3: &lt;/span&gt;t0 estimated values on the exponential scale (normal way) and on the real number line (logged)&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Log Transformed&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1207189&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Truncated values&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1004302&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;alpha-values&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;alpha values&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;staticalphaExpLogComparison-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;covariance-matrix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;covariance matrix&lt;/h2&gt;
&lt;p&gt;In this section, I investigate whether the covariance matrix could be a main cause for better estimates of t0. As mentioned earlier, PMwG samples from the real line, making it easy to specify the parameters (particles) distribution as an unconstrained multivariate normal distribution with full covariance structure. Using the particle approach has an advantage over other MCMC methods as we can jointly estimate the density of parameters, which enables the covariance matrix to be informed, which then constrains proposed particles.&lt;/p&gt;
&lt;p&gt;To test whether the covariance matrix is a main cause for more accurate t0 estimation, I again ran the above model of the Forstmann et al., (2008) data set (with the log transform) over two different iterations. The first iteration ran PMwG as the R package defaults too. In the second iteration, I changed the v-half parameter, which is the hyperparameter on Σ prior (Half-t degrees of freedom). We tend to use v-half = 1. But to constrain the covariance matrix, I set v-half to 1000, essentially rendering the matrix useless.&lt;/p&gt;
&lt;p&gt;The following shows the results of this change.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:vHalf&#34;&gt;Table 4: &lt;/span&gt;t0 estimated with and without the covariance structure (using a high v-half)&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Exponential with Covariance&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1207189&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Without Covariance&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1058109&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;alpha-values-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;alpha values&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;staticalphaVhalfComparison-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Evidently, removing the restraint from the covariance matrix leads to less reliable sampling and worse t0 estimates. Evidently, it is not directly one reason that t0 estimates more reliably in PMwG, but a combination of things.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;a-comment-on-hierarchical-shrinkage&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;a comment on hierarchical shrinkage&lt;/h2&gt;
&lt;p&gt;Evidently, although the sampler does a good job in recovering most values of t0, we still see some hierarchical shrinkage, especially with larger t0 values. This is to be expected with hierarchical Bayesian sampling models, however, should still be considered when reporting results.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Forstmann, B. U., Dutilh, G., Brown, S., Neumann, J., Von Cramon, D. Y., Ridderinkhof, K. R., &amp;amp; Wagenmakers, E. J. (2008). Striatum and pre-SMA facilitate decision-making under time pressure. &lt;em&gt;Proceedings of the National Academy of Sciences, 105(45)&lt;/em&gt;, 17538-17542.&lt;/p&gt;
&lt;p&gt;Gunawan, D., Hawkins, G. E., Tran, M. N., Kohn, R., &amp;amp; Brown, S. D. (2020). New estimation approaches for the hierarchical Linear Ballistic Accumulator model. &lt;em&gt;Journal of Mathematical Psychology, 96&lt;/em&gt;, 102368.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
